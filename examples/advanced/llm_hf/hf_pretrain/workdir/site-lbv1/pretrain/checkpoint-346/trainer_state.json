{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9985569985569986,
  "eval_steps": 500,
  "global_step": 346,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005772005772005772,
      "grad_norm": 9.1875,
      "learning_rate": 0.0,
      "loss": 7.683,
      "step": 1
    },
    {
      "epoch": 0.011544011544011544,
      "grad_norm": 8.625,
      "learning_rate": 8.333333333333333e-05,
      "loss": 7.6215,
      "step": 2
    },
    {
      "epoch": 0.017316017316017316,
      "grad_norm": 12.875,
      "learning_rate": 0.00016666666666666666,
      "loss": 7.1619,
      "step": 3
    },
    {
      "epoch": 0.023088023088023088,
      "grad_norm": 16.375,
      "learning_rate": 0.00025,
      "loss": 7.1018,
      "step": 4
    },
    {
      "epoch": 0.02886002886002886,
      "grad_norm": 18.0,
      "learning_rate": 0.0003333333333333333,
      "loss": 7.4656,
      "step": 5
    },
    {
      "epoch": 0.03463203463203463,
      "grad_norm": 20.0,
      "learning_rate": 0.0004166666666666667,
      "loss": 7.548,
      "step": 6
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 14.625,
      "learning_rate": 0.0005,
      "loss": 8.1751,
      "step": 7
    },
    {
      "epoch": 0.046176046176046176,
      "grad_norm": 11.5625,
      "learning_rate": 0.0004999557652060729,
      "loss": 7.3214,
      "step": 8
    },
    {
      "epoch": 0.05194805194805195,
      "grad_norm": 11.125,
      "learning_rate": 0.0004998230764780276,
      "loss": 6.9968,
      "step": 9
    },
    {
      "epoch": 0.05772005772005772,
      "grad_norm": 132.0,
      "learning_rate": 0.0004996019807715324,
      "loss": 8.9205,
      "step": 10
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 12.4375,
      "learning_rate": 0.0004992925563275714,
      "loss": 7.6885,
      "step": 11
    },
    {
      "epoch": 0.06926406926406926,
      "grad_norm": 8.3125,
      "learning_rate": 0.0004988949126447567,
      "loss": 7.1068,
      "step": 12
    },
    {
      "epoch": 0.07503607503607504,
      "grad_norm": 16.75,
      "learning_rate": 0.0004984091904405792,
      "loss": 6.9867,
      "step": 13
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 5.6875,
      "learning_rate": 0.000497835561601612,
      "loss": 6.9295,
      "step": 14
    },
    {
      "epoch": 0.08658008658008658,
      "grad_norm": 6.90625,
      "learning_rate": 0.0004971742291226826,
      "loss": 6.7825,
      "step": 15
    },
    {
      "epoch": 0.09235209235209235,
      "grad_norm": 4.125,
      "learning_rate": 0.0004964254270350387,
      "loss": 6.7623,
      "step": 16
    },
    {
      "epoch": 0.09812409812409813,
      "grad_norm": 4.4375,
      "learning_rate": 0.0004955894203235284,
      "loss": 6.7197,
      "step": 17
    },
    {
      "epoch": 0.1038961038961039,
      "grad_norm": 2.15625,
      "learning_rate": 0.0004946665048328287,
      "loss": 6.6911,
      "step": 18
    },
    {
      "epoch": 0.10966810966810966,
      "grad_norm": 3.921875,
      "learning_rate": 0.0004936570071627517,
      "loss": 6.7472,
      "step": 19
    },
    {
      "epoch": 0.11544011544011544,
      "grad_norm": 4.03125,
      "learning_rate": 0.0004925612845526691,
      "loss": 6.7398,
      "step": 20
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 2.59375,
      "learning_rate": 0.0004913797247550911,
      "loss": 6.6482,
      "step": 21
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 3.40625,
      "learning_rate": 0.0004901127458984516,
      "loss": 6.7035,
      "step": 22
    },
    {
      "epoch": 0.13275613275613277,
      "grad_norm": 3.75,
      "learning_rate": 0.0004887607963391394,
      "loss": 6.6536,
      "step": 23
    },
    {
      "epoch": 0.13852813852813853,
      "grad_norm": 4.78125,
      "learning_rate": 0.00048732435450283564,
      "loss": 6.7525,
      "step": 24
    },
    {
      "epoch": 0.1443001443001443,
      "grad_norm": 3.734375,
      "learning_rate": 0.00048580392871520943,
      "loss": 6.655,
      "step": 25
    },
    {
      "epoch": 0.15007215007215008,
      "grad_norm": 2.53125,
      "learning_rate": 0.00048420005702203196,
      "loss": 6.8409,
      "step": 26
    },
    {
      "epoch": 0.15584415584415584,
      "grad_norm": 2.171875,
      "learning_rate": 0.00048251330699877374,
      "loss": 6.6919,
      "step": 27
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 2.328125,
      "learning_rate": 0.00048074427554975236,
      "loss": 6.643,
      "step": 28
    },
    {
      "epoch": 0.1673881673881674,
      "grad_norm": 2.34375,
      "learning_rate": 0.00047889358869690056,
      "loss": 6.7296,
      "step": 29
    },
    {
      "epoch": 0.17316017316017315,
      "grad_norm": 2.640625,
      "learning_rate": 0.0004769619013582309,
      "loss": 6.7368,
      "step": 30
    },
    {
      "epoch": 0.17893217893217894,
      "grad_norm": 2.40625,
      "learning_rate": 0.00047494989711607415,
      "loss": 6.7248,
      "step": 31
    },
    {
      "epoch": 0.1847041847041847,
      "grad_norm": 2.640625,
      "learning_rate": 0.0004728582879751746,
      "loss": 6.6842,
      "step": 32
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 2.96875,
      "learning_rate": 0.00047068781411072687,
      "loss": 6.7111,
      "step": 33
    },
    {
      "epoch": 0.19624819624819625,
      "grad_norm": 2.765625,
      "learning_rate": 0.00046843924360644385,
      "loss": 6.6847,
      "step": 34
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 2.328125,
      "learning_rate": 0.00046611337218274864,
      "loss": 6.6925,
      "step": 35
    },
    {
      "epoch": 0.2077922077922078,
      "grad_norm": 3.25,
      "learning_rate": 0.0004637110229151863,
      "loss": 6.7241,
      "step": 36
    },
    {
      "epoch": 0.21356421356421357,
      "grad_norm": 2.515625,
      "learning_rate": 0.00046123304594315517,
      "loss": 6.7197,
      "step": 37
    },
    {
      "epoch": 0.21933621933621933,
      "grad_norm": 1.8828125,
      "learning_rate": 0.0004586803181690609,
      "loss": 6.7214,
      "step": 38
    },
    {
      "epoch": 0.22510822510822512,
      "grad_norm": 2.34375,
      "learning_rate": 0.0004560537429479998,
      "loss": 6.7052,
      "step": 39
    },
    {
      "epoch": 0.23088023088023088,
      "grad_norm": 2.34375,
      "learning_rate": 0.00045335424976808116,
      "loss": 6.7094,
      "step": 40
    },
    {
      "epoch": 0.23665223665223664,
      "grad_norm": 2.265625,
      "learning_rate": 0.0004505827939215009,
      "loss": 6.5937,
      "step": 41
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 2.78125,
      "learning_rate": 0.00044774035616648516,
      "loss": 6.6767,
      "step": 42
    },
    {
      "epoch": 0.2481962481962482,
      "grad_norm": 2.140625,
      "learning_rate": 0.0004448279423802207,
      "loss": 6.6795,
      "step": 43
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 3.484375,
      "learning_rate": 0.0004418465832028967,
      "loss": 6.6968,
      "step": 44
    },
    {
      "epoch": 0.2597402597402597,
      "grad_norm": 2.5625,
      "learning_rate": 0.00043879733367298404,
      "loss": 6.6949,
      "step": 45
    },
    {
      "epoch": 0.26551226551226553,
      "grad_norm": 5.03125,
      "learning_rate": 0.00043568127285387924,
      "loss": 6.7,
      "step": 46
    },
    {
      "epoch": 0.2712842712842713,
      "grad_norm": 4.71875,
      "learning_rate": 0.00043249950345204804,
      "loss": 6.6246,
      "step": 47
    },
    {
      "epoch": 0.27705627705627706,
      "grad_norm": 2.5,
      "learning_rate": 0.0004292531514268008,
      "loss": 6.6855,
      "step": 48
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 3.1875,
      "learning_rate": 0.00042594336559184035,
      "loss": 6.6632,
      "step": 49
    },
    {
      "epoch": 0.2886002886002886,
      "grad_norm": 2.75,
      "learning_rate": 0.0004225713172087216,
      "loss": 6.6428,
      "step": 50
    },
    {
      "epoch": 0.2943722943722944,
      "grad_norm": 3.046875,
      "learning_rate": 0.0004191381995723672,
      "loss": 6.6702,
      "step": 51
    },
    {
      "epoch": 0.30014430014430016,
      "grad_norm": 3.171875,
      "learning_rate": 0.00041564522758878654,
      "loss": 6.6048,
      "step": 52
    },
    {
      "epoch": 0.3059163059163059,
      "grad_norm": 2.015625,
      "learning_rate": 0.0004120936373451467,
      "loss": 6.7391,
      "step": 53
    },
    {
      "epoch": 0.3116883116883117,
      "grad_norm": 3.375,
      "learning_rate": 0.000408484685672348,
      "loss": 6.6858,
      "step": 54
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 2.921875,
      "learning_rate": 0.0004048196497002588,
      "loss": 6.6957,
      "step": 55
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 2.171875,
      "learning_rate": 0.0004010998264057667,
      "loss": 6.6525,
      "step": 56
    },
    {
      "epoch": 0.329004329004329,
      "grad_norm": 2.359375,
      "learning_rate": 0.0003973265321538069,
      "loss": 6.6019,
      "step": 57
    },
    {
      "epoch": 0.3347763347763348,
      "grad_norm": 2.015625,
      "learning_rate": 0.0003935011022315284,
      "loss": 6.5688,
      "step": 58
    },
    {
      "epoch": 0.34054834054834054,
      "grad_norm": 2.578125,
      "learning_rate": 0.00038962489037576583,
      "loss": 6.5635,
      "step": 59
    },
    {
      "epoch": 0.3463203463203463,
      "grad_norm": 2.671875,
      "learning_rate": 0.0003856992682939803,
      "loss": 6.6189,
      "step": 60
    },
    {
      "epoch": 0.35209235209235207,
      "grad_norm": 2.078125,
      "learning_rate": 0.0003817256251788425,
      "loss": 6.5839,
      "step": 61
    },
    {
      "epoch": 0.3578643578643579,
      "grad_norm": 2.40625,
      "learning_rate": 0.00037770536721662694,
      "loss": 6.6382,
      "step": 62
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 2.171875,
      "learning_rate": 0.0003736399170895938,
      "loss": 6.6222,
      "step": 63
    },
    {
      "epoch": 0.3694083694083694,
      "grad_norm": 1.921875,
      "learning_rate": 0.0003695307134725316,
      "loss": 6.6386,
      "step": 64
    },
    {
      "epoch": 0.37518037518037517,
      "grad_norm": 2.390625,
      "learning_rate": 0.0003653792105236422,
      "loss": 6.5938,
      "step": 65
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 1.8203125,
      "learning_rate": 0.00036118687736994487,
      "loss": 6.6015,
      "step": 66
    },
    {
      "epoch": 0.38672438672438675,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0003569551975873847,
      "loss": 6.5784,
      "step": 67
    },
    {
      "epoch": 0.3924963924963925,
      "grad_norm": 1.8515625,
      "learning_rate": 0.00035268566867582683,
      "loss": 6.5555,
      "step": 68
    },
    {
      "epoch": 0.39826839826839827,
      "grad_norm": 2.109375,
      "learning_rate": 0.0003483798015291239,
      "loss": 6.6261,
      "step": 69
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 2.296875,
      "learning_rate": 0.00034403911990044307,
      "loss": 6.6111,
      "step": 70
    },
    {
      "epoch": 0.4098124098124098,
      "grad_norm": 2.140625,
      "learning_rate": 0.00033966515986304317,
      "loss": 6.6236,
      "step": 71
    },
    {
      "epoch": 0.4155844155844156,
      "grad_norm": 2.1875,
      "learning_rate": 0.0003352594692666915,
      "loss": 6.5994,
      "step": 72
    },
    {
      "epoch": 0.4213564213564214,
      "grad_norm": 2.4375,
      "learning_rate": 0.000330823607189913,
      "loss": 6.6165,
      "step": 73
    },
    {
      "epoch": 0.42712842712842713,
      "grad_norm": 1.9765625,
      "learning_rate": 0.0003263591433882666,
      "loss": 6.6758,
      "step": 74
    },
    {
      "epoch": 0.4329004329004329,
      "grad_norm": 1.90625,
      "learning_rate": 0.00032186765773884244,
      "loss": 6.5715,
      "step": 75
    },
    {
      "epoch": 0.43867243867243866,
      "grad_norm": 2.140625,
      "learning_rate": 0.0003173507396811774,
      "loss": 6.639,
      "step": 76
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 2.125,
      "learning_rate": 0.00031280998765478727,
      "loss": 6.6811,
      "step": 77
    },
    {
      "epoch": 0.45021645021645024,
      "grad_norm": 2.21875,
      "learning_rate": 0.0003082470085335133,
      "loss": 6.6396,
      "step": 78
    },
    {
      "epoch": 0.455988455988456,
      "grad_norm": 2.0625,
      "learning_rate": 0.00030366341705688466,
      "loss": 6.6685,
      "step": 79
    },
    {
      "epoch": 0.46176046176046176,
      "grad_norm": 2.1875,
      "learning_rate": 0.0002990608352586965,
      "loss": 6.5816,
      "step": 80
    },
    {
      "epoch": 0.4675324675324675,
      "grad_norm": 2.09375,
      "learning_rate": 0.00029444089189300783,
      "loss": 6.6283,
      "step": 81
    },
    {
      "epoch": 0.4733044733044733,
      "grad_norm": 2.03125,
      "learning_rate": 0.00028980522185776065,
      "loss": 6.6205,
      "step": 82
    },
    {
      "epoch": 0.4790764790764791,
      "grad_norm": 2.0625,
      "learning_rate": 0.00028515546561622466,
      "loss": 6.6157,
      "step": 83
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 2.046875,
      "learning_rate": 0.000280493268616473,
      "loss": 6.5887,
      "step": 84
    },
    {
      "epoch": 0.4906204906204906,
      "grad_norm": 1.8046875,
      "learning_rate": 0.0002758202807090941,
      "loss": 6.6522,
      "step": 85
    },
    {
      "epoch": 0.4963924963924964,
      "grad_norm": 1.8359375,
      "learning_rate": 0.00027113815556334474,
      "loss": 6.653,
      "step": 86
    },
    {
      "epoch": 0.5021645021645021,
      "grad_norm": 2.171875,
      "learning_rate": 0.00026644855008195267,
      "loss": 6.5304,
      "step": 87
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 1.9375,
      "learning_rate": 0.0002617531238147744,
      "loss": 6.6143,
      "step": 88
    },
    {
      "epoch": 0.5137085137085137,
      "grad_norm": 1.9921875,
      "learning_rate": 0.0002570535383715165,
      "loss": 6.612,
      "step": 89
    },
    {
      "epoch": 0.5194805194805194,
      "grad_norm": 1.7421875,
      "learning_rate": 0.0002523514568337281,
      "loss": 6.6439,
      "step": 90
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 2.09375,
      "learning_rate": 0.000247648543166272,
      "loss": 6.5787,
      "step": 91
    },
    {
      "epoch": 0.5310245310245311,
      "grad_norm": 1.7578125,
      "learning_rate": 0.00024294646162848353,
      "loss": 6.5798,
      "step": 92
    },
    {
      "epoch": 0.5367965367965368,
      "grad_norm": 2.046875,
      "learning_rate": 0.00023824687618522567,
      "loss": 6.6322,
      "step": 93
    },
    {
      "epoch": 0.5425685425685426,
      "grad_norm": 1.75,
      "learning_rate": 0.00023355144991804737,
      "loss": 6.6376,
      "step": 94
    },
    {
      "epoch": 0.5483405483405484,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00022886184443665522,
      "loss": 6.6346,
      "step": 95
    },
    {
      "epoch": 0.5541125541125541,
      "grad_norm": 1.8828125,
      "learning_rate": 0.0002241797192909059,
      "loss": 6.5997,
      "step": 96
    },
    {
      "epoch": 0.5598845598845599,
      "grad_norm": 1.796875,
      "learning_rate": 0.000219506731383527,
      "loss": 6.5961,
      "step": 97
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 1.90625,
      "learning_rate": 0.0002148445343837755,
      "loss": 6.5533,
      "step": 98
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.6796875,
      "learning_rate": 0.00021019477814223942,
      "loss": 6.577,
      "step": 99
    },
    {
      "epoch": 0.5772005772005772,
      "grad_norm": 1.984375,
      "learning_rate": 0.0002055591081069922,
      "loss": 6.6141,
      "step": 100
    },
    {
      "epoch": 0.5829725829725829,
      "grad_norm": 1.8125,
      "learning_rate": 0.00020093916474130352,
      "loss": 6.6282,
      "step": 101
    },
    {
      "epoch": 0.5887445887445888,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00019633658294311535,
      "loss": 6.6839,
      "step": 102
    },
    {
      "epoch": 0.5945165945165946,
      "grad_norm": 2.1875,
      "learning_rate": 0.0001917529914664867,
      "loss": 6.5879,
      "step": 103
    },
    {
      "epoch": 0.6002886002886003,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00018719001234521283,
      "loss": 6.5692,
      "step": 104
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.765625,
      "learning_rate": 0.00018264926031882274,
      "loss": 6.5554,
      "step": 105
    },
    {
      "epoch": 0.6118326118326118,
      "grad_norm": 1.7421875,
      "learning_rate": 0.00017813234226115766,
      "loss": 6.5859,
      "step": 106
    },
    {
      "epoch": 0.6176046176046176,
      "grad_norm": 2.015625,
      "learning_rate": 0.00017364085661173345,
      "loss": 6.6494,
      "step": 107
    },
    {
      "epoch": 0.6233766233766234,
      "grad_norm": 1.8671875,
      "learning_rate": 0.000169176392810087,
      "loss": 6.4811,
      "step": 108
    },
    {
      "epoch": 0.6291486291486291,
      "grad_norm": 1.875,
      "learning_rate": 0.0001647405307333085,
      "loss": 6.5905,
      "step": 109
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 1.7734375,
      "learning_rate": 0.00016033484013695687,
      "loss": 6.5497,
      "step": 110
    },
    {
      "epoch": 0.6406926406926406,
      "grad_norm": 1.375,
      "learning_rate": 0.00015596088009955694,
      "loss": 6.5985,
      "step": 111
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00015162019847087617,
      "loss": 6.4815,
      "step": 112
    },
    {
      "epoch": 0.6522366522366523,
      "grad_norm": 1.8203125,
      "learning_rate": 0.00014731433132417315,
      "loss": 6.5801,
      "step": 113
    },
    {
      "epoch": 0.658008658008658,
      "grad_norm": 1.59375,
      "learning_rate": 0.00014304480241261527,
      "loss": 6.5567,
      "step": 114
    },
    {
      "epoch": 0.6637806637806638,
      "grad_norm": 1.828125,
      "learning_rate": 0.0001388131226300552,
      "loss": 6.5678,
      "step": 115
    },
    {
      "epoch": 0.6695526695526696,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0001346207894763578,
      "loss": 6.6041,
      "step": 116
    },
    {
      "epoch": 0.6753246753246753,
      "grad_norm": 1.671875,
      "learning_rate": 0.0001304692865274683,
      "loss": 6.6355,
      "step": 117
    },
    {
      "epoch": 0.6810966810966811,
      "grad_norm": 1.875,
      "learning_rate": 0.0001263600829104062,
      "loss": 6.5576,
      "step": 118
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 1.859375,
      "learning_rate": 0.00012229463278337307,
      "loss": 6.5773,
      "step": 119
    },
    {
      "epoch": 0.6926406926406926,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00011827437482115758,
      "loss": 6.6424,
      "step": 120
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 1.859375,
      "learning_rate": 0.00011430073170601968,
      "loss": 6.6601,
      "step": 121
    },
    {
      "epoch": 0.7041847041847041,
      "grad_norm": 1.6875,
      "learning_rate": 0.00011037510962423425,
      "loss": 6.5699,
      "step": 122
    },
    {
      "epoch": 0.70995670995671,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0001064988977684716,
      "loss": 6.63,
      "step": 123
    },
    {
      "epoch": 0.7157287157287158,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00010267346784619325,
      "loss": 6.6046,
      "step": 124
    },
    {
      "epoch": 0.7215007215007215,
      "grad_norm": 1.8828125,
      "learning_rate": 9.890017359423326e-05,
      "loss": 6.5946,
      "step": 125
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.640625,
      "learning_rate": 9.518035029974126e-05,
      "loss": 6.6103,
      "step": 126
    },
    {
      "epoch": 0.733044733044733,
      "grad_norm": 1.5546875,
      "learning_rate": 9.151531432765204e-05,
      "loss": 6.5237,
      "step": 127
    },
    {
      "epoch": 0.7388167388167388,
      "grad_norm": 1.296875,
      "learning_rate": 8.790636265485333e-05,
      "loss": 6.5325,
      "step": 128
    },
    {
      "epoch": 0.7445887445887446,
      "grad_norm": 1.453125,
      "learning_rate": 8.435477241121353e-05,
      "loss": 6.5076,
      "step": 129
    },
    {
      "epoch": 0.7503607503607503,
      "grad_norm": 1.3828125,
      "learning_rate": 8.086180042763284e-05,
      "loss": 6.5253,
      "step": 130
    },
    {
      "epoch": 0.7561327561327561,
      "grad_norm": 1.7578125,
      "learning_rate": 7.742868279127849e-05,
      "loss": 6.58,
      "step": 131
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 1.40625,
      "learning_rate": 7.405663440815968e-05,
      "loss": 6.5364,
      "step": 132
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 1.46875,
      "learning_rate": 7.074684857319927e-05,
      "loss": 6.6165,
      "step": 133
    },
    {
      "epoch": 0.7734487734487735,
      "grad_norm": 1.328125,
      "learning_rate": 6.750049654795198e-05,
      "loss": 6.5933,
      "step": 134
    },
    {
      "epoch": 0.7792207792207793,
      "grad_norm": 1.4296875,
      "learning_rate": 6.431872714612072e-05,
      "loss": 6.6163,
      "step": 135
    },
    {
      "epoch": 0.784992784992785,
      "grad_norm": 1.46875,
      "learning_rate": 6.120266632701598e-05,
      "loss": 6.5836,
      "step": 136
    },
    {
      "epoch": 0.7907647907647908,
      "grad_norm": 1.484375,
      "learning_rate": 5.815341679710326e-05,
      "loss": 6.563,
      "step": 137
    },
    {
      "epoch": 0.7965367965367965,
      "grad_norm": 1.46875,
      "learning_rate": 5.517205761977939e-05,
      "loss": 6.5757,
      "step": 138
    },
    {
      "epoch": 0.8023088023088023,
      "grad_norm": 1.359375,
      "learning_rate": 5.225964383351489e-05,
      "loss": 6.613,
      "step": 139
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.7265625,
      "learning_rate": 4.941720607849912e-05,
      "loss": 6.5185,
      "step": 140
    },
    {
      "epoch": 0.8138528138528138,
      "grad_norm": 1.359375,
      "learning_rate": 4.664575023191886e-05,
      "loss": 6.5536,
      "step": 141
    },
    {
      "epoch": 0.8196248196248196,
      "grad_norm": 1.34375,
      "learning_rate": 4.394625705200012e-05,
      "loss": 6.5345,
      "step": 142
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 1.578125,
      "learning_rate": 4.131968183093912e-05,
      "loss": 6.563,
      "step": 143
    },
    {
      "epoch": 0.8311688311688312,
      "grad_norm": 1.4765625,
      "learning_rate": 3.876695405684485e-05,
      "loss": 6.5664,
      "step": 144
    },
    {
      "epoch": 0.836940836940837,
      "grad_norm": 1.5546875,
      "learning_rate": 3.628897708481377e-05,
      "loss": 6.576,
      "step": 145
    },
    {
      "epoch": 0.8427128427128427,
      "grad_norm": 1.6328125,
      "learning_rate": 3.388662781725141e-05,
      "loss": 6.5539,
      "step": 146
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 1.3828125,
      "learning_rate": 3.1560756393556184e-05,
      "loss": 6.5534,
      "step": 147
    },
    {
      "epoch": 0.8542568542568543,
      "grad_norm": 1.5859375,
      "learning_rate": 2.9312185889273145e-05,
      "loss": 6.5411,
      "step": 148
    },
    {
      "epoch": 0.86002886002886,
      "grad_norm": 1.4140625,
      "learning_rate": 2.7141712024825378e-05,
      "loss": 6.6048,
      "step": 149
    },
    {
      "epoch": 0.8658008658008658,
      "grad_norm": 1.5859375,
      "learning_rate": 2.505010288392587e-05,
      "loss": 6.5132,
      "step": 150
    },
    {
      "epoch": 0.8715728715728716,
      "grad_norm": 1.3359375,
      "learning_rate": 2.3038098641769088e-05,
      "loss": 6.6281,
      "step": 151
    },
    {
      "epoch": 0.8773448773448773,
      "grad_norm": 1.3125,
      "learning_rate": 2.1106411303099453e-05,
      "loss": 6.556,
      "step": 152
    },
    {
      "epoch": 0.8831168831168831,
      "grad_norm": 1.2890625,
      "learning_rate": 1.9255724450247676e-05,
      "loss": 6.5808,
      "step": 153
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.3984375,
      "learning_rate": 1.7486693001226267e-05,
      "loss": 6.5759,
      "step": 154
    },
    {
      "epoch": 0.8946608946608947,
      "grad_norm": 1.2890625,
      "learning_rate": 1.579994297796808e-05,
      "loss": 6.6188,
      "step": 155
    },
    {
      "epoch": 0.9004329004329005,
      "grad_norm": 1.6328125,
      "learning_rate": 1.4196071284790529e-05,
      "loss": 6.5666,
      "step": 156
    },
    {
      "epoch": 0.9062049062049062,
      "grad_norm": 1.2109375,
      "learning_rate": 1.2675645497164351e-05,
      "loss": 6.5222,
      "step": 157
    },
    {
      "epoch": 0.911976911976912,
      "grad_norm": 1.34375,
      "learning_rate": 1.1239203660860647e-05,
      "loss": 6.5498,
      "step": 158
    },
    {
      "epoch": 0.9177489177489178,
      "grad_norm": 1.25,
      "learning_rate": 9.88725410154842e-06,
      "loss": 6.6006,
      "step": 159
    },
    {
      "epoch": 0.9235209235209235,
      "grad_norm": 1.34375,
      "learning_rate": 8.620275244908826e-06,
      "loss": 6.5603,
      "step": 160
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 1.421875,
      "learning_rate": 7.438715447331018e-06,
      "loss": 6.5345,
      "step": 161
    },
    {
      "epoch": 0.935064935064935,
      "grad_norm": 1.3515625,
      "learning_rate": 6.342992837248235e-06,
      "loss": 6.5845,
      "step": 162
    },
    {
      "epoch": 0.9408369408369408,
      "grad_norm": 1.328125,
      "learning_rate": 5.333495167171354e-06,
      "loss": 6.5696,
      "step": 163
    },
    {
      "epoch": 0.9466089466089466,
      "grad_norm": 1.296875,
      "learning_rate": 4.410579676471571e-06,
      "loss": 6.515,
      "step": 164
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 1.5234375,
      "learning_rate": 3.5745729649613034e-06,
      "loss": 6.4954,
      "step": 165
    },
    {
      "epoch": 0.9581529581529582,
      "grad_norm": 1.3125,
      "learning_rate": 2.8257708773173627e-06,
      "loss": 6.5282,
      "step": 166
    },
    {
      "epoch": 0.963924963924964,
      "grad_norm": 1.4140625,
      "learning_rate": 2.1644383983880357e-06,
      "loss": 6.5604,
      "step": 167
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.5078125,
      "learning_rate": 1.5908095594207582e-06,
      "loss": 6.5378,
      "step": 168
    },
    {
      "epoch": 0.9754689754689755,
      "grad_norm": 1.2734375,
      "learning_rate": 1.1050873552433394e-06,
      "loss": 6.4667,
      "step": 169
    },
    {
      "epoch": 0.9812409812409812,
      "grad_norm": 1.2265625,
      "learning_rate": 7.074436724286704e-07,
      "loss": 6.5453,
      "step": 170
    },
    {
      "epoch": 0.987012987012987,
      "grad_norm": 1.234375,
      "learning_rate": 3.9801922846766094e-07,
      "loss": 6.5909,
      "step": 171
    },
    {
      "epoch": 0.9927849927849928,
      "grad_norm": 1.515625,
      "learning_rate": 1.7692352197240524e-07,
      "loss": 6.5373,
      "step": 172
    },
    {
      "epoch": 0.9985569985569985,
      "grad_norm": 1.3671875,
      "learning_rate": 4.423479392709484e-08,
      "loss": 6.574,
      "step": 173
    },
    {
      "epoch": 1.0057720057720059,
      "grad_norm": 8.5625,
      "learning_rate": 0.0,
      "loss": 7.515,
      "step": 174
    },
    {
      "epoch": 1.0115440115440115,
      "grad_norm": 8.625,
      "learning_rate": 0.0002605469934405078,
      "loss": 7.5117,
      "step": 175
    },
    {
      "epoch": 1.0173160173160174,
      "grad_norm": 5.28125,
      "learning_rate": 0.0002582041791754375,
      "loss": 7.171,
      "step": 176
    },
    {
      "epoch": 1.023088023088023,
      "grad_norm": 6.0625,
      "learning_rate": 0.00025586064340081516,
      "loss": 6.9384,
      "step": 177
    },
    {
      "epoch": 1.028860028860029,
      "grad_norm": 7.75,
      "learning_rate": 0.00025351659221689896,
      "loss": 7.0699,
      "step": 178
    },
    {
      "epoch": 1.0346320346320346,
      "grad_norm": 8.9375,
      "learning_rate": 0.0002511722317692747,
      "loss": 7.0915,
      "step": 179
    },
    {
      "epoch": 1.0404040404040404,
      "grad_norm": 4.125,
      "learning_rate": 0.0002488277682307254,
      "loss": 6.925,
      "step": 180
    },
    {
      "epoch": 1.046176046176046,
      "grad_norm": 4.84375,
      "learning_rate": 0.00024648340778310105,
      "loss": 6.8892,
      "step": 181
    },
    {
      "epoch": 1.051948051948052,
      "grad_norm": 3.171875,
      "learning_rate": 0.0002441393565991849,
      "loss": 6.788,
      "step": 182
    },
    {
      "epoch": 1.0577200577200576,
      "grad_norm": 3.046875,
      "learning_rate": 0.00024179582082456253,
      "loss": 6.6563,
      "step": 183
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 2.53125,
      "learning_rate": 0.00023945300655949225,
      "loss": 6.6801,
      "step": 184
    },
    {
      "epoch": 1.0692640692640694,
      "grad_norm": 2.171875,
      "learning_rate": 0.00023711111984077966,
      "loss": 6.6396,
      "step": 185
    },
    {
      "epoch": 1.075036075036075,
      "grad_norm": 3.0625,
      "learning_rate": 0.00023477036662365828,
      "loss": 6.6516,
      "step": 186
    },
    {
      "epoch": 1.0808080808080809,
      "grad_norm": 2.703125,
      "learning_rate": 0.00023243095276367684,
      "loss": 6.5617,
      "step": 187
    },
    {
      "epoch": 1.0865800865800865,
      "grad_norm": 2.65625,
      "learning_rate": 0.00023009308399859506,
      "loss": 6.5745,
      "step": 188
    },
    {
      "epoch": 1.0923520923520924,
      "grad_norm": 2.53125,
      "learning_rate": 0.00022775696593029104,
      "loss": 6.5768,
      "step": 189
    },
    {
      "epoch": 1.098124098124098,
      "grad_norm": 1.875,
      "learning_rate": 0.00022542280400667918,
      "loss": 6.5561,
      "step": 190
    },
    {
      "epoch": 1.103896103896104,
      "grad_norm": 2.59375,
      "learning_rate": 0.00022309080350364253,
      "loss": 6.5321,
      "step": 191
    },
    {
      "epoch": 1.1096681096681096,
      "grad_norm": 1.828125,
      "learning_rate": 0.0002207611695069794,
      "loss": 6.5372,
      "step": 192
    },
    {
      "epoch": 1.1154401154401155,
      "grad_norm": 2.65625,
      "learning_rate": 0.00021843410689436824,
      "loss": 6.5829,
      "step": 193
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 1.9375,
      "learning_rate": 0.0002161098203173492,
      "loss": 6.5948,
      "step": 194
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 2.8125,
      "learning_rate": 0.000213788514183326,
      "loss": 6.5338,
      "step": 195
    },
    {
      "epoch": 1.1327561327561328,
      "grad_norm": 2.5625,
      "learning_rate": 0.00021147039263759028,
      "loss": 6.5508,
      "step": 196
    },
    {
      "epoch": 1.1385281385281385,
      "grad_norm": 2.390625,
      "learning_rate": 0.00020915565954536742,
      "loss": 6.6611,
      "step": 197
    },
    {
      "epoch": 1.1443001443001444,
      "grad_norm": 2.4375,
      "learning_rate": 0.0002068445184738886,
      "loss": 6.5816,
      "step": 198
    },
    {
      "epoch": 1.15007215007215,
      "grad_norm": 1.59375,
      "learning_rate": 0.00020453717267448717,
      "loss": 6.5754,
      "step": 199
    },
    {
      "epoch": 1.155844155844156,
      "grad_norm": 1.78125,
      "learning_rate": 0.00020223382506472505,
      "loss": 6.5046,
      "step": 200
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 2.90625,
      "learning_rate": 0.00019993467821054645,
      "loss": 6.5625,
      "step": 201
    },
    {
      "epoch": 1.1673881673881674,
      "grad_norm": 2.109375,
      "learning_rate": 0.00019763993430846395,
      "loss": 6.5264,
      "step": 202
    },
    {
      "epoch": 1.173160173160173,
      "grad_norm": 2.359375,
      "learning_rate": 0.000195349795167776,
      "loss": 6.5271,
      "step": 203
    },
    {
      "epoch": 1.178932178932179,
      "grad_norm": 1.9765625,
      "learning_rate": 0.0001930644621928194,
      "loss": 6.6052,
      "step": 204
    },
    {
      "epoch": 1.1847041847041848,
      "grad_norm": 2.03125,
      "learning_rate": 0.0001907841363652568,
      "loss": 6.6192,
      "step": 205
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 2.34375,
      "learning_rate": 0.00018850901822640146,
      "loss": 6.534,
      "step": 206
    },
    {
      "epoch": 1.1962481962481963,
      "grad_norm": 2.0,
      "learning_rate": 0.0001862393078595809,
      "loss": 6.5368,
      "step": 207
    },
    {
      "epoch": 1.202020202020202,
      "grad_norm": 2.34375,
      "learning_rate": 0.0001839752048725408,
      "loss": 6.4999,
      "step": 208
    },
    {
      "epoch": 1.2077922077922079,
      "grad_norm": 2.25,
      "learning_rate": 0.00018171690837989057,
      "loss": 6.5536,
      "step": 209
    },
    {
      "epoch": 1.2135642135642135,
      "grad_norm": 2.265625,
      "learning_rate": 0.00017946461698559237,
      "loss": 6.6195,
      "step": 210
    },
    {
      "epoch": 1.2193362193362194,
      "grad_norm": 2.390625,
      "learning_rate": 0.00017721852876549508,
      "loss": 6.6449,
      "step": 211
    },
    {
      "epoch": 1.225108225108225,
      "grad_norm": 2.0625,
      "learning_rate": 0.00017497884124991487,
      "loss": 6.5726,
      "step": 212
    },
    {
      "epoch": 1.230880230880231,
      "grad_norm": 2.421875,
      "learning_rate": 0.00017274575140626317,
      "loss": 6.5571,
      "step": 213
    },
    {
      "epoch": 1.2366522366522366,
      "grad_norm": 1.8203125,
      "learning_rate": 0.00017051945562172494,
      "loss": 6.538,
      "step": 214
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 1.96875,
      "learning_rate": 0.00016830014968598734,
      "loss": 6.5949,
      "step": 215
    },
    {
      "epoch": 1.248196248196248,
      "grad_norm": 2.734375,
      "learning_rate": 0.00016608802877402136,
      "loss": 6.5161,
      "step": 216
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 2.484375,
      "learning_rate": 0.00016388328742891677,
      "loss": 6.6057,
      "step": 217
    },
    {
      "epoch": 1.2597402597402598,
      "grad_norm": 2.828125,
      "learning_rate": 0.00016168611954477417,
      "loss": 6.6508,
      "step": 218
    },
    {
      "epoch": 1.2655122655122655,
      "grad_norm": 2.09375,
      "learning_rate": 0.00015949671834965222,
      "loss": 6.6131,
      "step": 219
    },
    {
      "epoch": 1.2712842712842713,
      "grad_norm": 2.03125,
      "learning_rate": 0.00015731527638857492,
      "loss": 6.5652,
      "step": 220
    },
    {
      "epoch": 1.277056277056277,
      "grad_norm": 2.390625,
      "learning_rate": 0.00015514198550659793,
      "loss": 6.5603,
      "step": 221
    },
    {
      "epoch": 1.2828282828282829,
      "grad_norm": 2.078125,
      "learning_rate": 0.00015297703683193753,
      "loss": 6.624,
      "step": 222
    },
    {
      "epoch": 1.2886002886002885,
      "grad_norm": 2.53125,
      "learning_rate": 0.00015082062075916165,
      "loss": 6.6092,
      "step": 223
    },
    {
      "epoch": 1.2943722943722944,
      "grad_norm": 2.9375,
      "learning_rate": 0.00014867292693244546,
      "loss": 6.5492,
      "step": 224
    },
    {
      "epoch": 1.3001443001443,
      "grad_norm": 1.9375,
      "learning_rate": 0.000146534144228894,
      "loss": 6.5904,
      "step": 225
    },
    {
      "epoch": 1.305916305916306,
      "grad_norm": 2.421875,
      "learning_rate": 0.00014440446074193099,
      "loss": 6.5825,
      "step": 226
    },
    {
      "epoch": 1.3116883116883118,
      "grad_norm": 1.7890625,
      "learning_rate": 0.00014228406376475743,
      "loss": 6.5896,
      "step": 227
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 2.0625,
      "learning_rate": 0.00014017313977387997,
      "loss": 6.5954,
      "step": 228
    },
    {
      "epoch": 1.3232323232323233,
      "grad_norm": 2.0,
      "learning_rate": 0.00013807187441271156,
      "loss": 6.581,
      "step": 229
    },
    {
      "epoch": 1.329004329004329,
      "grad_norm": 2.140625,
      "learning_rate": 0.00013598045247524554,
      "loss": 6.5491,
      "step": 230
    },
    {
      "epoch": 1.3347763347763348,
      "grad_norm": 2.125,
      "learning_rate": 0.00013389905788980294,
      "loss": 6.5959,
      "step": 231
    },
    {
      "epoch": 1.3405483405483405,
      "grad_norm": 2.5,
      "learning_rate": 0.00013182787370285865,
      "loss": 6.5811,
      "step": 232
    },
    {
      "epoch": 1.3463203463203464,
      "grad_norm": 2.453125,
      "learning_rate": 0.00012976708206294252,
      "loss": 6.5677,
      "step": 233
    },
    {
      "epoch": 1.352092352092352,
      "grad_norm": 2.046875,
      "learning_rate": 0.00012771686420462054,
      "loss": 6.5468,
      "step": 234
    },
    {
      "epoch": 1.3578643578643579,
      "grad_norm": 2.78125,
      "learning_rate": 0.0001256774004325565,
      "loss": 6.5936,
      "step": 235
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 1.9375,
      "learning_rate": 0.00012364887010565535,
      "loss": 6.5606,
      "step": 236
    },
    {
      "epoch": 1.3694083694083694,
      "grad_norm": 2.265625,
      "learning_rate": 0.00012163145162128947,
      "loss": 6.566,
      "step": 237
    },
    {
      "epoch": 1.375180375180375,
      "grad_norm": 1.859375,
      "learning_rate": 0.0001196253223996099,
      "loss": 6.5729,
      "step": 238
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 2.234375,
      "learning_rate": 0.00011763065886794258,
      "loss": 6.5909,
      "step": 239
    },
    {
      "epoch": 1.3867243867243868,
      "grad_norm": 2.40625,
      "learning_rate": 0.00011564763644527357,
      "loss": 6.5674,
      "step": 240
    },
    {
      "epoch": 1.3924963924963925,
      "grad_norm": 2.234375,
      "learning_rate": 0.00011367642952682153,
      "loss": 6.5942,
      "step": 241
    },
    {
      "epoch": 1.3982683982683983,
      "grad_norm": 2.53125,
      "learning_rate": 0.00011171721146870015,
      "loss": 6.5822,
      "step": 242
    },
    {
      "epoch": 1.404040404040404,
      "grad_norm": 1.875,
      "learning_rate": 0.00010977015457267365,
      "loss": 6.6091,
      "step": 243
    },
    {
      "epoch": 1.4098124098124099,
      "grad_norm": 2.1875,
      "learning_rate": 0.00010783543007100266,
      "loss": 6.6156,
      "step": 244
    },
    {
      "epoch": 1.4155844155844157,
      "grad_norm": 2.578125,
      "learning_rate": 0.00010591320811138636,
      "loss": 6.5313,
      "step": 245
    },
    {
      "epoch": 1.4213564213564214,
      "grad_norm": 2.375,
      "learning_rate": 0.00010400365774199818,
      "loss": 6.6181,
      "step": 246
    },
    {
      "epoch": 1.427128427128427,
      "grad_norm": 2.34375,
      "learning_rate": 0.0001021069468966194,
      "loss": 6.5653,
      "step": 247
    },
    {
      "epoch": 1.432900432900433,
      "grad_norm": 2.15625,
      "learning_rate": 0.00010022324237987047,
      "loss": 6.5127,
      "step": 248
    },
    {
      "epoch": 1.4386724386724388,
      "grad_norm": 2.046875,
      "learning_rate": 9.835270985254111e-05,
      "loss": 6.5402,
      "step": 249
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 2.09375,
      "learning_rate": 9.649551381702168e-05,
      "loss": 6.5357,
      "step": 250
    },
    {
      "epoch": 1.4502164502164503,
      "grad_norm": 2.15625,
      "learning_rate": 9.46518176028364e-05,
      "loss": 6.5874,
      "step": 251
    },
    {
      "epoch": 1.455988455988456,
      "grad_norm": 1.984375,
      "learning_rate": 9.282178335227883e-05,
      "loss": 6.6472,
      "step": 252
    },
    {
      "epoch": 1.4617604617604618,
      "grad_norm": 2.5,
      "learning_rate": 9.100557200615292e-05,
      "loss": 6.5945,
      "step": 253
    },
    {
      "epoch": 1.4675324675324675,
      "grad_norm": 2.15625,
      "learning_rate": 8.920334328961918e-05,
      "loss": 6.5183,
      "step": 254
    },
    {
      "epoch": 1.4733044733044733,
      "grad_norm": 2.625,
      "learning_rate": 8.74152556981474e-05,
      "loss": 6.5488,
      "step": 255
    },
    {
      "epoch": 1.479076479076479,
      "grad_norm": 2.515625,
      "learning_rate": 8.56414664835785e-05,
      "loss": 6.5861,
      "step": 256
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 2.203125,
      "learning_rate": 8.388213164029459e-05,
      "loss": 6.5291,
      "step": 257
    },
    {
      "epoch": 1.4906204906204907,
      "grad_norm": 1.9765625,
      "learning_rate": 8.213740589150032e-05,
      "loss": 6.5971,
      "step": 258
    },
    {
      "epoch": 1.4963924963924964,
      "grad_norm": 1.859375,
      "learning_rate": 8.040744267561637e-05,
      "loss": 6.5695,
      "step": 259
    },
    {
      "epoch": 1.502164502164502,
      "grad_norm": 2.03125,
      "learning_rate": 7.869239413278442e-05,
      "loss": 6.6123,
      "step": 260
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 1.984375,
      "learning_rate": 7.699241109148844e-05,
      "loss": 6.576,
      "step": 261
    },
    {
      "epoch": 1.5137085137085138,
      "grad_norm": 1.8828125,
      "learning_rate": 7.530764305528959e-05,
      "loss": 6.5618,
      "step": 262
    },
    {
      "epoch": 1.5194805194805194,
      "grad_norm": 1.9375,
      "learning_rate": 7.363823818967824e-05,
      "loss": 6.5574,
      "step": 263
    },
    {
      "epoch": 1.5252525252525253,
      "grad_norm": 2.03125,
      "learning_rate": 7.198434330904388e-05,
      "loss": 6.5206,
      "step": 264
    },
    {
      "epoch": 1.531024531024531,
      "grad_norm": 2.0625,
      "learning_rate": 7.034610386376342e-05,
      "loss": 6.5947,
      "step": 265
    },
    {
      "epoch": 1.5367965367965368,
      "grad_norm": 1.7109375,
      "learning_rate": 6.872366392741017e-05,
      "loss": 6.5153,
      "step": 266
    },
    {
      "epoch": 1.5425685425685427,
      "grad_norm": 1.8125,
      "learning_rate": 6.711716618408281e-05,
      "loss": 6.5049,
      "step": 267
    },
    {
      "epoch": 1.5483405483405484,
      "grad_norm": 1.9296875,
      "learning_rate": 6.552675191585741e-05,
      "loss": 6.5419,
      "step": 268
    },
    {
      "epoch": 1.554112554112554,
      "grad_norm": 1.8984375,
      "learning_rate": 6.395256099036278e-05,
      "loss": 6.5798,
      "step": 269
    },
    {
      "epoch": 1.5598845598845599,
      "grad_norm": 2.09375,
      "learning_rate": 6.239473184847941e-05,
      "loss": 6.51,
      "step": 270
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 1.84375,
      "learning_rate": 6.085340149216467e-05,
      "loss": 6.5569,
      "step": 271
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.6875,
      "learning_rate": 5.9328705472404546e-05,
      "loss": 6.5179,
      "step": 272
    },
    {
      "epoch": 1.577200577200577,
      "grad_norm": 2.140625,
      "learning_rate": 5.7820777877292065e-05,
      "loss": 6.6219,
      "step": 273
    },
    {
      "epoch": 1.582972582972583,
      "grad_norm": 2.109375,
      "learning_rate": 5.632975132023585e-05,
      "loss": 6.5794,
      "step": 274
    },
    {
      "epoch": 1.5887445887445888,
      "grad_norm": 2.1875,
      "learning_rate": 5.485575692829678e-05,
      "loss": 6.5941,
      "step": 275
    },
    {
      "epoch": 1.5945165945165947,
      "grad_norm": 1.96875,
      "learning_rate": 5.339892433065654e-05,
      "loss": 6.5628,
      "step": 276
    },
    {
      "epoch": 1.6002886002886003,
      "grad_norm": 1.78125,
      "learning_rate": 5.195938164721767e-05,
      "loss": 6.5675,
      "step": 277
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 1.96875,
      "learning_rate": 5.0537255477335644e-05,
      "loss": 6.5478,
      "step": 278
    },
    {
      "epoch": 1.6118326118326118,
      "grad_norm": 1.9140625,
      "learning_rate": 4.913267088868553e-05,
      "loss": 6.4471,
      "step": 279
    },
    {
      "epoch": 1.6176046176046177,
      "grad_norm": 1.765625,
      "learning_rate": 4.7745751406263163e-05,
      "loss": 6.5362,
      "step": 280
    },
    {
      "epoch": 1.6233766233766234,
      "grad_norm": 2.015625,
      "learning_rate": 4.637661900152143e-05,
      "loss": 6.5349,
      "step": 281
    },
    {
      "epoch": 1.629148629148629,
      "grad_norm": 1.8203125,
      "learning_rate": 4.5025394081643854e-05,
      "loss": 6.522,
      "step": 282
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 1.828125,
      "learning_rate": 4.3692195478955615e-05,
      "loss": 6.5874,
      "step": 283
    },
    {
      "epoch": 1.6406926406926408,
      "grad_norm": 1.7890625,
      "learning_rate": 4.237714044047258e-05,
      "loss": 6.5525,
      "step": 284
    },
    {
      "epoch": 1.6464646464646466,
      "grad_norm": 2.328125,
      "learning_rate": 4.108034461759036e-05,
      "loss": 6.4984,
      "step": 285
    },
    {
      "epoch": 1.6522366522366523,
      "grad_norm": 1.6953125,
      "learning_rate": 3.980192205591354e-05,
      "loss": 6.5603,
      "step": 286
    },
    {
      "epoch": 1.658008658008658,
      "grad_norm": 1.7265625,
      "learning_rate": 3.8541985185225645e-05,
      "loss": 6.5437,
      "step": 287
    },
    {
      "epoch": 1.6637806637806638,
      "grad_norm": 1.7265625,
      "learning_rate": 3.7300644809602155e-05,
      "loss": 6.574,
      "step": 288
    },
    {
      "epoch": 1.6695526695526697,
      "grad_norm": 1.5390625,
      "learning_rate": 3.6078010097665206e-05,
      "loss": 6.5346,
      "step": 289
    },
    {
      "epoch": 1.6753246753246753,
      "grad_norm": 1.7109375,
      "learning_rate": 3.487418857298366e-05,
      "loss": 6.5368,
      "step": 290
    },
    {
      "epoch": 1.681096681096681,
      "grad_norm": 1.7578125,
      "learning_rate": 3.368928610461652e-05,
      "loss": 6.4971,
      "step": 291
    },
    {
      "epoch": 1.6868686868686869,
      "grad_norm": 1.6875,
      "learning_rate": 3.2523406897802446e-05,
      "loss": 6.577,
      "step": 292
    },
    {
      "epoch": 1.6926406926406927,
      "grad_norm": 1.7421875,
      "learning_rate": 3.1376653484795545e-05,
      "loss": 6.5365,
      "step": 293
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 1.515625,
      "learning_rate": 3.0249126715848258e-05,
      "loss": 6.5716,
      "step": 294
    },
    {
      "epoch": 1.704184704184704,
      "grad_norm": 1.5,
      "learning_rate": 2.9140925750342357e-05,
      "loss": 6.5724,
      "step": 295
    },
    {
      "epoch": 1.70995670995671,
      "grad_norm": 1.671875,
      "learning_rate": 2.8052148048068076e-05,
      "loss": 6.5226,
      "step": 296
    },
    {
      "epoch": 1.7157287157287158,
      "grad_norm": 1.703125,
      "learning_rate": 2.698288936065338e-05,
      "loss": 6.4586,
      "step": 297
    },
    {
      "epoch": 1.7215007215007216,
      "grad_norm": 1.578125,
      "learning_rate": 2.593324372314318e-05,
      "loss": 6.5387,
      "step": 298
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 1.703125,
      "learning_rate": 2.4903303445729276e-05,
      "loss": 6.4844,
      "step": 299
    },
    {
      "epoch": 1.733044733044733,
      "grad_norm": 1.8515625,
      "learning_rate": 2.3893159105632362e-05,
      "loss": 6.5753,
      "step": 300
    },
    {
      "epoch": 1.7388167388167388,
      "grad_norm": 2.0,
      "learning_rate": 2.2902899539136435e-05,
      "loss": 6.5192,
      "step": 301
    },
    {
      "epoch": 1.7445887445887447,
      "grad_norm": 1.5546875,
      "learning_rate": 2.1932611833775846e-05,
      "loss": 6.5158,
      "step": 302
    },
    {
      "epoch": 1.7503607503607503,
      "grad_norm": 1.640625,
      "learning_rate": 2.0982381320676647e-05,
      "loss": 6.5209,
      "step": 303
    },
    {
      "epoch": 1.756132756132756,
      "grad_norm": 1.609375,
      "learning_rate": 2.0052291567052295e-05,
      "loss": 6.5411,
      "step": 304
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 1.6484375,
      "learning_rate": 1.9142424368854162e-05,
      "loss": 6.5177,
      "step": 305
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 1.6640625,
      "learning_rate": 1.825285974357835e-05,
      "loss": 6.4915,
      "step": 306
    },
    {
      "epoch": 1.7734487734487736,
      "grad_norm": 1.5859375,
      "learning_rate": 1.738367592322837e-05,
      "loss": 6.5313,
      "step": 307
    },
    {
      "epoch": 1.7792207792207793,
      "grad_norm": 1.6640625,
      "learning_rate": 1.6534949347435185e-05,
      "loss": 6.5527,
      "step": 308
    },
    {
      "epoch": 1.784992784992785,
      "grad_norm": 1.3515625,
      "learning_rate": 1.5706754656734908e-05,
      "loss": 6.5485,
      "step": 309
    },
    {
      "epoch": 1.7907647907647908,
      "grad_norm": 1.65625,
      "learning_rate": 1.4899164686004413e-05,
      "loss": 6.4998,
      "step": 310
    },
    {
      "epoch": 1.7965367965367967,
      "grad_norm": 1.4296875,
      "learning_rate": 1.4112250458055975e-05,
      "loss": 6.5866,
      "step": 311
    },
    {
      "epoch": 1.8023088023088023,
      "grad_norm": 1.515625,
      "learning_rate": 1.3346081177391473e-05,
      "loss": 6.5096,
      "step": 312
    },
    {
      "epoch": 1.808080808080808,
      "grad_norm": 1.5390625,
      "learning_rate": 1.2600724224115845e-05,
      "loss": 6.5295,
      "step": 313
    },
    {
      "epoch": 1.8138528138528138,
      "grad_norm": 1.59375,
      "learning_rate": 1.1876245148011694e-05,
      "loss": 6.4816,
      "step": 314
    },
    {
      "epoch": 1.8196248196248197,
      "grad_norm": 1.375,
      "learning_rate": 1.1172707662774561e-05,
      "loss": 6.5349,
      "step": 315
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 1.609375,
      "learning_rate": 1.0490173640409468e-05,
      "loss": 6.4735,
      "step": 316
    },
    {
      "epoch": 1.8311688311688312,
      "grad_norm": 1.5390625,
      "learning_rate": 9.828703105789983e-06,
      "loss": 6.466,
      "step": 317
    },
    {
      "epoch": 1.8369408369408369,
      "grad_norm": 1.625,
      "learning_rate": 9.188354231378899e-06,
      "loss": 6.5227,
      "step": 318
    },
    {
      "epoch": 1.8427128427128427,
      "grad_norm": 1.5390625,
      "learning_rate": 8.569183332112846e-06,
      "loss": 6.4376,
      "step": 319
    },
    {
      "epoch": 1.8484848484848486,
      "grad_norm": 1.4375,
      "learning_rate": 7.971244860449395e-06,
      "loss": 6.5551,
      "step": 320
    },
    {
      "epoch": 1.8542568542568543,
      "grad_norm": 1.421875,
      "learning_rate": 7.394591401578166e-06,
      "loss": 6.4643,
      "step": 321
    },
    {
      "epoch": 1.86002886002886,
      "grad_norm": 1.484375,
      "learning_rate": 6.839273668796747e-06,
      "loss": 6.4841,
      "step": 322
    },
    {
      "epoch": 1.8658008658008658,
      "grad_norm": 1.4609375,
      "learning_rate": 6.3053404990502384e-06,
      "loss": 6.5698,
      "step": 323
    },
    {
      "epoch": 1.8715728715728717,
      "grad_norm": 1.4609375,
      "learning_rate": 5.7928388486366555e-06,
      "loss": 6.5065,
      "step": 324
    },
    {
      "epoch": 1.8773448773448773,
      "grad_norm": 1.796875,
      "learning_rate": 5.301813789077264e-06,
      "loss": 6.5017,
      "step": 325
    },
    {
      "epoch": 1.883116883116883,
      "grad_norm": 1.484375,
      "learning_rate": 4.832308503152832e-06,
      "loss": 6.4953,
      "step": 326
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.515625,
      "learning_rate": 4.384364281105973e-06,
      "loss": 6.4956,
      "step": 327
    },
    {
      "epoch": 1.8946608946608947,
      "grad_norm": 1.5078125,
      "learning_rate": 3.9580205170098856e-06,
      "loss": 6.5174,
      "step": 328
    },
    {
      "epoch": 1.9004329004329006,
      "grad_norm": 1.5546875,
      "learning_rate": 3.553314705303845e-06,
      "loss": 6.4857,
      "step": 329
    },
    {
      "epoch": 1.9062049062049062,
      "grad_norm": 1.546875,
      "learning_rate": 3.1702824374959527e-06,
      "loss": 6.606,
      "step": 330
    },
    {
      "epoch": 1.9119769119769119,
      "grad_norm": 1.6328125,
      "learning_rate": 2.8089573990328076e-06,
      "loss": 6.5643,
      "step": 331
    },
    {
      "epoch": 1.9177489177489178,
      "grad_norm": 1.484375,
      "learning_rate": 2.469371366337264e-06,
      "loss": 6.4998,
      "step": 332
    },
    {
      "epoch": 1.9235209235209236,
      "grad_norm": 1.4765625,
      "learning_rate": 2.1515542040138335e-06,
      "loss": 6.4988,
      "step": 333
    },
    {
      "epoch": 1.9292929292929293,
      "grad_norm": 1.6171875,
      "learning_rate": 1.8555338622222583e-06,
      "loss": 6.5219,
      "step": 334
    },
    {
      "epoch": 1.935064935064935,
      "grad_norm": 1.546875,
      "learning_rate": 1.581336374219422e-06,
      "loss": 6.5464,
      "step": 335
    },
    {
      "epoch": 1.9408369408369408,
      "grad_norm": 1.5,
      "learning_rate": 1.3289858540699584e-06,
      "loss": 6.5141,
      "step": 336
    },
    {
      "epoch": 1.9466089466089467,
      "grad_norm": 1.4921875,
      "learning_rate": 1.0985044945254763e-06,
      "loss": 6.5803,
      "step": 337
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 1.5,
      "learning_rate": 8.899125650729256e-07,
      "loss": 6.5226,
      "step": 338
    },
    {
      "epoch": 1.9581529581529582,
      "grad_norm": 1.6171875,
      "learning_rate": 7.032284101518849e-07,
      "loss": 6.5351,
      "step": 339
    },
    {
      "epoch": 1.9639249639249639,
      "grad_norm": 1.484375,
      "learning_rate": 5.384684475414625e-07,
      "loss": 6.5689,
      "step": 340
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 1.53125,
      "learning_rate": 3.9564716691622984e-07,
      "loss": 6.4818,
      "step": 341
    },
    {
      "epoch": 1.9754689754689756,
      "grad_norm": 1.578125,
      "learning_rate": 2.7477712857215677e-07,
      "loss": 6.5384,
      "step": 342
    },
    {
      "epoch": 1.9812409812409812,
      "grad_norm": 1.4765625,
      "learning_rate": 1.7586896232180128e-07,
      "loss": 6.4657,
      "step": 343
    },
    {
      "epoch": 1.987012987012987,
      "grad_norm": 1.4375,
      "learning_rate": 9.89313665596403e-08,
      "loss": 6.5562,
      "step": 344
    },
    {
      "epoch": 1.9927849927849928,
      "grad_norm": 1.46875,
      "learning_rate": 4.3971107497042806e-08,
      "loss": 6.5014,
      "step": 345
    },
    {
      "epoch": 1.9985569985569986,
      "grad_norm": 1.53125,
      "learning_rate": 1.0993018567162505e-08,
      "loss": 6.5472,
      "step": 346
    }
  ],
  "logging_steps": 1,
  "max_steps": 346,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.35988519108608e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9985569985569985,
  "eval_steps": 500,
  "global_step": 173,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005772005772005772,
      "grad_norm": 14.5625,
      "learning_rate": 0.0,
      "loss": 9.9829,
      "step": 1
    },
    {
      "epoch": 0.011544011544011544,
      "grad_norm": 14.125,
      "learning_rate": 8.333333333333333e-05,
      "loss": 10.0735,
      "step": 2
    },
    {
      "epoch": 0.017316017316017316,
      "grad_norm": 10.375,
      "learning_rate": 0.00016666666666666666,
      "loss": 9.2063,
      "step": 3
    },
    {
      "epoch": 0.023088023088023088,
      "grad_norm": 30.0,
      "learning_rate": 0.00025,
      "loss": 9.8324,
      "step": 4
    },
    {
      "epoch": 0.02886002886002886,
      "grad_norm": 51.5,
      "learning_rate": 0.0003333333333333333,
      "loss": 11.5662,
      "step": 5
    },
    {
      "epoch": 0.03463203463203463,
      "grad_norm": 61.25,
      "learning_rate": 0.0004166666666666667,
      "loss": 12.038,
      "step": 6
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 18.875,
      "learning_rate": 0.0005,
      "loss": 9.3774,
      "step": 7
    },
    {
      "epoch": 0.046176046176046176,
      "grad_norm": 8.625,
      "learning_rate": 0.0004999557652060729,
      "loss": 9.5477,
      "step": 8
    },
    {
      "epoch": 0.05194805194805195,
      "grad_norm": 31.625,
      "learning_rate": 0.0004998230764780276,
      "loss": 10.8648,
      "step": 9
    },
    {
      "epoch": 0.05772005772005772,
      "grad_norm": 16.75,
      "learning_rate": 0.0004996019807715324,
      "loss": 9.0822,
      "step": 10
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 6.84375,
      "learning_rate": 0.0004992925563275714,
      "loss": 8.7376,
      "step": 11
    },
    {
      "epoch": 0.06926406926406926,
      "grad_norm": 22.25,
      "learning_rate": 0.0004988949126447567,
      "loss": 8.8624,
      "step": 12
    },
    {
      "epoch": 0.07503607503607504,
      "grad_norm": 5.0,
      "learning_rate": 0.0004984091904405792,
      "loss": 8.4415,
      "step": 13
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 14.9375,
      "learning_rate": 0.000497835561601612,
      "loss": 8.8092,
      "step": 14
    },
    {
      "epoch": 0.08658008658008658,
      "grad_norm": 30.5,
      "learning_rate": 0.0004971742291226826,
      "loss": 9.3403,
      "step": 15
    },
    {
      "epoch": 0.09235209235209235,
      "grad_norm": 21.5,
      "learning_rate": 0.0004964254270350387,
      "loss": 9.1326,
      "step": 16
    },
    {
      "epoch": 0.09812409812409813,
      "grad_norm": 6.09375,
      "learning_rate": 0.0004955894203235284,
      "loss": 8.5939,
      "step": 17
    },
    {
      "epoch": 0.1038961038961039,
      "grad_norm": 10.4375,
      "learning_rate": 0.0004946665048328287,
      "loss": 8.6717,
      "step": 18
    },
    {
      "epoch": 0.10966810966810966,
      "grad_norm": 4.375,
      "learning_rate": 0.0004936570071627517,
      "loss": 8.3633,
      "step": 19
    },
    {
      "epoch": 0.11544011544011544,
      "grad_norm": 7.9375,
      "learning_rate": 0.0004925612845526691,
      "loss": 8.385,
      "step": 20
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 2.75,
      "learning_rate": 0.0004913797247550911,
      "loss": 8.1043,
      "step": 21
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 5.71875,
      "learning_rate": 0.0004901127458984516,
      "loss": 8.2422,
      "step": 22
    },
    {
      "epoch": 0.13275613275613277,
      "grad_norm": 5.46875,
      "learning_rate": 0.0004887607963391394,
      "loss": 8.2323,
      "step": 23
    },
    {
      "epoch": 0.13852813852813853,
      "grad_norm": 5.59375,
      "learning_rate": 0.00048732435450283564,
      "loss": 8.2145,
      "step": 24
    },
    {
      "epoch": 0.1443001443001443,
      "grad_norm": 4.21875,
      "learning_rate": 0.00048580392871520943,
      "loss": 8.1502,
      "step": 25
    },
    {
      "epoch": 0.15007215007215008,
      "grad_norm": 3.28125,
      "learning_rate": 0.00048420005702203196,
      "loss": 8.2143,
      "step": 26
    },
    {
      "epoch": 0.15584415584415584,
      "grad_norm": 3.9375,
      "learning_rate": 0.00048251330699877374,
      "loss": 8.2088,
      "step": 27
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 2.671875,
      "learning_rate": 0.00048074427554975236,
      "loss": 8.1592,
      "step": 28
    },
    {
      "epoch": 0.1673881673881674,
      "grad_norm": 2.640625,
      "learning_rate": 0.00047889358869690056,
      "loss": 8.1112,
      "step": 29
    },
    {
      "epoch": 0.17316017316017315,
      "grad_norm": 3.6875,
      "learning_rate": 0.0004769619013582309,
      "loss": 8.1882,
      "step": 30
    },
    {
      "epoch": 0.17893217893217894,
      "grad_norm": 2.78125,
      "learning_rate": 0.00047494989711607415,
      "loss": 8.157,
      "step": 31
    },
    {
      "epoch": 0.1847041847041847,
      "grad_norm": 2.1875,
      "learning_rate": 0.0004728582879751746,
      "loss": 8.1928,
      "step": 32
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.65625,
      "learning_rate": 0.00047068781411072687,
      "loss": 8.1253,
      "step": 33
    },
    {
      "epoch": 0.19624819624819625,
      "grad_norm": 2.09375,
      "learning_rate": 0.00046843924360644385,
      "loss": 8.1661,
      "step": 34
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 4.3125,
      "learning_rate": 0.00046611337218274864,
      "loss": 8.1769,
      "step": 35
    },
    {
      "epoch": 0.2077922077922078,
      "grad_norm": 3.046875,
      "learning_rate": 0.0004637110229151863,
      "loss": 8.2317,
      "step": 36
    },
    {
      "epoch": 0.21356421356421357,
      "grad_norm": 2.625,
      "learning_rate": 0.00046123304594315517,
      "loss": 8.1467,
      "step": 37
    },
    {
      "epoch": 0.21933621933621933,
      "grad_norm": 4.03125,
      "learning_rate": 0.0004586803181690609,
      "loss": 8.2042,
      "step": 38
    },
    {
      "epoch": 0.22510822510822512,
      "grad_norm": 2.90625,
      "learning_rate": 0.0004560537429479998,
      "loss": 8.1968,
      "step": 39
    },
    {
      "epoch": 0.23088023088023088,
      "grad_norm": 2.78125,
      "learning_rate": 0.00045335424976808116,
      "loss": 8.1421,
      "step": 40
    },
    {
      "epoch": 0.23665223665223664,
      "grad_norm": 2.8125,
      "learning_rate": 0.0004505827939215009,
      "loss": 8.1562,
      "step": 41
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 2.671875,
      "learning_rate": 0.00044774035616648516,
      "loss": 8.0573,
      "step": 42
    },
    {
      "epoch": 0.2481962481962482,
      "grad_norm": 3.015625,
      "learning_rate": 0.0004448279423802207,
      "loss": 8.2113,
      "step": 43
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 2.375,
      "learning_rate": 0.0004418465832028967,
      "loss": 8.0617,
      "step": 44
    },
    {
      "epoch": 0.2597402597402597,
      "grad_norm": 2.125,
      "learning_rate": 0.00043879733367298404,
      "loss": 8.1265,
      "step": 45
    },
    {
      "epoch": 0.26551226551226553,
      "grad_norm": 2.125,
      "learning_rate": 0.00043568127285387924,
      "loss": 8.137,
      "step": 46
    },
    {
      "epoch": 0.2712842712842713,
      "grad_norm": 2.0625,
      "learning_rate": 0.00043249950345204804,
      "loss": 8.1275,
      "step": 47
    },
    {
      "epoch": 0.27705627705627706,
      "grad_norm": 1.8125,
      "learning_rate": 0.0004292531514268008,
      "loss": 8.0685,
      "step": 48
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 3.03125,
      "learning_rate": 0.00042594336559184035,
      "loss": 8.0958,
      "step": 49
    },
    {
      "epoch": 0.2886002886002886,
      "grad_norm": 2.484375,
      "learning_rate": 0.0004225713172087216,
      "loss": 8.062,
      "step": 50
    },
    {
      "epoch": 0.2943722943722944,
      "grad_norm": 1.8984375,
      "learning_rate": 0.0004191381995723672,
      "loss": 8.0802,
      "step": 51
    },
    {
      "epoch": 0.30014430014430016,
      "grad_norm": 2.21875,
      "learning_rate": 0.00041564522758878654,
      "loss": 8.1133,
      "step": 52
    },
    {
      "epoch": 0.3059163059163059,
      "grad_norm": 1.96875,
      "learning_rate": 0.0004120936373451467,
      "loss": 8.0476,
      "step": 53
    },
    {
      "epoch": 0.3116883116883117,
      "grad_norm": 1.7421875,
      "learning_rate": 0.000408484685672348,
      "loss": 8.0834,
      "step": 54
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 2.03125,
      "learning_rate": 0.0004048196497002588,
      "loss": 8.057,
      "step": 55
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 2.28125,
      "learning_rate": 0.0004010998264057667,
      "loss": 8.0262,
      "step": 56
    },
    {
      "epoch": 0.329004329004329,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0003973265321538069,
      "loss": 8.0431,
      "step": 57
    },
    {
      "epoch": 0.3347763347763348,
      "grad_norm": 1.8984375,
      "learning_rate": 0.0003935011022315284,
      "loss": 8.0947,
      "step": 58
    },
    {
      "epoch": 0.34054834054834054,
      "grad_norm": 2.078125,
      "learning_rate": 0.00038962489037576583,
      "loss": 8.1085,
      "step": 59
    },
    {
      "epoch": 0.3463203463203463,
      "grad_norm": 1.7578125,
      "learning_rate": 0.0003856992682939803,
      "loss": 8.0925,
      "step": 60
    },
    {
      "epoch": 0.35209235209235207,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0003817256251788425,
      "loss": 8.1044,
      "step": 61
    },
    {
      "epoch": 0.3578643578643579,
      "grad_norm": 1.984375,
      "learning_rate": 0.00037770536721662694,
      "loss": 8.0263,
      "step": 62
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 1.9609375,
      "learning_rate": 0.0003736399170895938,
      "loss": 8.1046,
      "step": 63
    },
    {
      "epoch": 0.3694083694083694,
      "grad_norm": 1.8671875,
      "learning_rate": 0.0003695307134725316,
      "loss": 8.0857,
      "step": 64
    },
    {
      "epoch": 0.37518037518037517,
      "grad_norm": 1.8359375,
      "learning_rate": 0.0003653792105236422,
      "loss": 7.9992,
      "step": 65
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 1.640625,
      "learning_rate": 0.00036118687736994487,
      "loss": 8.0316,
      "step": 66
    },
    {
      "epoch": 0.38672438672438675,
      "grad_norm": 1.9765625,
      "learning_rate": 0.0003569551975873847,
      "loss": 8.0453,
      "step": 67
    },
    {
      "epoch": 0.3924963924963925,
      "grad_norm": 1.6875,
      "learning_rate": 0.00035268566867582683,
      "loss": 8.0653,
      "step": 68
    },
    {
      "epoch": 0.39826839826839827,
      "grad_norm": 1.765625,
      "learning_rate": 0.0003483798015291239,
      "loss": 8.0296,
      "step": 69
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 2.046875,
      "learning_rate": 0.00034403911990044307,
      "loss": 8.1144,
      "step": 70
    },
    {
      "epoch": 0.4098124098124098,
      "grad_norm": 2.453125,
      "learning_rate": 0.00033966515986304317,
      "loss": 7.9971,
      "step": 71
    },
    {
      "epoch": 0.4155844155844156,
      "grad_norm": 1.7890625,
      "learning_rate": 0.0003352594692666915,
      "loss": 8.0769,
      "step": 72
    },
    {
      "epoch": 0.4213564213564214,
      "grad_norm": 1.84375,
      "learning_rate": 0.000330823607189913,
      "loss": 8.0346,
      "step": 73
    },
    {
      "epoch": 0.42712842712842713,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0003263591433882666,
      "loss": 8.0492,
      "step": 74
    },
    {
      "epoch": 0.4329004329004329,
      "grad_norm": 2.0,
      "learning_rate": 0.00032186765773884244,
      "loss": 8.0107,
      "step": 75
    },
    {
      "epoch": 0.43867243867243866,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0003173507396811774,
      "loss": 8.0198,
      "step": 76
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.875,
      "learning_rate": 0.00031280998765478727,
      "loss": 8.0375,
      "step": 77
    },
    {
      "epoch": 0.45021645021645024,
      "grad_norm": 1.703125,
      "learning_rate": 0.0003082470085335133,
      "loss": 8.0064,
      "step": 78
    },
    {
      "epoch": 0.455988455988456,
      "grad_norm": 2.015625,
      "learning_rate": 0.00030366341705688466,
      "loss": 8.072,
      "step": 79
    },
    {
      "epoch": 0.46176046176046176,
      "grad_norm": 1.796875,
      "learning_rate": 0.0002990608352586965,
      "loss": 7.9727,
      "step": 80
    },
    {
      "epoch": 0.4675324675324675,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00029444089189300783,
      "loss": 8.0572,
      "step": 81
    },
    {
      "epoch": 0.4733044733044733,
      "grad_norm": 1.8359375,
      "learning_rate": 0.00028980522185776065,
      "loss": 7.9624,
      "step": 82
    },
    {
      "epoch": 0.4790764790764791,
      "grad_norm": 1.5,
      "learning_rate": 0.00028515546561622466,
      "loss": 8.0296,
      "step": 83
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 1.890625,
      "learning_rate": 0.000280493268616473,
      "loss": 7.9691,
      "step": 84
    },
    {
      "epoch": 0.4906204906204906,
      "grad_norm": 1.703125,
      "learning_rate": 0.0002758202807090941,
      "loss": 7.982,
      "step": 85
    },
    {
      "epoch": 0.4963924963924964,
      "grad_norm": 1.828125,
      "learning_rate": 0.00027113815556334474,
      "loss": 7.9991,
      "step": 86
    },
    {
      "epoch": 0.5021645021645021,
      "grad_norm": 1.7109375,
      "learning_rate": 0.00026644855008195267,
      "loss": 7.9295,
      "step": 87
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002617531238147744,
      "loss": 8.0512,
      "step": 88
    },
    {
      "epoch": 0.5137085137085137,
      "grad_norm": 2.234375,
      "learning_rate": 0.0002570535383715165,
      "loss": 7.9438,
      "step": 89
    },
    {
      "epoch": 0.5194805194805194,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002523514568337281,
      "loss": 7.9315,
      "step": 90
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 1.9921875,
      "learning_rate": 0.000247648543166272,
      "loss": 7.9477,
      "step": 91
    },
    {
      "epoch": 0.5310245310245311,
      "grad_norm": 2.171875,
      "learning_rate": 0.00024294646162848353,
      "loss": 8.0017,
      "step": 92
    },
    {
      "epoch": 0.5367965367965368,
      "grad_norm": 1.3671875,
      "learning_rate": 0.00023824687618522567,
      "loss": 7.958,
      "step": 93
    },
    {
      "epoch": 0.5425685425685426,
      "grad_norm": 1.953125,
      "learning_rate": 0.00023355144991804737,
      "loss": 8.0388,
      "step": 94
    },
    {
      "epoch": 0.5483405483405484,
      "grad_norm": 1.8359375,
      "learning_rate": 0.00022886184443665522,
      "loss": 8.0198,
      "step": 95
    },
    {
      "epoch": 0.5541125541125541,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002241797192909059,
      "loss": 8.0289,
      "step": 96
    },
    {
      "epoch": 0.5598845598845599,
      "grad_norm": 1.71875,
      "learning_rate": 0.000219506731383527,
      "loss": 7.9571,
      "step": 97
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0002148445343837755,
      "loss": 7.977,
      "step": 98
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.5,
      "learning_rate": 0.00021019477814223942,
      "loss": 8.0177,
      "step": 99
    },
    {
      "epoch": 0.5772005772005772,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002055591081069922,
      "loss": 8.0195,
      "step": 100
    },
    {
      "epoch": 0.5829725829725829,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00020093916474130352,
      "loss": 7.9327,
      "step": 101
    },
    {
      "epoch": 0.5887445887445888,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00019633658294311535,
      "loss": 7.9211,
      "step": 102
    },
    {
      "epoch": 0.5945165945165946,
      "grad_norm": 1.8046875,
      "learning_rate": 0.0001917529914664867,
      "loss": 7.9637,
      "step": 103
    },
    {
      "epoch": 0.6002886002886003,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00018719001234521283,
      "loss": 7.9785,
      "step": 104
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.5625,
      "learning_rate": 0.00018264926031882274,
      "loss": 8.0137,
      "step": 105
    },
    {
      "epoch": 0.6118326118326118,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00017813234226115766,
      "loss": 7.9462,
      "step": 106
    },
    {
      "epoch": 0.6176046176046176,
      "grad_norm": 1.9765625,
      "learning_rate": 0.00017364085661173345,
      "loss": 7.9456,
      "step": 107
    },
    {
      "epoch": 0.6233766233766234,
      "grad_norm": 1.6953125,
      "learning_rate": 0.000169176392810087,
      "loss": 7.9988,
      "step": 108
    },
    {
      "epoch": 0.6291486291486291,
      "grad_norm": 1.5,
      "learning_rate": 0.0001647405307333085,
      "loss": 7.9011,
      "step": 109
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 1.65625,
      "learning_rate": 0.00016033484013695687,
      "loss": 7.9402,
      "step": 110
    },
    {
      "epoch": 0.6406926406926406,
      "grad_norm": 2.09375,
      "learning_rate": 0.00015596088009955694,
      "loss": 7.9126,
      "step": 111
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 2.03125,
      "learning_rate": 0.00015162019847087617,
      "loss": 7.9838,
      "step": 112
    },
    {
      "epoch": 0.6522366522366523,
      "grad_norm": 1.234375,
      "learning_rate": 0.00014731433132417315,
      "loss": 7.8632,
      "step": 113
    },
    {
      "epoch": 0.658008658008658,
      "grad_norm": 1.359375,
      "learning_rate": 0.00014304480241261527,
      "loss": 7.9836,
      "step": 114
    },
    {
      "epoch": 0.6637806637806638,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0001388131226300552,
      "loss": 8.0018,
      "step": 115
    },
    {
      "epoch": 0.6695526695526696,
      "grad_norm": 1.578125,
      "learning_rate": 0.0001346207894763578,
      "loss": 7.9257,
      "step": 116
    },
    {
      "epoch": 0.6753246753246753,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0001304692865274683,
      "loss": 8.0056,
      "step": 117
    },
    {
      "epoch": 0.6810966810966811,
      "grad_norm": 1.21875,
      "learning_rate": 0.0001263600829104062,
      "loss": 7.9562,
      "step": 118
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 1.40625,
      "learning_rate": 0.00012229463278337307,
      "loss": 7.9725,
      "step": 119
    },
    {
      "epoch": 0.6926406926406926,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00011827437482115758,
      "loss": 7.9155,
      "step": 120
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 1.375,
      "learning_rate": 0.00011430073170601968,
      "loss": 7.9656,
      "step": 121
    },
    {
      "epoch": 0.7041847041847041,
      "grad_norm": 1.265625,
      "learning_rate": 0.00011037510962423425,
      "loss": 7.9116,
      "step": 122
    },
    {
      "epoch": 0.70995670995671,
      "grad_norm": 1.28125,
      "learning_rate": 0.0001064988977684716,
      "loss": 8.0221,
      "step": 123
    },
    {
      "epoch": 0.7157287157287158,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00010267346784619325,
      "loss": 8.0241,
      "step": 124
    },
    {
      "epoch": 0.7215007215007215,
      "grad_norm": 1.4609375,
      "learning_rate": 9.890017359423326e-05,
      "loss": 7.9424,
      "step": 125
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.390625,
      "learning_rate": 9.518035029974126e-05,
      "loss": 7.902,
      "step": 126
    },
    {
      "epoch": 0.733044733044733,
      "grad_norm": 1.265625,
      "learning_rate": 9.151531432765204e-05,
      "loss": 7.9318,
      "step": 127
    },
    {
      "epoch": 0.7388167388167388,
      "grad_norm": 1.234375,
      "learning_rate": 8.790636265485333e-05,
      "loss": 7.9299,
      "step": 128
    },
    {
      "epoch": 0.7445887445887446,
      "grad_norm": 1.1796875,
      "learning_rate": 8.435477241121353e-05,
      "loss": 7.9018,
      "step": 129
    },
    {
      "epoch": 0.7503607503607503,
      "grad_norm": 1.5,
      "learning_rate": 8.086180042763284e-05,
      "loss": 7.9427,
      "step": 130
    },
    {
      "epoch": 0.7561327561327561,
      "grad_norm": 1.203125,
      "learning_rate": 7.742868279127849e-05,
      "loss": 7.9319,
      "step": 131
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 1.21875,
      "learning_rate": 7.405663440815968e-05,
      "loss": 7.9433,
      "step": 132
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 1.3203125,
      "learning_rate": 7.074684857319927e-05,
      "loss": 7.9456,
      "step": 133
    },
    {
      "epoch": 0.7734487734487735,
      "grad_norm": 1.234375,
      "learning_rate": 6.750049654795198e-05,
      "loss": 7.912,
      "step": 134
    },
    {
      "epoch": 0.7792207792207793,
      "grad_norm": 1.2265625,
      "learning_rate": 6.431872714612072e-05,
      "loss": 7.9466,
      "step": 135
    },
    {
      "epoch": 0.784992784992785,
      "grad_norm": 1.359375,
      "learning_rate": 6.120266632701598e-05,
      "loss": 7.9205,
      "step": 136
    },
    {
      "epoch": 0.7907647907647908,
      "grad_norm": 1.3125,
      "learning_rate": 5.815341679710326e-05,
      "loss": 7.9046,
      "step": 137
    },
    {
      "epoch": 0.7965367965367965,
      "grad_norm": 1.2578125,
      "learning_rate": 5.517205761977939e-05,
      "loss": 7.9473,
      "step": 138
    },
    {
      "epoch": 0.8023088023088023,
      "grad_norm": 1.1796875,
      "learning_rate": 5.225964383351489e-05,
      "loss": 7.8508,
      "step": 139
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.125,
      "learning_rate": 4.941720607849912e-05,
      "loss": 7.9217,
      "step": 140
    },
    {
      "epoch": 0.8138528138528138,
      "grad_norm": 1.21875,
      "learning_rate": 4.664575023191886e-05,
      "loss": 7.9105,
      "step": 141
    },
    {
      "epoch": 0.8196248196248196,
      "grad_norm": 1.1796875,
      "learning_rate": 4.394625705200012e-05,
      "loss": 7.9154,
      "step": 142
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 1.3203125,
      "learning_rate": 4.131968183093912e-05,
      "loss": 7.8561,
      "step": 143
    },
    {
      "epoch": 0.8311688311688312,
      "grad_norm": 1.3984375,
      "learning_rate": 3.876695405684485e-05,
      "loss": 8.0501,
      "step": 144
    },
    {
      "epoch": 0.836940836940837,
      "grad_norm": 1.203125,
      "learning_rate": 3.628897708481377e-05,
      "loss": 7.9667,
      "step": 145
    },
    {
      "epoch": 0.8427128427128427,
      "grad_norm": 1.2109375,
      "learning_rate": 3.388662781725141e-05,
      "loss": 7.899,
      "step": 146
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 1.1328125,
      "learning_rate": 3.1560756393556184e-05,
      "loss": 7.8567,
      "step": 147
    },
    {
      "epoch": 0.8542568542568543,
      "grad_norm": 1.296875,
      "learning_rate": 2.9312185889273145e-05,
      "loss": 7.973,
      "step": 148
    },
    {
      "epoch": 0.86002886002886,
      "grad_norm": 1.1953125,
      "learning_rate": 2.7141712024825378e-05,
      "loss": 7.8728,
      "step": 149
    },
    {
      "epoch": 0.8658008658008658,
      "grad_norm": 1.15625,
      "learning_rate": 2.505010288392587e-05,
      "loss": 7.8931,
      "step": 150
    },
    {
      "epoch": 0.8715728715728716,
      "grad_norm": 1.2265625,
      "learning_rate": 2.3038098641769088e-05,
      "loss": 7.9046,
      "step": 151
    },
    {
      "epoch": 0.8773448773448773,
      "grad_norm": 1.2265625,
      "learning_rate": 2.1106411303099453e-05,
      "loss": 7.9571,
      "step": 152
    },
    {
      "epoch": 0.8831168831168831,
      "grad_norm": 1.203125,
      "learning_rate": 1.9255724450247676e-05,
      "loss": 7.942,
      "step": 153
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.265625,
      "learning_rate": 1.7486693001226267e-05,
      "loss": 7.9148,
      "step": 154
    },
    {
      "epoch": 0.8946608946608947,
      "grad_norm": 1.2734375,
      "learning_rate": 1.579994297796808e-05,
      "loss": 7.9268,
      "step": 155
    },
    {
      "epoch": 0.9004329004329005,
      "grad_norm": 1.2890625,
      "learning_rate": 1.4196071284790529e-05,
      "loss": 7.8032,
      "step": 156
    },
    {
      "epoch": 0.9062049062049062,
      "grad_norm": 1.1875,
      "learning_rate": 1.2675645497164351e-05,
      "loss": 7.9132,
      "step": 157
    },
    {
      "epoch": 0.911976911976912,
      "grad_norm": 1.171875,
      "learning_rate": 1.1239203660860647e-05,
      "loss": 7.9581,
      "step": 158
    },
    {
      "epoch": 0.9177489177489178,
      "grad_norm": 1.2421875,
      "learning_rate": 9.88725410154842e-06,
      "loss": 7.9374,
      "step": 159
    },
    {
      "epoch": 0.9235209235209235,
      "grad_norm": 1.2109375,
      "learning_rate": 8.620275244908826e-06,
      "loss": 7.9449,
      "step": 160
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 1.2109375,
      "learning_rate": 7.438715447331018e-06,
      "loss": 7.9258,
      "step": 161
    },
    {
      "epoch": 0.935064935064935,
      "grad_norm": 1.234375,
      "learning_rate": 6.342992837248235e-06,
      "loss": 7.9198,
      "step": 162
    },
    {
      "epoch": 0.9408369408369408,
      "grad_norm": 1.2578125,
      "learning_rate": 5.333495167171354e-06,
      "loss": 7.9402,
      "step": 163
    },
    {
      "epoch": 0.9466089466089466,
      "grad_norm": 1.328125,
      "learning_rate": 4.410579676471571e-06,
      "loss": 8.0436,
      "step": 164
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 1.265625,
      "learning_rate": 3.5745729649613034e-06,
      "loss": 7.9191,
      "step": 165
    },
    {
      "epoch": 0.9581529581529582,
      "grad_norm": 1.25,
      "learning_rate": 2.8257708773173627e-06,
      "loss": 7.8881,
      "step": 166
    },
    {
      "epoch": 0.963924963924964,
      "grad_norm": 1.25,
      "learning_rate": 2.1644383983880357e-06,
      "loss": 7.9092,
      "step": 167
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.1953125,
      "learning_rate": 1.5908095594207582e-06,
      "loss": 7.9875,
      "step": 168
    },
    {
      "epoch": 0.9754689754689755,
      "grad_norm": 1.2578125,
      "learning_rate": 1.1050873552433394e-06,
      "loss": 7.962,
      "step": 169
    },
    {
      "epoch": 0.9812409812409812,
      "grad_norm": 1.1953125,
      "learning_rate": 7.074436724286704e-07,
      "loss": 7.9649,
      "step": 170
    },
    {
      "epoch": 0.987012987012987,
      "grad_norm": 1.203125,
      "learning_rate": 3.9801922846766094e-07,
      "loss": 7.9409,
      "step": 171
    },
    {
      "epoch": 0.9927849927849928,
      "grad_norm": 1.203125,
      "learning_rate": 1.7692352197240524e-07,
      "loss": 7.894,
      "step": 172
    },
    {
      "epoch": 0.9985569985569985,
      "grad_norm": 1.265625,
      "learning_rate": 4.423479392709484e-08,
      "loss": 7.9152,
      "step": 173
    }
  ],
  "logging_steps": 1,
  "max_steps": 173,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6799425955430400.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9985569985569986,
  "eval_steps": 500,
  "global_step": 346,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005772005772005772,
      "grad_norm": 14.5625,
      "learning_rate": 0.0,
      "loss": 9.9829,
      "step": 1
    },
    {
      "epoch": 0.011544011544011544,
      "grad_norm": 14.125,
      "learning_rate": 8.333333333333333e-05,
      "loss": 10.0735,
      "step": 2
    },
    {
      "epoch": 0.017316017316017316,
      "grad_norm": 10.375,
      "learning_rate": 0.00016666666666666666,
      "loss": 9.2063,
      "step": 3
    },
    {
      "epoch": 0.023088023088023088,
      "grad_norm": 30.0,
      "learning_rate": 0.00025,
      "loss": 9.8324,
      "step": 4
    },
    {
      "epoch": 0.02886002886002886,
      "grad_norm": 51.5,
      "learning_rate": 0.0003333333333333333,
      "loss": 11.5662,
      "step": 5
    },
    {
      "epoch": 0.03463203463203463,
      "grad_norm": 61.25,
      "learning_rate": 0.0004166666666666667,
      "loss": 12.038,
      "step": 6
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 18.875,
      "learning_rate": 0.0005,
      "loss": 9.3774,
      "step": 7
    },
    {
      "epoch": 0.046176046176046176,
      "grad_norm": 8.625,
      "learning_rate": 0.0004999557652060729,
      "loss": 9.5477,
      "step": 8
    },
    {
      "epoch": 0.05194805194805195,
      "grad_norm": 31.625,
      "learning_rate": 0.0004998230764780276,
      "loss": 10.8648,
      "step": 9
    },
    {
      "epoch": 0.05772005772005772,
      "grad_norm": 16.75,
      "learning_rate": 0.0004996019807715324,
      "loss": 9.0822,
      "step": 10
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 6.84375,
      "learning_rate": 0.0004992925563275714,
      "loss": 8.7376,
      "step": 11
    },
    {
      "epoch": 0.06926406926406926,
      "grad_norm": 22.25,
      "learning_rate": 0.0004988949126447567,
      "loss": 8.8624,
      "step": 12
    },
    {
      "epoch": 0.07503607503607504,
      "grad_norm": 5.0,
      "learning_rate": 0.0004984091904405792,
      "loss": 8.4415,
      "step": 13
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 14.9375,
      "learning_rate": 0.000497835561601612,
      "loss": 8.8092,
      "step": 14
    },
    {
      "epoch": 0.08658008658008658,
      "grad_norm": 30.5,
      "learning_rate": 0.0004971742291226826,
      "loss": 9.3403,
      "step": 15
    },
    {
      "epoch": 0.09235209235209235,
      "grad_norm": 21.5,
      "learning_rate": 0.0004964254270350387,
      "loss": 9.1326,
      "step": 16
    },
    {
      "epoch": 0.09812409812409813,
      "grad_norm": 6.09375,
      "learning_rate": 0.0004955894203235284,
      "loss": 8.5939,
      "step": 17
    },
    {
      "epoch": 0.1038961038961039,
      "grad_norm": 10.4375,
      "learning_rate": 0.0004946665048328287,
      "loss": 8.6717,
      "step": 18
    },
    {
      "epoch": 0.10966810966810966,
      "grad_norm": 4.375,
      "learning_rate": 0.0004936570071627517,
      "loss": 8.3633,
      "step": 19
    },
    {
      "epoch": 0.11544011544011544,
      "grad_norm": 7.9375,
      "learning_rate": 0.0004925612845526691,
      "loss": 8.385,
      "step": 20
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 2.75,
      "learning_rate": 0.0004913797247550911,
      "loss": 8.1043,
      "step": 21
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 5.71875,
      "learning_rate": 0.0004901127458984516,
      "loss": 8.2422,
      "step": 22
    },
    {
      "epoch": 0.13275613275613277,
      "grad_norm": 5.46875,
      "learning_rate": 0.0004887607963391394,
      "loss": 8.2323,
      "step": 23
    },
    {
      "epoch": 0.13852813852813853,
      "grad_norm": 5.59375,
      "learning_rate": 0.00048732435450283564,
      "loss": 8.2145,
      "step": 24
    },
    {
      "epoch": 0.1443001443001443,
      "grad_norm": 4.21875,
      "learning_rate": 0.00048580392871520943,
      "loss": 8.1502,
      "step": 25
    },
    {
      "epoch": 0.15007215007215008,
      "grad_norm": 3.28125,
      "learning_rate": 0.00048420005702203196,
      "loss": 8.2143,
      "step": 26
    },
    {
      "epoch": 0.15584415584415584,
      "grad_norm": 3.9375,
      "learning_rate": 0.00048251330699877374,
      "loss": 8.2088,
      "step": 27
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 2.671875,
      "learning_rate": 0.00048074427554975236,
      "loss": 8.1592,
      "step": 28
    },
    {
      "epoch": 0.1673881673881674,
      "grad_norm": 2.640625,
      "learning_rate": 0.00047889358869690056,
      "loss": 8.1112,
      "step": 29
    },
    {
      "epoch": 0.17316017316017315,
      "grad_norm": 3.6875,
      "learning_rate": 0.0004769619013582309,
      "loss": 8.1882,
      "step": 30
    },
    {
      "epoch": 0.17893217893217894,
      "grad_norm": 2.78125,
      "learning_rate": 0.00047494989711607415,
      "loss": 8.157,
      "step": 31
    },
    {
      "epoch": 0.1847041847041847,
      "grad_norm": 2.1875,
      "learning_rate": 0.0004728582879751746,
      "loss": 8.1928,
      "step": 32
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.65625,
      "learning_rate": 0.00047068781411072687,
      "loss": 8.1253,
      "step": 33
    },
    {
      "epoch": 0.19624819624819625,
      "grad_norm": 2.09375,
      "learning_rate": 0.00046843924360644385,
      "loss": 8.1661,
      "step": 34
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 4.3125,
      "learning_rate": 0.00046611337218274864,
      "loss": 8.1769,
      "step": 35
    },
    {
      "epoch": 0.2077922077922078,
      "grad_norm": 3.046875,
      "learning_rate": 0.0004637110229151863,
      "loss": 8.2317,
      "step": 36
    },
    {
      "epoch": 0.21356421356421357,
      "grad_norm": 2.625,
      "learning_rate": 0.00046123304594315517,
      "loss": 8.1467,
      "step": 37
    },
    {
      "epoch": 0.21933621933621933,
      "grad_norm": 4.03125,
      "learning_rate": 0.0004586803181690609,
      "loss": 8.2042,
      "step": 38
    },
    {
      "epoch": 0.22510822510822512,
      "grad_norm": 2.90625,
      "learning_rate": 0.0004560537429479998,
      "loss": 8.1968,
      "step": 39
    },
    {
      "epoch": 0.23088023088023088,
      "grad_norm": 2.78125,
      "learning_rate": 0.00045335424976808116,
      "loss": 8.1421,
      "step": 40
    },
    {
      "epoch": 0.23665223665223664,
      "grad_norm": 2.8125,
      "learning_rate": 0.0004505827939215009,
      "loss": 8.1562,
      "step": 41
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 2.671875,
      "learning_rate": 0.00044774035616648516,
      "loss": 8.0573,
      "step": 42
    },
    {
      "epoch": 0.2481962481962482,
      "grad_norm": 3.015625,
      "learning_rate": 0.0004448279423802207,
      "loss": 8.2113,
      "step": 43
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 2.375,
      "learning_rate": 0.0004418465832028967,
      "loss": 8.0617,
      "step": 44
    },
    {
      "epoch": 0.2597402597402597,
      "grad_norm": 2.125,
      "learning_rate": 0.00043879733367298404,
      "loss": 8.1265,
      "step": 45
    },
    {
      "epoch": 0.26551226551226553,
      "grad_norm": 2.125,
      "learning_rate": 0.00043568127285387924,
      "loss": 8.137,
      "step": 46
    },
    {
      "epoch": 0.2712842712842713,
      "grad_norm": 2.0625,
      "learning_rate": 0.00043249950345204804,
      "loss": 8.1275,
      "step": 47
    },
    {
      "epoch": 0.27705627705627706,
      "grad_norm": 1.8125,
      "learning_rate": 0.0004292531514268008,
      "loss": 8.0685,
      "step": 48
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 3.03125,
      "learning_rate": 0.00042594336559184035,
      "loss": 8.0958,
      "step": 49
    },
    {
      "epoch": 0.2886002886002886,
      "grad_norm": 2.484375,
      "learning_rate": 0.0004225713172087216,
      "loss": 8.062,
      "step": 50
    },
    {
      "epoch": 0.2943722943722944,
      "grad_norm": 1.8984375,
      "learning_rate": 0.0004191381995723672,
      "loss": 8.0802,
      "step": 51
    },
    {
      "epoch": 0.30014430014430016,
      "grad_norm": 2.21875,
      "learning_rate": 0.00041564522758878654,
      "loss": 8.1133,
      "step": 52
    },
    {
      "epoch": 0.3059163059163059,
      "grad_norm": 1.96875,
      "learning_rate": 0.0004120936373451467,
      "loss": 8.0476,
      "step": 53
    },
    {
      "epoch": 0.3116883116883117,
      "grad_norm": 1.7421875,
      "learning_rate": 0.000408484685672348,
      "loss": 8.0834,
      "step": 54
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 2.03125,
      "learning_rate": 0.0004048196497002588,
      "loss": 8.057,
      "step": 55
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 2.28125,
      "learning_rate": 0.0004010998264057667,
      "loss": 8.0262,
      "step": 56
    },
    {
      "epoch": 0.329004329004329,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0003973265321538069,
      "loss": 8.0431,
      "step": 57
    },
    {
      "epoch": 0.3347763347763348,
      "grad_norm": 1.8984375,
      "learning_rate": 0.0003935011022315284,
      "loss": 8.0947,
      "step": 58
    },
    {
      "epoch": 0.34054834054834054,
      "grad_norm": 2.078125,
      "learning_rate": 0.00038962489037576583,
      "loss": 8.1085,
      "step": 59
    },
    {
      "epoch": 0.3463203463203463,
      "grad_norm": 1.7578125,
      "learning_rate": 0.0003856992682939803,
      "loss": 8.0925,
      "step": 60
    },
    {
      "epoch": 0.35209235209235207,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0003817256251788425,
      "loss": 8.1044,
      "step": 61
    },
    {
      "epoch": 0.3578643578643579,
      "grad_norm": 1.984375,
      "learning_rate": 0.00037770536721662694,
      "loss": 8.0263,
      "step": 62
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 1.9609375,
      "learning_rate": 0.0003736399170895938,
      "loss": 8.1046,
      "step": 63
    },
    {
      "epoch": 0.3694083694083694,
      "grad_norm": 1.8671875,
      "learning_rate": 0.0003695307134725316,
      "loss": 8.0857,
      "step": 64
    },
    {
      "epoch": 0.37518037518037517,
      "grad_norm": 1.8359375,
      "learning_rate": 0.0003653792105236422,
      "loss": 7.9992,
      "step": 65
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 1.640625,
      "learning_rate": 0.00036118687736994487,
      "loss": 8.0316,
      "step": 66
    },
    {
      "epoch": 0.38672438672438675,
      "grad_norm": 1.9765625,
      "learning_rate": 0.0003569551975873847,
      "loss": 8.0453,
      "step": 67
    },
    {
      "epoch": 0.3924963924963925,
      "grad_norm": 1.6875,
      "learning_rate": 0.00035268566867582683,
      "loss": 8.0653,
      "step": 68
    },
    {
      "epoch": 0.39826839826839827,
      "grad_norm": 1.765625,
      "learning_rate": 0.0003483798015291239,
      "loss": 8.0296,
      "step": 69
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 2.046875,
      "learning_rate": 0.00034403911990044307,
      "loss": 8.1144,
      "step": 70
    },
    {
      "epoch": 0.4098124098124098,
      "grad_norm": 2.453125,
      "learning_rate": 0.00033966515986304317,
      "loss": 7.9971,
      "step": 71
    },
    {
      "epoch": 0.4155844155844156,
      "grad_norm": 1.7890625,
      "learning_rate": 0.0003352594692666915,
      "loss": 8.0769,
      "step": 72
    },
    {
      "epoch": 0.4213564213564214,
      "grad_norm": 1.84375,
      "learning_rate": 0.000330823607189913,
      "loss": 8.0346,
      "step": 73
    },
    {
      "epoch": 0.42712842712842713,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0003263591433882666,
      "loss": 8.0492,
      "step": 74
    },
    {
      "epoch": 0.4329004329004329,
      "grad_norm": 2.0,
      "learning_rate": 0.00032186765773884244,
      "loss": 8.0107,
      "step": 75
    },
    {
      "epoch": 0.43867243867243866,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0003173507396811774,
      "loss": 8.0198,
      "step": 76
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.875,
      "learning_rate": 0.00031280998765478727,
      "loss": 8.0375,
      "step": 77
    },
    {
      "epoch": 0.45021645021645024,
      "grad_norm": 1.703125,
      "learning_rate": 0.0003082470085335133,
      "loss": 8.0064,
      "step": 78
    },
    {
      "epoch": 0.455988455988456,
      "grad_norm": 2.015625,
      "learning_rate": 0.00030366341705688466,
      "loss": 8.072,
      "step": 79
    },
    {
      "epoch": 0.46176046176046176,
      "grad_norm": 1.796875,
      "learning_rate": 0.0002990608352586965,
      "loss": 7.9727,
      "step": 80
    },
    {
      "epoch": 0.4675324675324675,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00029444089189300783,
      "loss": 8.0572,
      "step": 81
    },
    {
      "epoch": 0.4733044733044733,
      "grad_norm": 1.8359375,
      "learning_rate": 0.00028980522185776065,
      "loss": 7.9624,
      "step": 82
    },
    {
      "epoch": 0.4790764790764791,
      "grad_norm": 1.5,
      "learning_rate": 0.00028515546561622466,
      "loss": 8.0296,
      "step": 83
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 1.890625,
      "learning_rate": 0.000280493268616473,
      "loss": 7.9691,
      "step": 84
    },
    {
      "epoch": 0.4906204906204906,
      "grad_norm": 1.703125,
      "learning_rate": 0.0002758202807090941,
      "loss": 7.982,
      "step": 85
    },
    {
      "epoch": 0.4963924963924964,
      "grad_norm": 1.828125,
      "learning_rate": 0.00027113815556334474,
      "loss": 7.9991,
      "step": 86
    },
    {
      "epoch": 0.5021645021645021,
      "grad_norm": 1.7109375,
      "learning_rate": 0.00026644855008195267,
      "loss": 7.9295,
      "step": 87
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 1.453125,
      "learning_rate": 0.0002617531238147744,
      "loss": 8.0512,
      "step": 88
    },
    {
      "epoch": 0.5137085137085137,
      "grad_norm": 2.234375,
      "learning_rate": 0.0002570535383715165,
      "loss": 7.9438,
      "step": 89
    },
    {
      "epoch": 0.5194805194805194,
      "grad_norm": 1.6640625,
      "learning_rate": 0.0002523514568337281,
      "loss": 7.9315,
      "step": 90
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 1.9921875,
      "learning_rate": 0.000247648543166272,
      "loss": 7.9477,
      "step": 91
    },
    {
      "epoch": 0.5310245310245311,
      "grad_norm": 2.171875,
      "learning_rate": 0.00024294646162848353,
      "loss": 8.0017,
      "step": 92
    },
    {
      "epoch": 0.5367965367965368,
      "grad_norm": 1.3671875,
      "learning_rate": 0.00023824687618522567,
      "loss": 7.958,
      "step": 93
    },
    {
      "epoch": 0.5425685425685426,
      "grad_norm": 1.953125,
      "learning_rate": 0.00023355144991804737,
      "loss": 8.0388,
      "step": 94
    },
    {
      "epoch": 0.5483405483405484,
      "grad_norm": 1.8359375,
      "learning_rate": 0.00022886184443665522,
      "loss": 8.0198,
      "step": 95
    },
    {
      "epoch": 0.5541125541125541,
      "grad_norm": 1.46875,
      "learning_rate": 0.0002241797192909059,
      "loss": 8.0289,
      "step": 96
    },
    {
      "epoch": 0.5598845598845599,
      "grad_norm": 1.71875,
      "learning_rate": 0.000219506731383527,
      "loss": 7.9571,
      "step": 97
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0002148445343837755,
      "loss": 7.977,
      "step": 98
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.5,
      "learning_rate": 0.00021019477814223942,
      "loss": 8.0177,
      "step": 99
    },
    {
      "epoch": 0.5772005772005772,
      "grad_norm": 1.5703125,
      "learning_rate": 0.0002055591081069922,
      "loss": 8.0195,
      "step": 100
    },
    {
      "epoch": 0.5829725829725829,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00020093916474130352,
      "loss": 7.9327,
      "step": 101
    },
    {
      "epoch": 0.5887445887445888,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00019633658294311535,
      "loss": 7.9211,
      "step": 102
    },
    {
      "epoch": 0.5945165945165946,
      "grad_norm": 1.8046875,
      "learning_rate": 0.0001917529914664867,
      "loss": 7.9637,
      "step": 103
    },
    {
      "epoch": 0.6002886002886003,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00018719001234521283,
      "loss": 7.9785,
      "step": 104
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.5625,
      "learning_rate": 0.00018264926031882274,
      "loss": 8.0137,
      "step": 105
    },
    {
      "epoch": 0.6118326118326118,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00017813234226115766,
      "loss": 7.9462,
      "step": 106
    },
    {
      "epoch": 0.6176046176046176,
      "grad_norm": 1.9765625,
      "learning_rate": 0.00017364085661173345,
      "loss": 7.9456,
      "step": 107
    },
    {
      "epoch": 0.6233766233766234,
      "grad_norm": 1.6953125,
      "learning_rate": 0.000169176392810087,
      "loss": 7.9988,
      "step": 108
    },
    {
      "epoch": 0.6291486291486291,
      "grad_norm": 1.5,
      "learning_rate": 0.0001647405307333085,
      "loss": 7.9011,
      "step": 109
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 1.65625,
      "learning_rate": 0.00016033484013695687,
      "loss": 7.9402,
      "step": 110
    },
    {
      "epoch": 0.6406926406926406,
      "grad_norm": 2.09375,
      "learning_rate": 0.00015596088009955694,
      "loss": 7.9126,
      "step": 111
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 2.03125,
      "learning_rate": 0.00015162019847087617,
      "loss": 7.9838,
      "step": 112
    },
    {
      "epoch": 0.6522366522366523,
      "grad_norm": 1.234375,
      "learning_rate": 0.00014731433132417315,
      "loss": 7.8632,
      "step": 113
    },
    {
      "epoch": 0.658008658008658,
      "grad_norm": 1.359375,
      "learning_rate": 0.00014304480241261527,
      "loss": 7.9836,
      "step": 114
    },
    {
      "epoch": 0.6637806637806638,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0001388131226300552,
      "loss": 8.0018,
      "step": 115
    },
    {
      "epoch": 0.6695526695526696,
      "grad_norm": 1.578125,
      "learning_rate": 0.0001346207894763578,
      "loss": 7.9257,
      "step": 116
    },
    {
      "epoch": 0.6753246753246753,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0001304692865274683,
      "loss": 8.0056,
      "step": 117
    },
    {
      "epoch": 0.6810966810966811,
      "grad_norm": 1.21875,
      "learning_rate": 0.0001263600829104062,
      "loss": 7.9562,
      "step": 118
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 1.40625,
      "learning_rate": 0.00012229463278337307,
      "loss": 7.9725,
      "step": 119
    },
    {
      "epoch": 0.6926406926406926,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00011827437482115758,
      "loss": 7.9155,
      "step": 120
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 1.375,
      "learning_rate": 0.00011430073170601968,
      "loss": 7.9656,
      "step": 121
    },
    {
      "epoch": 0.7041847041847041,
      "grad_norm": 1.265625,
      "learning_rate": 0.00011037510962423425,
      "loss": 7.9116,
      "step": 122
    },
    {
      "epoch": 0.70995670995671,
      "grad_norm": 1.28125,
      "learning_rate": 0.0001064988977684716,
      "loss": 8.0221,
      "step": 123
    },
    {
      "epoch": 0.7157287157287158,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00010267346784619325,
      "loss": 8.0241,
      "step": 124
    },
    {
      "epoch": 0.7215007215007215,
      "grad_norm": 1.4609375,
      "learning_rate": 9.890017359423326e-05,
      "loss": 7.9424,
      "step": 125
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.390625,
      "learning_rate": 9.518035029974126e-05,
      "loss": 7.902,
      "step": 126
    },
    {
      "epoch": 0.733044733044733,
      "grad_norm": 1.265625,
      "learning_rate": 9.151531432765204e-05,
      "loss": 7.9318,
      "step": 127
    },
    {
      "epoch": 0.7388167388167388,
      "grad_norm": 1.234375,
      "learning_rate": 8.790636265485333e-05,
      "loss": 7.9299,
      "step": 128
    },
    {
      "epoch": 0.7445887445887446,
      "grad_norm": 1.1796875,
      "learning_rate": 8.435477241121353e-05,
      "loss": 7.9018,
      "step": 129
    },
    {
      "epoch": 0.7503607503607503,
      "grad_norm": 1.5,
      "learning_rate": 8.086180042763284e-05,
      "loss": 7.9427,
      "step": 130
    },
    {
      "epoch": 0.7561327561327561,
      "grad_norm": 1.203125,
      "learning_rate": 7.742868279127849e-05,
      "loss": 7.9319,
      "step": 131
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 1.21875,
      "learning_rate": 7.405663440815968e-05,
      "loss": 7.9433,
      "step": 132
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 1.3203125,
      "learning_rate": 7.074684857319927e-05,
      "loss": 7.9456,
      "step": 133
    },
    {
      "epoch": 0.7734487734487735,
      "grad_norm": 1.234375,
      "learning_rate": 6.750049654795198e-05,
      "loss": 7.912,
      "step": 134
    },
    {
      "epoch": 0.7792207792207793,
      "grad_norm": 1.2265625,
      "learning_rate": 6.431872714612072e-05,
      "loss": 7.9466,
      "step": 135
    },
    {
      "epoch": 0.784992784992785,
      "grad_norm": 1.359375,
      "learning_rate": 6.120266632701598e-05,
      "loss": 7.9205,
      "step": 136
    },
    {
      "epoch": 0.7907647907647908,
      "grad_norm": 1.3125,
      "learning_rate": 5.815341679710326e-05,
      "loss": 7.9046,
      "step": 137
    },
    {
      "epoch": 0.7965367965367965,
      "grad_norm": 1.2578125,
      "learning_rate": 5.517205761977939e-05,
      "loss": 7.9473,
      "step": 138
    },
    {
      "epoch": 0.8023088023088023,
      "grad_norm": 1.1796875,
      "learning_rate": 5.225964383351489e-05,
      "loss": 7.8508,
      "step": 139
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.125,
      "learning_rate": 4.941720607849912e-05,
      "loss": 7.9217,
      "step": 140
    },
    {
      "epoch": 0.8138528138528138,
      "grad_norm": 1.21875,
      "learning_rate": 4.664575023191886e-05,
      "loss": 7.9105,
      "step": 141
    },
    {
      "epoch": 0.8196248196248196,
      "grad_norm": 1.1796875,
      "learning_rate": 4.394625705200012e-05,
      "loss": 7.9154,
      "step": 142
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 1.3203125,
      "learning_rate": 4.131968183093912e-05,
      "loss": 7.8561,
      "step": 143
    },
    {
      "epoch": 0.8311688311688312,
      "grad_norm": 1.3984375,
      "learning_rate": 3.876695405684485e-05,
      "loss": 8.0501,
      "step": 144
    },
    {
      "epoch": 0.836940836940837,
      "grad_norm": 1.203125,
      "learning_rate": 3.628897708481377e-05,
      "loss": 7.9667,
      "step": 145
    },
    {
      "epoch": 0.8427128427128427,
      "grad_norm": 1.2109375,
      "learning_rate": 3.388662781725141e-05,
      "loss": 7.899,
      "step": 146
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 1.1328125,
      "learning_rate": 3.1560756393556184e-05,
      "loss": 7.8567,
      "step": 147
    },
    {
      "epoch": 0.8542568542568543,
      "grad_norm": 1.296875,
      "learning_rate": 2.9312185889273145e-05,
      "loss": 7.973,
      "step": 148
    },
    {
      "epoch": 0.86002886002886,
      "grad_norm": 1.1953125,
      "learning_rate": 2.7141712024825378e-05,
      "loss": 7.8728,
      "step": 149
    },
    {
      "epoch": 0.8658008658008658,
      "grad_norm": 1.15625,
      "learning_rate": 2.505010288392587e-05,
      "loss": 7.8931,
      "step": 150
    },
    {
      "epoch": 0.8715728715728716,
      "grad_norm": 1.2265625,
      "learning_rate": 2.3038098641769088e-05,
      "loss": 7.9046,
      "step": 151
    },
    {
      "epoch": 0.8773448773448773,
      "grad_norm": 1.2265625,
      "learning_rate": 2.1106411303099453e-05,
      "loss": 7.9571,
      "step": 152
    },
    {
      "epoch": 0.8831168831168831,
      "grad_norm": 1.203125,
      "learning_rate": 1.9255724450247676e-05,
      "loss": 7.942,
      "step": 153
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.265625,
      "learning_rate": 1.7486693001226267e-05,
      "loss": 7.9148,
      "step": 154
    },
    {
      "epoch": 0.8946608946608947,
      "grad_norm": 1.2734375,
      "learning_rate": 1.579994297796808e-05,
      "loss": 7.9268,
      "step": 155
    },
    {
      "epoch": 0.9004329004329005,
      "grad_norm": 1.2890625,
      "learning_rate": 1.4196071284790529e-05,
      "loss": 7.8032,
      "step": 156
    },
    {
      "epoch": 0.9062049062049062,
      "grad_norm": 1.1875,
      "learning_rate": 1.2675645497164351e-05,
      "loss": 7.9132,
      "step": 157
    },
    {
      "epoch": 0.911976911976912,
      "grad_norm": 1.171875,
      "learning_rate": 1.1239203660860647e-05,
      "loss": 7.9581,
      "step": 158
    },
    {
      "epoch": 0.9177489177489178,
      "grad_norm": 1.2421875,
      "learning_rate": 9.88725410154842e-06,
      "loss": 7.9374,
      "step": 159
    },
    {
      "epoch": 0.9235209235209235,
      "grad_norm": 1.2109375,
      "learning_rate": 8.620275244908826e-06,
      "loss": 7.9449,
      "step": 160
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 1.2109375,
      "learning_rate": 7.438715447331018e-06,
      "loss": 7.9258,
      "step": 161
    },
    {
      "epoch": 0.935064935064935,
      "grad_norm": 1.234375,
      "learning_rate": 6.342992837248235e-06,
      "loss": 7.9198,
      "step": 162
    },
    {
      "epoch": 0.9408369408369408,
      "grad_norm": 1.2578125,
      "learning_rate": 5.333495167171354e-06,
      "loss": 7.9402,
      "step": 163
    },
    {
      "epoch": 0.9466089466089466,
      "grad_norm": 1.328125,
      "learning_rate": 4.410579676471571e-06,
      "loss": 8.0436,
      "step": 164
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 1.265625,
      "learning_rate": 3.5745729649613034e-06,
      "loss": 7.9191,
      "step": 165
    },
    {
      "epoch": 0.9581529581529582,
      "grad_norm": 1.25,
      "learning_rate": 2.8257708773173627e-06,
      "loss": 7.8881,
      "step": 166
    },
    {
      "epoch": 0.963924963924964,
      "grad_norm": 1.25,
      "learning_rate": 2.1644383983880357e-06,
      "loss": 7.9092,
      "step": 167
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.1953125,
      "learning_rate": 1.5908095594207582e-06,
      "loss": 7.9875,
      "step": 168
    },
    {
      "epoch": 0.9754689754689755,
      "grad_norm": 1.2578125,
      "learning_rate": 1.1050873552433394e-06,
      "loss": 7.962,
      "step": 169
    },
    {
      "epoch": 0.9812409812409812,
      "grad_norm": 1.1953125,
      "learning_rate": 7.074436724286704e-07,
      "loss": 7.9649,
      "step": 170
    },
    {
      "epoch": 0.987012987012987,
      "grad_norm": 1.203125,
      "learning_rate": 3.9801922846766094e-07,
      "loss": 7.9409,
      "step": 171
    },
    {
      "epoch": 0.9927849927849928,
      "grad_norm": 1.203125,
      "learning_rate": 1.7692352197240524e-07,
      "loss": 7.894,
      "step": 172
    },
    {
      "epoch": 0.9985569985569985,
      "grad_norm": 1.265625,
      "learning_rate": 4.423479392709484e-08,
      "loss": 7.9152,
      "step": 173
    },
    {
      "epoch": 1.0057720057720059,
      "grad_norm": 13.5625,
      "learning_rate": 0.0,
      "loss": 9.6277,
      "step": 174
    },
    {
      "epoch": 1.0115440115440115,
      "grad_norm": 13.6875,
      "learning_rate": 0.0002605469934405078,
      "loss": 9.6951,
      "step": 175
    },
    {
      "epoch": 1.0173160173160174,
      "grad_norm": 5.96875,
      "learning_rate": 0.0002582041791754375,
      "loss": 8.8685,
      "step": 176
    },
    {
      "epoch": 1.023088023088023,
      "grad_norm": 8.875,
      "learning_rate": 0.00025586064340081516,
      "loss": 8.8405,
      "step": 177
    },
    {
      "epoch": 1.028860028860029,
      "grad_norm": 11.9375,
      "learning_rate": 0.00025351659221689896,
      "loss": 9.063,
      "step": 178
    },
    {
      "epoch": 1.0346320346320346,
      "grad_norm": 7.9375,
      "learning_rate": 0.0002511722317692747,
      "loss": 8.7219,
      "step": 179
    },
    {
      "epoch": 1.0404040404040404,
      "grad_norm": 5.59375,
      "learning_rate": 0.0002488277682307254,
      "loss": 8.5083,
      "step": 180
    },
    {
      "epoch": 1.046176046176046,
      "grad_norm": 6.03125,
      "learning_rate": 0.00024648340778310105,
      "loss": 8.3409,
      "step": 181
    },
    {
      "epoch": 1.051948051948052,
      "grad_norm": 5.8125,
      "learning_rate": 0.0002441393565991849,
      "loss": 8.3564,
      "step": 182
    },
    {
      "epoch": 1.0577200577200576,
      "grad_norm": 2.96875,
      "learning_rate": 0.00024179582082456253,
      "loss": 8.2286,
      "step": 183
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 3.625,
      "learning_rate": 0.00023945300655949225,
      "loss": 8.1732,
      "step": 184
    },
    {
      "epoch": 1.0692640692640694,
      "grad_norm": 3.328125,
      "learning_rate": 0.00023711111984077966,
      "loss": 8.1342,
      "step": 185
    },
    {
      "epoch": 1.075036075036075,
      "grad_norm": 4.59375,
      "learning_rate": 0.00023477036662365828,
      "loss": 8.1113,
      "step": 186
    },
    {
      "epoch": 1.0808080808080809,
      "grad_norm": 4.59375,
      "learning_rate": 0.00023243095276367684,
      "loss": 8.0563,
      "step": 187
    },
    {
      "epoch": 1.0865800865800865,
      "grad_norm": 2.953125,
      "learning_rate": 0.00023009308399859506,
      "loss": 8.1001,
      "step": 188
    },
    {
      "epoch": 1.0923520923520924,
      "grad_norm": 3.03125,
      "learning_rate": 0.00022775696593029104,
      "loss": 8.0052,
      "step": 189
    },
    {
      "epoch": 1.098124098124098,
      "grad_norm": 2.390625,
      "learning_rate": 0.00022542280400667918,
      "loss": 7.9898,
      "step": 190
    },
    {
      "epoch": 1.103896103896104,
      "grad_norm": 2.921875,
      "learning_rate": 0.00022309080350364253,
      "loss": 7.9606,
      "step": 191
    },
    {
      "epoch": 1.1096681096681096,
      "grad_norm": 2.59375,
      "learning_rate": 0.0002207611695069794,
      "loss": 7.9657,
      "step": 192
    },
    {
      "epoch": 1.1154401154401155,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00021843410689436824,
      "loss": 7.9598,
      "step": 193
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 2.328125,
      "learning_rate": 0.0002161098203173492,
      "loss": 7.9252,
      "step": 194
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 2.453125,
      "learning_rate": 0.000213788514183326,
      "loss": 7.9181,
      "step": 195
    },
    {
      "epoch": 1.1327561327561328,
      "grad_norm": 2.234375,
      "learning_rate": 0.00021147039263759028,
      "loss": 7.936,
      "step": 196
    },
    {
      "epoch": 1.1385281385281385,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00020915565954536742,
      "loss": 7.9029,
      "step": 197
    },
    {
      "epoch": 1.1443001443001444,
      "grad_norm": 1.7890625,
      "learning_rate": 0.0002068445184738886,
      "loss": 7.9554,
      "step": 198
    },
    {
      "epoch": 1.15007215007215,
      "grad_norm": 1.75,
      "learning_rate": 0.00020453717267448717,
      "loss": 7.934,
      "step": 199
    },
    {
      "epoch": 1.155844155844156,
      "grad_norm": 1.828125,
      "learning_rate": 0.00020223382506472505,
      "loss": 7.9336,
      "step": 200
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 1.890625,
      "learning_rate": 0.00019993467821054645,
      "loss": 7.9609,
      "step": 201
    },
    {
      "epoch": 1.1673881673881674,
      "grad_norm": 1.75,
      "learning_rate": 0.00019763993430846395,
      "loss": 7.8881,
      "step": 202
    },
    {
      "epoch": 1.173160173160173,
      "grad_norm": 1.59375,
      "learning_rate": 0.000195349795167776,
      "loss": 7.9386,
      "step": 203
    },
    {
      "epoch": 1.178932178932179,
      "grad_norm": 1.953125,
      "learning_rate": 0.0001930644621928194,
      "loss": 7.9159,
      "step": 204
    },
    {
      "epoch": 1.1847041847041848,
      "grad_norm": 1.734375,
      "learning_rate": 0.0001907841363652568,
      "loss": 7.9238,
      "step": 205
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 1.6171875,
      "learning_rate": 0.00018850901822640146,
      "loss": 7.9779,
      "step": 206
    },
    {
      "epoch": 1.1962481962481963,
      "grad_norm": 1.59375,
      "learning_rate": 0.0001862393078595809,
      "loss": 7.9469,
      "step": 207
    },
    {
      "epoch": 1.202020202020202,
      "grad_norm": 1.46875,
      "learning_rate": 0.0001839752048725408,
      "loss": 7.9002,
      "step": 208
    },
    {
      "epoch": 1.2077922077922079,
      "grad_norm": 1.9296875,
      "learning_rate": 0.00018171690837989057,
      "loss": 7.9703,
      "step": 209
    },
    {
      "epoch": 1.2135642135642135,
      "grad_norm": 1.578125,
      "learning_rate": 0.00017946461698559237,
      "loss": 7.9104,
      "step": 210
    },
    {
      "epoch": 1.2193362193362194,
      "grad_norm": 1.40625,
      "learning_rate": 0.00017721852876549508,
      "loss": 7.961,
      "step": 211
    },
    {
      "epoch": 1.225108225108225,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00017497884124991487,
      "loss": 7.9619,
      "step": 212
    },
    {
      "epoch": 1.230880230880231,
      "grad_norm": 1.625,
      "learning_rate": 0.00017274575140626317,
      "loss": 7.948,
      "step": 213
    },
    {
      "epoch": 1.2366522366522366,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00017051945562172494,
      "loss": 7.9799,
      "step": 214
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 1.5703125,
      "learning_rate": 0.00016830014968598734,
      "loss": 7.9809,
      "step": 215
    },
    {
      "epoch": 1.248196248196248,
      "grad_norm": 1.5625,
      "learning_rate": 0.00016608802877402136,
      "loss": 7.9782,
      "step": 216
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 1.640625,
      "learning_rate": 0.00016388328742891677,
      "loss": 7.906,
      "step": 217
    },
    {
      "epoch": 1.2597402597402598,
      "grad_norm": 1.796875,
      "learning_rate": 0.00016168611954477417,
      "loss": 7.9805,
      "step": 218
    },
    {
      "epoch": 1.2655122655122655,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00015949671834965222,
      "loss": 7.9731,
      "step": 219
    },
    {
      "epoch": 1.2712842712842713,
      "grad_norm": 1.7421875,
      "learning_rate": 0.00015731527638857492,
      "loss": 7.9529,
      "step": 220
    },
    {
      "epoch": 1.277056277056277,
      "grad_norm": 1.609375,
      "learning_rate": 0.00015514198550659793,
      "loss": 7.9353,
      "step": 221
    },
    {
      "epoch": 1.2828282828282829,
      "grad_norm": 1.6875,
      "learning_rate": 0.00015297703683193753,
      "loss": 7.9442,
      "step": 222
    },
    {
      "epoch": 1.2886002886002885,
      "grad_norm": 1.65625,
      "learning_rate": 0.00015082062075916165,
      "loss": 7.9697,
      "step": 223
    },
    {
      "epoch": 1.2943722943722944,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00014867292693244546,
      "loss": 7.9531,
      "step": 224
    },
    {
      "epoch": 1.3001443001443,
      "grad_norm": 1.7734375,
      "learning_rate": 0.000146534144228894,
      "loss": 8.0048,
      "step": 225
    },
    {
      "epoch": 1.305916305916306,
      "grad_norm": 1.7578125,
      "learning_rate": 0.00014440446074193099,
      "loss": 7.9729,
      "step": 226
    },
    {
      "epoch": 1.3116883116883118,
      "grad_norm": 1.65625,
      "learning_rate": 0.00014228406376475743,
      "loss": 7.9435,
      "step": 227
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 1.6875,
      "learning_rate": 0.00014017313977387997,
      "loss": 7.9878,
      "step": 228
    },
    {
      "epoch": 1.3232323232323233,
      "grad_norm": 1.890625,
      "learning_rate": 0.00013807187441271156,
      "loss": 7.9425,
      "step": 229
    },
    {
      "epoch": 1.329004329004329,
      "grad_norm": 1.75,
      "learning_rate": 0.00013598045247524554,
      "loss": 7.9502,
      "step": 230
    },
    {
      "epoch": 1.3347763347763348,
      "grad_norm": 1.421875,
      "learning_rate": 0.00013389905788980294,
      "loss": 8.0311,
      "step": 231
    },
    {
      "epoch": 1.3405483405483405,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00013182787370285865,
      "loss": 7.9075,
      "step": 232
    },
    {
      "epoch": 1.3463203463203464,
      "grad_norm": 2.0625,
      "learning_rate": 0.00012976708206294252,
      "loss": 8.0524,
      "step": 233
    },
    {
      "epoch": 1.352092352092352,
      "grad_norm": 1.78125,
      "learning_rate": 0.00012771686420462054,
      "loss": 7.9453,
      "step": 234
    },
    {
      "epoch": 1.3578643578643579,
      "grad_norm": 1.9453125,
      "learning_rate": 0.0001256774004325565,
      "loss": 7.876,
      "step": 235
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00012364887010565535,
      "loss": 7.9701,
      "step": 236
    },
    {
      "epoch": 1.3694083694083694,
      "grad_norm": 1.6875,
      "learning_rate": 0.00012163145162128947,
      "loss": 7.9952,
      "step": 237
    },
    {
      "epoch": 1.375180375180375,
      "grad_norm": 1.625,
      "learning_rate": 0.0001196253223996099,
      "loss": 7.8433,
      "step": 238
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 1.75,
      "learning_rate": 0.00011763065886794258,
      "loss": 7.927,
      "step": 239
    },
    {
      "epoch": 1.3867243867243868,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00011564763644527357,
      "loss": 8.004,
      "step": 240
    },
    {
      "epoch": 1.3924963924963925,
      "grad_norm": 1.609375,
      "learning_rate": 0.00011367642952682153,
      "loss": 7.9596,
      "step": 241
    },
    {
      "epoch": 1.3982683982683983,
      "grad_norm": 1.9375,
      "learning_rate": 0.00011171721146870015,
      "loss": 7.9188,
      "step": 242
    },
    {
      "epoch": 1.404040404040404,
      "grad_norm": 1.875,
      "learning_rate": 0.00010977015457267365,
      "loss": 7.8878,
      "step": 243
    },
    {
      "epoch": 1.4098124098124099,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00010783543007100266,
      "loss": 7.9751,
      "step": 244
    },
    {
      "epoch": 1.4155844155844157,
      "grad_norm": 2.015625,
      "learning_rate": 0.00010591320811138636,
      "loss": 8.0389,
      "step": 245
    },
    {
      "epoch": 1.4213564213564214,
      "grad_norm": 1.578125,
      "learning_rate": 0.00010400365774199818,
      "loss": 7.9772,
      "step": 246
    },
    {
      "epoch": 1.427128427128427,
      "grad_norm": 1.7578125,
      "learning_rate": 0.0001021069468966194,
      "loss": 7.9205,
      "step": 247
    },
    {
      "epoch": 1.432900432900433,
      "grad_norm": 1.796875,
      "learning_rate": 0.00010022324237987047,
      "loss": 8.0315,
      "step": 248
    },
    {
      "epoch": 1.4386724386724388,
      "grad_norm": 1.7734375,
      "learning_rate": 9.835270985254111e-05,
      "loss": 7.9724,
      "step": 249
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 1.5859375,
      "learning_rate": 9.649551381702168e-05,
      "loss": 7.9357,
      "step": 250
    },
    {
      "epoch": 1.4502164502164503,
      "grad_norm": 1.609375,
      "learning_rate": 9.46518176028364e-05,
      "loss": 7.9434,
      "step": 251
    },
    {
      "epoch": 1.455988455988456,
      "grad_norm": 1.515625,
      "learning_rate": 9.282178335227883e-05,
      "loss": 7.9427,
      "step": 252
    },
    {
      "epoch": 1.4617604617604618,
      "grad_norm": 1.6484375,
      "learning_rate": 9.100557200615292e-05,
      "loss": 7.8914,
      "step": 253
    },
    {
      "epoch": 1.4675324675324675,
      "grad_norm": 1.609375,
      "learning_rate": 8.920334328961918e-05,
      "loss": 7.9333,
      "step": 254
    },
    {
      "epoch": 1.4733044733044733,
      "grad_norm": 1.609375,
      "learning_rate": 8.74152556981474e-05,
      "loss": 7.8835,
      "step": 255
    },
    {
      "epoch": 1.479076479076479,
      "grad_norm": 1.5,
      "learning_rate": 8.56414664835785e-05,
      "loss": 7.97,
      "step": 256
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 1.71875,
      "learning_rate": 8.388213164029459e-05,
      "loss": 7.8971,
      "step": 257
    },
    {
      "epoch": 1.4906204906204907,
      "grad_norm": 1.8828125,
      "learning_rate": 8.213740589150032e-05,
      "loss": 7.959,
      "step": 258
    },
    {
      "epoch": 1.4963924963924964,
      "grad_norm": 1.421875,
      "learning_rate": 8.040744267561637e-05,
      "loss": 7.8851,
      "step": 259
    },
    {
      "epoch": 1.502164502164502,
      "grad_norm": 1.46875,
      "learning_rate": 7.869239413278442e-05,
      "loss": 7.9454,
      "step": 260
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 1.40625,
      "learning_rate": 7.699241109148844e-05,
      "loss": 7.956,
      "step": 261
    },
    {
      "epoch": 1.5137085137085138,
      "grad_norm": 1.6953125,
      "learning_rate": 7.530764305528959e-05,
      "loss": 7.9309,
      "step": 262
    },
    {
      "epoch": 1.5194805194805194,
      "grad_norm": 1.5234375,
      "learning_rate": 7.363823818967824e-05,
      "loss": 7.9078,
      "step": 263
    },
    {
      "epoch": 1.5252525252525253,
      "grad_norm": 1.4921875,
      "learning_rate": 7.198434330904388e-05,
      "loss": 7.8731,
      "step": 264
    },
    {
      "epoch": 1.531024531024531,
      "grad_norm": 1.640625,
      "learning_rate": 7.034610386376342e-05,
      "loss": 7.8961,
      "step": 265
    },
    {
      "epoch": 1.5367965367965368,
      "grad_norm": 1.5078125,
      "learning_rate": 6.872366392741017e-05,
      "loss": 7.9249,
      "step": 266
    },
    {
      "epoch": 1.5425685425685427,
      "grad_norm": 1.3984375,
      "learning_rate": 6.711716618408281e-05,
      "loss": 7.9197,
      "step": 267
    },
    {
      "epoch": 1.5483405483405484,
      "grad_norm": 1.46875,
      "learning_rate": 6.552675191585741e-05,
      "loss": 7.9266,
      "step": 268
    },
    {
      "epoch": 1.554112554112554,
      "grad_norm": 1.484375,
      "learning_rate": 6.395256099036278e-05,
      "loss": 7.9994,
      "step": 269
    },
    {
      "epoch": 1.5598845598845599,
      "grad_norm": 1.6015625,
      "learning_rate": 6.239473184847941e-05,
      "loss": 7.959,
      "step": 270
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 1.546875,
      "learning_rate": 6.085340149216467e-05,
      "loss": 7.9352,
      "step": 271
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.515625,
      "learning_rate": 5.9328705472404546e-05,
      "loss": 7.8702,
      "step": 272
    },
    {
      "epoch": 1.577200577200577,
      "grad_norm": 1.4765625,
      "learning_rate": 5.7820777877292065e-05,
      "loss": 7.9146,
      "step": 273
    },
    {
      "epoch": 1.582972582972583,
      "grad_norm": 1.46875,
      "learning_rate": 5.632975132023585e-05,
      "loss": 7.9127,
      "step": 274
    },
    {
      "epoch": 1.5887445887445888,
      "grad_norm": 1.515625,
      "learning_rate": 5.485575692829678e-05,
      "loss": 7.9399,
      "step": 275
    },
    {
      "epoch": 1.5945165945165947,
      "grad_norm": 1.546875,
      "learning_rate": 5.339892433065654e-05,
      "loss": 7.8492,
      "step": 276
    },
    {
      "epoch": 1.6002886002886003,
      "grad_norm": 1.578125,
      "learning_rate": 5.195938164721767e-05,
      "loss": 7.9104,
      "step": 277
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 1.421875,
      "learning_rate": 5.0537255477335644e-05,
      "loss": 7.9048,
      "step": 278
    },
    {
      "epoch": 1.6118326118326118,
      "grad_norm": 1.6171875,
      "learning_rate": 4.913267088868553e-05,
      "loss": 7.9003,
      "step": 279
    },
    {
      "epoch": 1.6176046176046177,
      "grad_norm": 1.5546875,
      "learning_rate": 4.7745751406263163e-05,
      "loss": 7.8914,
      "step": 280
    },
    {
      "epoch": 1.6233766233766234,
      "grad_norm": 1.5390625,
      "learning_rate": 4.637661900152143e-05,
      "loss": 7.9312,
      "step": 281
    },
    {
      "epoch": 1.629148629148629,
      "grad_norm": 1.625,
      "learning_rate": 4.5025394081643854e-05,
      "loss": 7.9407,
      "step": 282
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 1.4375,
      "learning_rate": 4.3692195478955615e-05,
      "loss": 8.0063,
      "step": 283
    },
    {
      "epoch": 1.6406926406926408,
      "grad_norm": 1.4296875,
      "learning_rate": 4.237714044047258e-05,
      "loss": 7.9012,
      "step": 284
    },
    {
      "epoch": 1.6464646464646466,
      "grad_norm": 1.53125,
      "learning_rate": 4.108034461759036e-05,
      "loss": 7.8957,
      "step": 285
    },
    {
      "epoch": 1.6522366522366523,
      "grad_norm": 1.5546875,
      "learning_rate": 3.980192205591354e-05,
      "loss": 7.8713,
      "step": 286
    },
    {
      "epoch": 1.658008658008658,
      "grad_norm": 1.4453125,
      "learning_rate": 3.8541985185225645e-05,
      "loss": 7.9374,
      "step": 287
    },
    {
      "epoch": 1.6637806637806638,
      "grad_norm": 1.484375,
      "learning_rate": 3.7300644809602155e-05,
      "loss": 7.9204,
      "step": 288
    },
    {
      "epoch": 1.6695526695526697,
      "grad_norm": 1.546875,
      "learning_rate": 3.6078010097665206e-05,
      "loss": 7.9574,
      "step": 289
    },
    {
      "epoch": 1.6753246753246753,
      "grad_norm": 1.6015625,
      "learning_rate": 3.487418857298366e-05,
      "loss": 7.9588,
      "step": 290
    },
    {
      "epoch": 1.681096681096681,
      "grad_norm": 1.53125,
      "learning_rate": 3.368928610461652e-05,
      "loss": 7.9566,
      "step": 291
    },
    {
      "epoch": 1.6868686868686869,
      "grad_norm": 1.421875,
      "learning_rate": 3.2523406897802446e-05,
      "loss": 7.9024,
      "step": 292
    },
    {
      "epoch": 1.6926406926406927,
      "grad_norm": 1.5,
      "learning_rate": 3.1376653484795545e-05,
      "loss": 7.8896,
      "step": 293
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 1.4765625,
      "learning_rate": 3.0249126715848258e-05,
      "loss": 7.8833,
      "step": 294
    },
    {
      "epoch": 1.704184704184704,
      "grad_norm": 1.5078125,
      "learning_rate": 2.9140925750342357e-05,
      "loss": 7.9611,
      "step": 295
    },
    {
      "epoch": 1.70995670995671,
      "grad_norm": 1.5859375,
      "learning_rate": 2.8052148048068076e-05,
      "loss": 7.8818,
      "step": 296
    },
    {
      "epoch": 1.7157287157287158,
      "grad_norm": 1.5859375,
      "learning_rate": 2.698288936065338e-05,
      "loss": 7.9421,
      "step": 297
    },
    {
      "epoch": 1.7215007215007216,
      "grad_norm": 1.46875,
      "learning_rate": 2.593324372314318e-05,
      "loss": 7.8933,
      "step": 298
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 1.453125,
      "learning_rate": 2.4903303445729276e-05,
      "loss": 7.8716,
      "step": 299
    },
    {
      "epoch": 1.733044733044733,
      "grad_norm": 1.5,
      "learning_rate": 2.3893159105632362e-05,
      "loss": 7.8928,
      "step": 300
    },
    {
      "epoch": 1.7388167388167388,
      "grad_norm": 1.4375,
      "learning_rate": 2.2902899539136435e-05,
      "loss": 7.9181,
      "step": 301
    },
    {
      "epoch": 1.7445887445887447,
      "grad_norm": 1.3984375,
      "learning_rate": 2.1932611833775846e-05,
      "loss": 7.9009,
      "step": 302
    },
    {
      "epoch": 1.7503607503607503,
      "grad_norm": 1.3984375,
      "learning_rate": 2.0982381320676647e-05,
      "loss": 7.9324,
      "step": 303
    },
    {
      "epoch": 1.756132756132756,
      "grad_norm": 1.4609375,
      "learning_rate": 2.0052291567052295e-05,
      "loss": 7.9134,
      "step": 304
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 1.53125,
      "learning_rate": 1.9142424368854162e-05,
      "loss": 7.8904,
      "step": 305
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 1.390625,
      "learning_rate": 1.825285974357835e-05,
      "loss": 7.9514,
      "step": 306
    },
    {
      "epoch": 1.7734487734487736,
      "grad_norm": 1.5234375,
      "learning_rate": 1.738367592322837e-05,
      "loss": 7.8585,
      "step": 307
    },
    {
      "epoch": 1.7792207792207793,
      "grad_norm": 1.4296875,
      "learning_rate": 1.6534949347435185e-05,
      "loss": 7.9624,
      "step": 308
    },
    {
      "epoch": 1.784992784992785,
      "grad_norm": 1.390625,
      "learning_rate": 1.5706754656734908e-05,
      "loss": 7.8932,
      "step": 309
    },
    {
      "epoch": 1.7907647907647908,
      "grad_norm": 1.4921875,
      "learning_rate": 1.4899164686004413e-05,
      "loss": 7.91,
      "step": 310
    },
    {
      "epoch": 1.7965367965367967,
      "grad_norm": 1.4921875,
      "learning_rate": 1.4112250458055975e-05,
      "loss": 7.8912,
      "step": 311
    },
    {
      "epoch": 1.8023088023088023,
      "grad_norm": 1.4609375,
      "learning_rate": 1.3346081177391473e-05,
      "loss": 7.9202,
      "step": 312
    },
    {
      "epoch": 1.808080808080808,
      "grad_norm": 1.4140625,
      "learning_rate": 1.2600724224115845e-05,
      "loss": 7.8589,
      "step": 313
    },
    {
      "epoch": 1.8138528138528138,
      "grad_norm": 1.4921875,
      "learning_rate": 1.1876245148011694e-05,
      "loss": 7.879,
      "step": 314
    },
    {
      "epoch": 1.8196248196248197,
      "grad_norm": 1.484375,
      "learning_rate": 1.1172707662774561e-05,
      "loss": 7.87,
      "step": 315
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 1.5,
      "learning_rate": 1.0490173640409468e-05,
      "loss": 7.9098,
      "step": 316
    },
    {
      "epoch": 1.8311688311688312,
      "grad_norm": 1.40625,
      "learning_rate": 9.828703105789983e-06,
      "loss": 7.9283,
      "step": 317
    },
    {
      "epoch": 1.8369408369408369,
      "grad_norm": 1.4609375,
      "learning_rate": 9.188354231378899e-06,
      "loss": 7.9774,
      "step": 318
    },
    {
      "epoch": 1.8427128427128427,
      "grad_norm": 1.5078125,
      "learning_rate": 8.569183332112846e-06,
      "loss": 7.8334,
      "step": 319
    },
    {
      "epoch": 1.8484848484848486,
      "grad_norm": 1.609375,
      "learning_rate": 7.971244860449395e-06,
      "loss": 7.9484,
      "step": 320
    },
    {
      "epoch": 1.8542568542568543,
      "grad_norm": 1.53125,
      "learning_rate": 7.394591401578166e-06,
      "loss": 7.8994,
      "step": 321
    },
    {
      "epoch": 1.86002886002886,
      "grad_norm": 1.4296875,
      "learning_rate": 6.839273668796747e-06,
      "loss": 7.859,
      "step": 322
    },
    {
      "epoch": 1.8658008658008658,
      "grad_norm": 1.34375,
      "learning_rate": 6.3053404990502384e-06,
      "loss": 7.8918,
      "step": 323
    },
    {
      "epoch": 1.8715728715728717,
      "grad_norm": 1.3671875,
      "learning_rate": 5.7928388486366555e-06,
      "loss": 7.881,
      "step": 324
    },
    {
      "epoch": 1.8773448773448773,
      "grad_norm": 1.453125,
      "learning_rate": 5.301813789077264e-06,
      "loss": 7.8644,
      "step": 325
    },
    {
      "epoch": 1.883116883116883,
      "grad_norm": 1.4453125,
      "learning_rate": 4.832308503152832e-06,
      "loss": 7.8725,
      "step": 326
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.4375,
      "learning_rate": 4.384364281105973e-06,
      "loss": 7.9281,
      "step": 327
    },
    {
      "epoch": 1.8946608946608947,
      "grad_norm": 1.40625,
      "learning_rate": 3.9580205170098856e-06,
      "loss": 7.8786,
      "step": 328
    },
    {
      "epoch": 1.9004329004329006,
      "grad_norm": 1.3984375,
      "learning_rate": 3.553314705303845e-06,
      "loss": 7.9188,
      "step": 329
    },
    {
      "epoch": 1.9062049062049062,
      "grad_norm": 1.4140625,
      "learning_rate": 3.1702824374959527e-06,
      "loss": 7.9451,
      "step": 330
    },
    {
      "epoch": 1.9119769119769119,
      "grad_norm": 1.5546875,
      "learning_rate": 2.8089573990328076e-06,
      "loss": 7.9284,
      "step": 331
    },
    {
      "epoch": 1.9177489177489178,
      "grad_norm": 1.4296875,
      "learning_rate": 2.469371366337264e-06,
      "loss": 8.0018,
      "step": 332
    },
    {
      "epoch": 1.9235209235209236,
      "grad_norm": 1.3828125,
      "learning_rate": 2.1515542040138335e-06,
      "loss": 7.9066,
      "step": 333
    },
    {
      "epoch": 1.9292929292929293,
      "grad_norm": 1.3984375,
      "learning_rate": 1.8555338622222583e-06,
      "loss": 7.8789,
      "step": 334
    },
    {
      "epoch": 1.935064935064935,
      "grad_norm": 1.4140625,
      "learning_rate": 1.581336374219422e-06,
      "loss": 7.8698,
      "step": 335
    },
    {
      "epoch": 1.9408369408369408,
      "grad_norm": 1.5078125,
      "learning_rate": 1.3289858540699584e-06,
      "loss": 7.9696,
      "step": 336
    },
    {
      "epoch": 1.9466089466089467,
      "grad_norm": 1.4921875,
      "learning_rate": 1.0985044945254763e-06,
      "loss": 7.9127,
      "step": 337
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 1.5390625,
      "learning_rate": 8.899125650729256e-07,
      "loss": 7.9764,
      "step": 338
    },
    {
      "epoch": 1.9581529581529582,
      "grad_norm": 1.375,
      "learning_rate": 7.032284101518849e-07,
      "loss": 7.8838,
      "step": 339
    },
    {
      "epoch": 1.9639249639249639,
      "grad_norm": 1.4921875,
      "learning_rate": 5.384684475414625e-07,
      "loss": 7.8851,
      "step": 340
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 1.390625,
      "learning_rate": 3.9564716691622984e-07,
      "loss": 7.9319,
      "step": 341
    },
    {
      "epoch": 1.9754689754689756,
      "grad_norm": 1.421875,
      "learning_rate": 2.7477712857215677e-07,
      "loss": 7.9022,
      "step": 342
    },
    {
      "epoch": 1.9812409812409812,
      "grad_norm": 1.3984375,
      "learning_rate": 1.7586896232180128e-07,
      "loss": 7.9228,
      "step": 343
    },
    {
      "epoch": 1.987012987012987,
      "grad_norm": 1.390625,
      "learning_rate": 9.89313665596403e-08,
      "loss": 7.8996,
      "step": 344
    },
    {
      "epoch": 1.9927849927849928,
      "grad_norm": 1.421875,
      "learning_rate": 4.3971107497042806e-08,
      "loss": 7.894,
      "step": 345
    },
    {
      "epoch": 1.9985569985569986,
      "grad_norm": 1.421875,
      "learning_rate": 1.0993018567162505e-08,
      "loss": 7.9089,
      "step": 346
    }
  ],
  "logging_steps": 1,
  "max_steps": 346,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.35988519108608e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

2025-05-15 18:28:20,609 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-15 18:28:20,627 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-15 18:28:27,517 - IntimeModelSelector - INFO - model selection weights control: {}
2025-05-15 18:28:27,768 - ModelQuantizer - INFO - Using model quantizator.
2025-05-15 18:28:27,769 - ModelDequantizer - INFO - Using model dequantizator.
2025-05-15 18:28:27,776 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Initializing BaseModelController workflow.
2025-05-15 18:28:27,778 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Beginning model controller run.
2025-05-15 18:28:27,778 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Start FedAvg.
2025-05-15 18:28:27,779 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - loading initial model from persistor
2025-05-15 18:28:27,779 - PTFileModelPersistor - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Both source_ckpt_file_full_name and ckpt_preload_path are not provided. Using the default model weights initialized on the persistor side.
2025-05-15 18:28:27,782 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Round 0 started.
2025-05-15 18:28:27,782 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sampled clients: ['site-code', 'site-math', 'site-lbv1']
2025-05-15 18:28:27,783 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sending task train to ['site-code', 'site-math', 'site-lbv1']
2025-05-15 18:28:34,368 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Running quantization...
2025-05-15 18:28:34,369 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Running quantization on 179 variables
2025-05-15 18:28:35,246 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-15 18:28:35,247 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-15 18:28:35,799 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=53edd170-ca16-40ba-beb1-c6e588e0f38b] - Running quantization...
2025-05-15 18:28:35,862 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=53edd170-ca16-40ba-beb1-c6e588e0f38b] - Already quantized, skip quantization
2025-05-15 18:32:39,005 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Running dequantization...
2025-05-15 18:32:39,006 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Running dequantization on 179 variables
2025-05-15 18:32:43,111 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:32:43,114 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=412ac6fd-7905-4052-9f24-dd4cc7ac21ad] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:33:02,501 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=53edd170-ca16-40ba-beb1-c6e588e0f38b] - Running dequantization...
2025-05-15 18:33:02,502 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=53edd170-ca16-40ba-beb1-c6e588e0f38b] - Running dequantization on 179 variables
2025-05-15 18:33:06,460 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=53edd170-ca16-40ba-beb1-c6e588e0f38b] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:33:06,462 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=53edd170-ca16-40ba-beb1-c6e588e0f38b] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:33:13,933 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Running quantization...
2025-05-15 18:33:13,933 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Already quantized, skip quantization
2025-05-15 18:37:26,591 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Running dequantization...
2025-05-15 18:37:26,592 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Running dequantization on 179 variables
2025-05-15 18:37:30,590 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:37:30,592 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:37:30,972 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - aggregating 3 update(s) at round 0
2025-05-15 18:37:41,355 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Start persist model on server.
2025-05-15 18:38:34,801 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - End persist model on server.
2025-05-15 18:38:34,805 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Round 1 started.
2025-05-15 18:38:34,808 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Sampled clients: ['site-code', 'site-math', 'site-lbv1']
2025-05-15 18:38:34,808 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47302b76-813b-47d7-8813-213e8870a2d0] - Sending task train to ['site-code', 'site-math', 'site-lbv1']
2025-05-15 18:38:35,314 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Running quantization...
2025-05-15 18:38:35,315 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Running quantization on 179 variables
2025-05-15 18:38:39,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Running quantization...
2025-05-15 18:38:39,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Running quantization on 179 variables
2025-05-15 18:38:39,079 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.embed_tokens.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,080 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,080 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,083 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,084 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,084 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,085 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,085 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,086 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,086 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,087 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,088 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,088 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,089 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,089 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,090 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,090 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,091 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,091 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,092 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,093 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,093 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,094 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,094 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,095 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,095 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,096 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,096 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,097 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,098 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,098 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,099 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,099 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,100 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,100 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,101 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,101 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,102 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,102 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,103 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,103 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,104 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,104 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,105 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,105 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,106 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,106 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,107 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,108 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,108 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,109 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,109 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,110 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,110 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,111 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,111 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,112 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,112 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,113 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,113 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,114 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,114 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,115 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,115 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,116 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,116 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,117 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,117 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,118 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,118 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,119 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,202 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,203 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,204 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,204 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.7.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,243 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,244 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,491 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,492 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,563 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,564 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,844 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,844 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,915 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:39,916 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,161 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,232 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,233 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,480 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,481 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,482 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.11.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,536 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,782 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,783 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,784 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.12.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,838 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:40,839 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,090 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,091 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,092 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,146 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,146 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,389 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,390 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,462 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,462 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,707 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,708 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,709 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,763 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:41,763 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:42,010 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:42,011 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:42,012 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:38:42,994 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Quantized 98/179 params. Before quantization: 4392.39 MB. After quantization: 1560.14 MB with meta: 0.00 MB.
2025-05-15 18:38:42,996 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Quantized 140/179 params. Before quantization: 5632.37 MB. After quantization: 2800.12 MB with meta: 0.00 MB.
2025-05-15 18:38:42,999 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Quantized from {'model.model.embed_tokens.weight': 'float16', 'model.model.layers.0.self_attn.q_proj.weight': 'float16', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float16', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float16', 'model.model.layers.0.mlp.up_proj.weight': 'float16', 'model.model.layers.0.mlp.down_proj.weight': 'float16', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float16', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float16', 'model.model.layers.1.mlp.up_proj.weight': 'float16', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float16', 'model.model.layers.2.self_attn.o_proj.weight': 'float16', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float16', 'model.model.layers.2.mlp.up_proj.weight': 'float16', 'model.model.layers.2.mlp.down_proj.weight': 'float16', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float16', 'model.model.layers.3.mlp.up_proj.weight': 'float16', 'model.model.layers.3.mlp.down_proj.weight': 'float16', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float16', 'model.model.layers.4.self_attn.o_proj.weight': 'float16', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float16', 'model.model.layers.4.mlp.up_proj.weight': 'float16', 'model.model.layers.4.mlp.down_proj.weight': 'float16', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float16', 'model.model.layers.5.self_attn.o_proj.weight': 'float16', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float16', 'model.model.layers.5.mlp.up_proj.weight': 'float16', 'model.model.layers.5.mlp.down_proj.weight': 'float16', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float16', 'model.model.layers.6.self_attn.o_proj.weight': 'float16', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float16', 'model.model.layers.6.mlp.up_proj.weight': 'float16', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float16', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-15 18:38:43,000 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float16', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float16', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-15 18:42:56,704 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Running dequantization...
2025-05-15 18:42:56,705 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Running dequantization on 179 variables
2025-05-15 18:43:00,645 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:43:00,647 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:43:00,649 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d21d2d15-49a2-47ff-8ae2-c9a379538f89] - validation metric -8.16836166381836 from client site-math
2025-05-15 18:43:16,471 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Running dequantization...
2025-05-15 18:43:16,472 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Running dequantization on 179 variables
2025-05-15 18:43:20,348 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:43:20,349 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:43:20,351 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4f3b330-1c9d-4c26-839e-98761ce6edd4] - validation metric -7.632326602935791 from client site-lbv1
2025-05-15 18:43:26,835 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Running quantization...
2025-05-15 18:43:26,836 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Already quantized, skip quantization
2025-05-15 18:48:23,612 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Running dequantization...
2025-05-15 18:48:23,613 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Running dequantization on 179 variables
2025-05-15 18:48:27,538 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:48:27,540 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:48:27,542 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - validation metric -9.77275276184082 from client site-code
2025-05-15 18:48:27,962 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - new best validation metric at round 1: -8.52448034286499
2025-05-15 18:48:58,212 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - aggregating 3 update(s) at round 1
2025-05-15 18:49:09,799 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Start persist model on server.
2025-05-15 18:50:12,418 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - End persist model on server.
2025-05-15 18:50:12,422 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Round 2 started.
2025-05-15 18:50:12,425 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Sampled clients: ['site-code', 'site-math', 'site-lbv1']
2025-05-15 18:50:12,430 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=25fdb978-3af6-4d39-974d-3b17b2666927] - Sending task train to ['site-code', 'site-math', 'site-lbv1']
2025-05-15 18:50:12,474 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Running quantization...
2025-05-15 18:50:12,475 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Running quantization on 179 variables
2025-05-15 18:50:14,427 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Running quantization...
2025-05-15 18:50:14,428 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Running quantization on 179 variables
2025-05-15 18:50:14,429 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.embed_tokens.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,429 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,430 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,430 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,431 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,431 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,432 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,432 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,433 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,433 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,434 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,434 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,435 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,435 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,436 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,436 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,437 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,437 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,438 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,438 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,439 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,439 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,440 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,440 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,441 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,442 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,442 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,443 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,443 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,444 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,444 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,523 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,524 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,525 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,525 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,526 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,547 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,548 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,781 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,782 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,855 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:14,855 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,092 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,093 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,396 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,398 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,470 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,471 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,705 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,706 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,779 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:15,779 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,014 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,015 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,089 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,090 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,322 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,323 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,401 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,402 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,636 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,637 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,714 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,715 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,949 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:16,950 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,029 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,030 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,264 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,265 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,340 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,341 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,574 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,575 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,656 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,657 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,892 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,892 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:17,968 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:18,202 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:18,203 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:18,279 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:18,279 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:18,517 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:18,518 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:18,519 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-15 18:50:19,473 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-15 18:50:19,474 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-15 18:50:19,486 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Quantized 90/179 params. Before quantization: 4896.25 MB. After quantization: 2064.00 MB with meta: 0.00 MB.
2025-05-15 18:50:19,554 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Quantized from {'model.model.embed_tokens.weight': 'float16', 'model.model.layers.0.self_attn.q_proj.weight': 'float16', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float16', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float16', 'model.model.layers.0.mlp.up_proj.weight': 'float16', 'model.model.layers.0.mlp.down_proj.weight': 'float16', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float16', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float16', 'model.model.layers.1.mlp.up_proj.weight': 'float16', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float16', 'model.model.layers.2.self_attn.o_proj.weight': 'float16', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float16', 'model.model.layers.2.mlp.up_proj.weight': 'float16', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-15 18:50:43,831 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Running quantization...
2025-05-15 18:50:43,834 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Already quantized, skip quantization
2025-05-15 18:51:39,345 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Running dequantization...
2025-05-15 18:51:39,346 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Running dequantization on 179 variables
2025-05-15 18:51:43,290 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:51:43,293 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:51:43,295 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d0f76b46-c172-4f94-a1e3-9f90c853470a] - validation metric -9.853433609008789 from client site-code
2025-05-15 18:51:50,015 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=a4bc5b25-f01d-4187-abcc-1eebfd822f6e] - Running quantization...
2025-05-15 18:51:50,015 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=a4bc5b25-f01d-4187-abcc-1eebfd822f6e] - Already quantized, skip quantization
2025-05-15 18:52:58,019 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a4bc5b25-f01d-4187-abcc-1eebfd822f6e] - Running dequantization...
2025-05-15 18:52:58,020 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a4bc5b25-f01d-4187-abcc-1eebfd822f6e] - Running dequantization on 179 variables
2025-05-15 18:53:01,896 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a4bc5b25-f01d-4187-abcc-1eebfd822f6e] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:53:01,898 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a4bc5b25-f01d-4187-abcc-1eebfd822f6e] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:53:01,900 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a4bc5b25-f01d-4187-abcc-1eebfd822f6e] - validation metric -7.2994208335876465 from client site-lbv1
2025-05-15 18:54:32,937 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Running dequantization...
2025-05-15 18:54:32,937 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Running dequantization on 179 variables
2025-05-15 18:54:37,062 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-15 18:54:37,064 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-15 18:54:37,067 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - validation metric -8.167317390441895 from client site-math
2025-05-15 18:54:37,546 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - new best validation metric at round 2: -8.440057277679443
2025-05-15 18:55:15,154 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - aggregating 3 update(s) at round 2
2025-05-15 18:55:28,852 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Start persist model on server.
2025-05-15 18:56:28,849 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - End persist model on server.
2025-05-15 18:56:28,857 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7079b0b4-abba-4809-8d1a-2f7c36df37f0] - Finished FedAvg.

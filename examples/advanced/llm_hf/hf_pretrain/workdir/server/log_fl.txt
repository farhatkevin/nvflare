2025-05-20 23:14:22,282 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-20 23:14:22,292 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-20 23:14:26,354 - IntimeModelSelector - INFO - model selection weights control: {}
2025-05-20 23:14:26,438 - ModelQuantizer - INFO - Using model quantizator.
2025-05-20 23:14:26,439 - ModelDequantizer - INFO - Using model dequantizator.
2025-05-20 23:14:26,443 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Initializing BaseModelController workflow.
2025-05-20 23:14:26,443 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Beginning model controller run.
2025-05-20 23:14:26,443 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Start FedAvg.
2025-05-20 23:14:26,444 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - loading initial model from persistor
2025-05-20 23:14:26,445 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Round 0 started.
2025-05-20 23:14:26,445 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sampled clients: ['site-code', 'site-math', 'site-lbv1']
2025-05-20 23:14:26,445 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sending task train to ['site-code', 'site-math', 'site-lbv1']
2025-05-20 23:14:31,513 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Running quantization...
2025-05-20 23:14:31,514 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Running quantization on 179 variables
2025-05-20 23:14:31,646 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Running quantization...
2025-05-20 23:14:31,646 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Running quantization on 179 variables
2025-05-20 23:14:31,646 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.embed_tokens.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,647 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,647 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,647 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,647 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,648 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,648 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,701 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,701 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,702 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,702 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,702 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,702 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,703 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,709 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,710 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,743 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.1.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,767 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,768 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,769 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,769 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,807 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,807 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,808 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.2.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,905 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,905 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,906 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,906 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,907 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,925 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,926 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,953 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,976 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:31,976 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,074 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.4.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,075 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,075 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,146 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.4.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,147 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,147 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.5.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,177 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,178 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,178 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.5.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,224 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,224 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,225 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,225 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,226 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,226 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,227 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,228 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,291 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,291 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,292 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,292 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,293 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,293 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,294 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,294 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,295 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,354 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,355 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,355 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,356 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,356 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,357 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,357 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,358 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,359 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,412 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,413 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,413 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,414 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,414 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,429 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,430 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,430 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,431 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,465 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,466 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,467 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,467 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.10.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,467 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.10.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,467 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.10.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,480 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,480 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,481 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.10.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,533 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,534 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,534 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.11.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.11.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,545 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.11.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,546 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,546 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,617 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,618 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,618 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,618 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,619 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,630 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,631 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,664 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,665 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,665 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,666 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,666 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,681 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,681 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,682 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,682 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,714 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,732 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,733 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,734 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.14.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,745 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.14.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,745 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.14.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,746 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,746 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,821 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,822 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,822 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,822 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,823 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,823 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,824 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,825 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,886 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,887 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,887 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:32,887 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:14:33,036 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Quantized 138/179 params. Before quantization: 5376.43 MB. After quantization: 2544.18 MB with meta: 0.00 MB.
2025-05-20 23:14:33,037 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float16', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float16', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float16', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float16', 'model.model.layers.11.self_attn.k_proj.weight': 'float16', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float16', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float16', 'model.model.layers.12.self_attn.k_proj.weight': 'float16', 'model.model.layers.12.self_attn.v_proj.weight': 'float16', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float16', 'model.model.layers.12.mlp.down_proj.weight': 'float16', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float16', 'model.model.layers.13.self_attn.o_proj.weight': 'float16', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float16', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float16', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float16', 'model.model.layers.14.self_attn.o_proj.weight': 'float16', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-20 23:14:33,136 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Quantized 88/179 params. Before quantization: 4624.33 MB. After quantization: 1792.08 MB with meta: 0.00 MB.
2025-05-20 23:14:33,222 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Quantized from {'model.model.embed_tokens.weight': 'float16', 'model.model.layers.0.self_attn.q_proj.weight': 'float16', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float16', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float16', 'model.model.layers.0.mlp.down_proj.weight': 'float16', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float16', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float16', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float16', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float16', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float16', 'model.model.layers.6.self_attn.o_proj.weight': 'float16', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float16', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float16', 'model.model.layers.7.self_attn.v_proj.weight': 'float16', 'model.model.layers.7.self_attn.o_proj.weight': 'float16', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float16', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float16', 'model.model.layers.8.self_attn.k_proj.weight': 'float16', 'model.model.layers.8.self_attn.v_proj.weight': 'float16', 'model.model.layers.8.self_attn.o_proj.weight': 'float16', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float16', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float16', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float16', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float16', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float16', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float16', 'model.model.layers.10.self_attn.k_proj.weight': 'float16', 'model.model.layers.10.self_attn.v_proj.weight': 'float16', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float16', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float16', 'model.model.layers.15.self_attn.v_proj.weight': 'float16', 'model.model.layers.15.self_attn.o_proj.weight': 'float16', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float16', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-20 23:15:22,184 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Running dequantization...
2025-05-20 23:15:22,185 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Running dequantization on 179 variables
2025-05-20 23:15:26,143 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:15:26,144 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=87011389-6fdd-46e8-9147-e9b7de05c906] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:15:46,983 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Running dequantization...
2025-05-20 23:15:46,984 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Running dequantization on 179 variables
2025-05-20 23:15:50,879 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:15:50,881 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=14db1c33-a33c-482a-85b8-1db75a112cfb] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:15:56,009 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Running quantization...
2025-05-20 23:15:56,009 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Already quantized, skip quantization
2025-05-20 23:16:57,023 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Running dequantization...
2025-05-20 23:16:57,024 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Running dequantization on 179 variables
2025-05-20 23:17:00,998 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:17:00,999 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:17:01,493 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - aggregating 3 update(s) at round 0
2025-05-20 23:17:11,459 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Start persist model on server.
2025-05-20 23:18:02,800 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - End persist model on server.
2025-05-20 23:18:02,800 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Round 1 started.
2025-05-20 23:18:02,800 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Sampled clients: ['site-code', 'site-math', 'site-lbv1']
2025-05-20 23:18:02,801 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=582e1cd9-4d8f-4540-9b5c-23e8782d3f74] - Sending task train to ['site-code', 'site-math', 'site-lbv1']
2025-05-20 23:18:02,992 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Running quantization...
2025-05-20 23:18:02,992 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Running quantization on 179 variables
2025-05-20 23:18:05,072 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Running quantization...
2025-05-20 23:18:05,072 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Running quantization on 179 variables
2025-05-20 23:18:05,072 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.embed_tokens.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,072 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,073 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,073 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,073 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,073 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,074 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,074 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,074 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,074 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,075 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,075 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,075 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,075 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,076 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,076 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,076 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,076 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,076 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,077 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,077 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,077 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,077 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,079 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,079 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,079 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,079 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,080 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,080 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,080 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,080 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,237 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,244 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,244 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,245 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,284 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,284 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,573 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,574 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,643 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,643 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,879 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,880 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,947 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:05,948 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,183 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,184 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,269 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,269 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,503 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,503 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,504 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.8.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,564 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,565 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,799 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,799 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,843 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.9.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,861 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:06,861 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,096 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,096 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,140 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.10.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,168 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,168 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,403 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,403 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,464 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.11.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,464 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,465 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,700 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,701 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,778 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:07,779 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,020 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,020 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,047 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.13.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,090 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,091 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,325 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,325 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,386 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.14.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,387 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,387 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,622 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,622 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,700 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,701 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,938 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,938 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:08,938 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:18:09,890 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Quantized 136/179 params. Before quantization: 5616.36 MB. After quantization: 2784.11 MB with meta: 0.00 MB.
2025-05-20 23:18:09,892 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Quantized 122/179 params. Before quantization: 4808.40 MB. After quantization: 1976.14 MB with meta: 0.00 MB.
2025-05-20 23:18:09,893 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float16', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float16', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float16', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float16', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float16', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float16', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-20 23:18:09,894 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Quantized from {'model.model.embed_tokens.weight': 'float16', 'model.model.layers.0.self_attn.q_proj.weight': 'float16', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float16', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float16', 'model.model.layers.0.mlp.up_proj.weight': 'float16', 'model.model.layers.0.mlp.down_proj.weight': 'float16', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float16', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float16', 'model.model.layers.1.mlp.up_proj.weight': 'float16', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float16', 'model.model.layers.2.self_attn.o_proj.weight': 'float16', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float16', 'model.model.layers.2.mlp.up_proj.weight': 'float16', 'model.model.layers.2.mlp.down_proj.weight': 'float16', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float16', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-20 23:19:20,116 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Running dequantization...
2025-05-20 23:19:20,116 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Running dequantization on 179 variables
2025-05-20 23:19:24,077 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:19:24,078 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:19:24,079 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=7907a425-af0d-4870-bb70-be75eb0166bf] - validation metric nan from client site-math
2025-05-20 23:19:39,629 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Running dequantization...
2025-05-20 23:19:39,630 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Running dequantization on 179 variables
2025-05-20 23:19:43,512 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:19:43,514 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:19:43,515 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=532a81ea-5edc-4853-9975-d178d6e59b4a] - validation metric nan from client site-code
2025-05-20 23:19:48,691 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Running quantization...
2025-05-20 23:19:48,692 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Already quantized, skip quantization
2025-05-20 23:21:09,018 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Running dequantization...
2025-05-20 23:21:09,019 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Running dequantization on 179 variables
2025-05-20 23:21:12,947 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:21:12,948 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:21:12,949 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - validation metric nan from client site-lbv1
2025-05-20 23:21:13,345 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - aggregating 3 update(s) at round 1
2025-05-20 23:21:23,029 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Start persist model on server.
2025-05-20 23:22:20,880 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - End persist model on server.
2025-05-20 23:22:20,881 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Round 2 started.
2025-05-20 23:22:20,881 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Sampled clients: ['site-code', 'site-math', 'site-lbv1']
2025-05-20 23:22:20,882 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=392ef6d7-ab77-4a78-81cd-ebcdba5c44e0] - Sending task train to ['site-code', 'site-math', 'site-lbv1']
2025-05-20 23:22:21,010 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Running quantization...
2025-05-20 23:22:21,011 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Running quantization on 179 variables
2025-05-20 23:22:21,019 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Running quantization...
2025-05-20 23:22:21,019 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Running quantization on 179 variables
2025-05-20 23:22:22,073 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,074 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,310 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,310 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,311 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,367 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,367 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,606 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,607 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,607 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,663 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,663 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,901 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,902 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,902 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,960 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:22,960 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,197 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,197 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,197 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,255 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,256 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,493 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,493 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,493 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,494 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,532 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,533 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,770 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,771 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,771 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,829 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:23,829 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,066 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,067 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,067 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,085 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.7.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,106 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,106 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,345 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,345 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,345 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.8.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,403 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,403 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,641 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,641 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,641 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,680 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.9.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,681 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,681 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,918 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,919 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,919 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.10.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,977 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:24,977 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,215 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,216 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,216 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.11.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,255 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.11.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,256 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,256 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,494 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,495 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,495 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.12.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,553 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,553 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,788 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,789 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,789 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,846 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:25,847 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.14.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,101 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.14.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,119 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,119 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,355 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,356 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,356 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,414 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,414 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,649 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,650 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:26,650 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-20 23:22:27,586 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-20 23:22:27,587 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-20 23:22:27,639 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Quantized 94/179 params. Before quantization: 5504.25 MB. After quantization: 2672.00 MB with meta: 0.00 MB.
2025-05-20 23:22:27,686 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float16', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float16', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float16', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float16', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float16', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float16', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float16', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float16', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float16', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-20 23:23:36,174 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Running dequantization...
2025-05-20 23:23:36,175 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Running dequantization on 179 variables
2025-05-20 23:23:40,093 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:23:40,095 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:23:40,096 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-math, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=47756b1d-5e8a-469b-85d6-ca1d87283a35] - validation metric nan from client site-math
2025-05-20 23:23:50,069 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Running dequantization...
2025-05-20 23:23:50,069 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Running dequantization on 179 variables
2025-05-20 23:23:54,013 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:23:54,015 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:23:54,016 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-lbv1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=daaf1cc3-b82c-46a1-9804-f04a18d5bb4f] - validation metric nan from client site-lbv1
2025-05-20 23:23:59,227 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Running quantization...
2025-05-20 23:23:59,228 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Already quantized, skip quantization
2025-05-20 23:25:39,968 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Running dequantization...
2025-05-20 23:25:39,968 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Running dequantization on 179 variables
2025-05-20 23:25:43,897 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-20 23:25:43,898 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-20 23:25:43,899 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - validation metric nan from client site-code
2025-05-20 23:25:44,439 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - aggregating 3 update(s) at round 2
2025-05-20 23:25:54,176 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Start persist model on server.
2025-05-20 23:26:46,448 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - End persist model on server.
2025-05-20 23:26:46,449 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-code, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=1fd3a67f-07eb-471f-9e9a-aebe2625cd6a] - Finished FedAvg.

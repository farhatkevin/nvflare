{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9985569985569986,
  "eval_steps": 500,
  "global_step": 519,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005772005772005772,
      "grad_norm": 13.5,
      "learning_rate": 0.0,
      "loss": 8.5981,
      "step": 1
    },
    {
      "epoch": 0.011544011544011544,
      "grad_norm": 10.875,
      "learning_rate": 8.333333333333333e-05,
      "loss": 8.6744,
      "step": 2
    },
    {
      "epoch": 0.017316017316017316,
      "grad_norm": 7.625,
      "learning_rate": 0.00016666666666666666,
      "loss": 7.9187,
      "step": 3
    },
    {
      "epoch": 0.023088023088023088,
      "grad_norm": 16.625,
      "learning_rate": 0.00025,
      "loss": 7.9568,
      "step": 4
    },
    {
      "epoch": 0.02886002886002886,
      "grad_norm": 101.0,
      "learning_rate": 0.0003333333333333333,
      "loss": 11.9362,
      "step": 5
    },
    {
      "epoch": 0.03463203463203463,
      "grad_norm": 28.5,
      "learning_rate": 0.0004166666666666667,
      "loss": 9.6598,
      "step": 6
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 35.0,
      "learning_rate": 0.0005,
      "loss": 9.4126,
      "step": 7
    },
    {
      "epoch": 0.046176046176046176,
      "grad_norm": 38.5,
      "learning_rate": 0.0004999557652060729,
      "loss": 9.1622,
      "step": 8
    },
    {
      "epoch": 0.05194805194805195,
      "grad_norm": 8.25,
      "learning_rate": 0.0004998230764780276,
      "loss": 8.5241,
      "step": 9
    },
    {
      "epoch": 0.05772005772005772,
      "grad_norm": 10.375,
      "learning_rate": 0.0004996019807715324,
      "loss": 8.3689,
      "step": 10
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 8.125,
      "learning_rate": 0.0004992925563275714,
      "loss": 7.9521,
      "step": 11
    },
    {
      "epoch": 0.06926406926406926,
      "grad_norm": 14.375,
      "learning_rate": 0.0004988949126447567,
      "loss": 8.529,
      "step": 12
    },
    {
      "epoch": 0.07503607503607504,
      "grad_norm": 11.3125,
      "learning_rate": 0.0004984091904405792,
      "loss": 7.954,
      "step": 13
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 7.0625,
      "learning_rate": 0.000497835561601612,
      "loss": 7.7266,
      "step": 14
    },
    {
      "epoch": 0.08658008658008658,
      "grad_norm": 10.4375,
      "learning_rate": 0.0004971742291226826,
      "loss": 8.1219,
      "step": 15
    },
    {
      "epoch": 0.09235209235209235,
      "grad_norm": 23.125,
      "learning_rate": 0.0004964254270350387,
      "loss": 8.0693,
      "step": 16
    },
    {
      "epoch": 0.09812409812409813,
      "grad_norm": 8.375,
      "learning_rate": 0.0004955894203235284,
      "loss": 7.7709,
      "step": 17
    },
    {
      "epoch": 0.1038961038961039,
      "grad_norm": 5.375,
      "learning_rate": 0.0004946665048328287,
      "loss": 7.7282,
      "step": 18
    },
    {
      "epoch": 0.10966810966810966,
      "grad_norm": 5.21875,
      "learning_rate": 0.0004936570071627517,
      "loss": 7.6397,
      "step": 19
    },
    {
      "epoch": 0.11544011544011544,
      "grad_norm": 8.6875,
      "learning_rate": 0.0004925612845526691,
      "loss": 7.5681,
      "step": 20
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 5.5,
      "learning_rate": 0.0004913797247550911,
      "loss": 7.5938,
      "step": 21
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 4.34375,
      "learning_rate": 0.0004901127458984516,
      "loss": 7.5745,
      "step": 22
    },
    {
      "epoch": 0.13275613275613277,
      "grad_norm": 4.5625,
      "learning_rate": 0.0004887607963391394,
      "loss": 7.5515,
      "step": 23
    },
    {
      "epoch": 0.13852813852813853,
      "grad_norm": 3.515625,
      "learning_rate": 0.00048732435450283564,
      "loss": 7.471,
      "step": 24
    },
    {
      "epoch": 0.1443001443001443,
      "grad_norm": 5.59375,
      "learning_rate": 0.00048580392871520943,
      "loss": 7.5775,
      "step": 25
    },
    {
      "epoch": 0.15007215007215008,
      "grad_norm": 3.015625,
      "learning_rate": 0.00048420005702203196,
      "loss": 7.4914,
      "step": 26
    },
    {
      "epoch": 0.15584415584415584,
      "grad_norm": 3.140625,
      "learning_rate": 0.00048251330699877374,
      "loss": 7.5697,
      "step": 27
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 3.046875,
      "learning_rate": 0.00048074427554975236,
      "loss": 7.5102,
      "step": 28
    },
    {
      "epoch": 0.1673881673881674,
      "grad_norm": 3.0,
      "learning_rate": 0.00047889358869690056,
      "loss": 7.5294,
      "step": 29
    },
    {
      "epoch": 0.17316017316017315,
      "grad_norm": 3.15625,
      "learning_rate": 0.0004769619013582309,
      "loss": 7.5263,
      "step": 30
    },
    {
      "epoch": 0.17893217893217894,
      "grad_norm": 2.53125,
      "learning_rate": 0.00047494989711607415,
      "loss": 7.5215,
      "step": 31
    },
    {
      "epoch": 0.1847041847041847,
      "grad_norm": 2.609375,
      "learning_rate": 0.0004728582879751746,
      "loss": 7.524,
      "step": 32
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.078125,
      "learning_rate": 0.00047068781411072687,
      "loss": 7.4817,
      "step": 33
    },
    {
      "epoch": 0.19624819624819625,
      "grad_norm": 2.828125,
      "learning_rate": 0.00046843924360644385,
      "loss": 7.4674,
      "step": 34
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 2.78125,
      "learning_rate": 0.00046611337218274864,
      "loss": 7.5368,
      "step": 35
    },
    {
      "epoch": 0.2077922077922078,
      "grad_norm": 2.734375,
      "learning_rate": 0.0004637110229151863,
      "loss": 7.5882,
      "step": 36
    },
    {
      "epoch": 0.21356421356421357,
      "grad_norm": 2.09375,
      "learning_rate": 0.00046123304594315517,
      "loss": 7.4634,
      "step": 37
    },
    {
      "epoch": 0.21933621933621933,
      "grad_norm": 2.875,
      "learning_rate": 0.0004586803181690609,
      "loss": 7.5288,
      "step": 38
    },
    {
      "epoch": 0.22510822510822512,
      "grad_norm": 2.640625,
      "learning_rate": 0.0004560537429479998,
      "loss": 7.5046,
      "step": 39
    },
    {
      "epoch": 0.23088023088023088,
      "grad_norm": 2.78125,
      "learning_rate": 0.00045335424976808116,
      "loss": 7.5484,
      "step": 40
    },
    {
      "epoch": 0.23665223665223664,
      "grad_norm": 2.03125,
      "learning_rate": 0.0004505827939215009,
      "loss": 7.5354,
      "step": 41
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 2.75,
      "learning_rate": 0.00044774035616648516,
      "loss": 7.4529,
      "step": 42
    },
    {
      "epoch": 0.2481962481962482,
      "grad_norm": 2.515625,
      "learning_rate": 0.0004448279423802207,
      "loss": 7.5861,
      "step": 43
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 2.1875,
      "learning_rate": 0.0004418465832028967,
      "loss": 7.5338,
      "step": 44
    },
    {
      "epoch": 0.2597402597402597,
      "grad_norm": 2.109375,
      "learning_rate": 0.00043879733367298404,
      "loss": 7.4979,
      "step": 45
    },
    {
      "epoch": 0.26551226551226553,
      "grad_norm": 2.109375,
      "learning_rate": 0.00043568127285387924,
      "loss": 7.5978,
      "step": 46
    },
    {
      "epoch": 0.2712842712842713,
      "grad_norm": 2.234375,
      "learning_rate": 0.00043249950345204804,
      "loss": 7.5724,
      "step": 47
    },
    {
      "epoch": 0.27705627705627706,
      "grad_norm": 2.234375,
      "learning_rate": 0.0004292531514268008,
      "loss": 7.5828,
      "step": 48
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 1.828125,
      "learning_rate": 0.00042594336559184035,
      "loss": 7.4608,
      "step": 49
    },
    {
      "epoch": 0.2886002886002886,
      "grad_norm": 2.296875,
      "learning_rate": 0.0004225713172087216,
      "loss": 7.4937,
      "step": 50
    },
    {
      "epoch": 0.2943722943722944,
      "grad_norm": 2.40625,
      "learning_rate": 0.0004191381995723672,
      "loss": 7.4861,
      "step": 51
    },
    {
      "epoch": 0.30014430014430016,
      "grad_norm": 2.40625,
      "learning_rate": 0.00041564522758878654,
      "loss": 7.4319,
      "step": 52
    },
    {
      "epoch": 0.3059163059163059,
      "grad_norm": 2.078125,
      "learning_rate": 0.0004120936373451467,
      "loss": 7.5124,
      "step": 53
    },
    {
      "epoch": 0.3116883116883117,
      "grad_norm": 2.046875,
      "learning_rate": 0.000408484685672348,
      "loss": 7.5071,
      "step": 54
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 2.28125,
      "learning_rate": 0.0004048196497002588,
      "loss": 7.5044,
      "step": 55
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 1.859375,
      "learning_rate": 0.0004010998264057667,
      "loss": 7.4585,
      "step": 56
    },
    {
      "epoch": 0.329004329004329,
      "grad_norm": 1.984375,
      "learning_rate": 0.0003973265321538069,
      "loss": 7.4923,
      "step": 57
    },
    {
      "epoch": 0.3347763347763348,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0003935011022315284,
      "loss": 7.4486,
      "step": 58
    },
    {
      "epoch": 0.34054834054834054,
      "grad_norm": 1.875,
      "learning_rate": 0.00038962489037576583,
      "loss": 7.4381,
      "step": 59
    },
    {
      "epoch": 0.3463203463203463,
      "grad_norm": 2.125,
      "learning_rate": 0.0003856992682939803,
      "loss": 7.5355,
      "step": 60
    },
    {
      "epoch": 0.35209235209235207,
      "grad_norm": 1.75,
      "learning_rate": 0.0003817256251788425,
      "loss": 7.4856,
      "step": 61
    },
    {
      "epoch": 0.3578643578643579,
      "grad_norm": 1.9609375,
      "learning_rate": 0.00037770536721662694,
      "loss": 7.4742,
      "step": 62
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 1.9296875,
      "learning_rate": 0.0003736399170895938,
      "loss": 7.4431,
      "step": 63
    },
    {
      "epoch": 0.3694083694083694,
      "grad_norm": 2.21875,
      "learning_rate": 0.0003695307134725316,
      "loss": 7.5604,
      "step": 64
    },
    {
      "epoch": 0.37518037518037517,
      "grad_norm": 1.9453125,
      "learning_rate": 0.0003653792105236422,
      "loss": 7.4036,
      "step": 65
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 1.8828125,
      "learning_rate": 0.00036118687736994487,
      "loss": 7.4354,
      "step": 66
    },
    {
      "epoch": 0.38672438672438675,
      "grad_norm": 1.8515625,
      "learning_rate": 0.0003569551975873847,
      "loss": 7.495,
      "step": 67
    },
    {
      "epoch": 0.3924963924963925,
      "grad_norm": 1.921875,
      "learning_rate": 0.00035268566867582683,
      "loss": 7.4922,
      "step": 68
    },
    {
      "epoch": 0.39826839826839827,
      "grad_norm": 1.7109375,
      "learning_rate": 0.0003483798015291239,
      "loss": 7.3793,
      "step": 69
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 1.71875,
      "learning_rate": 0.00034403911990044307,
      "loss": 7.3737,
      "step": 70
    },
    {
      "epoch": 0.4098124098124098,
      "grad_norm": 2.15625,
      "learning_rate": 0.00033966515986304317,
      "loss": 7.4197,
      "step": 71
    },
    {
      "epoch": 0.4155844155844156,
      "grad_norm": 2.15625,
      "learning_rate": 0.0003352594692666915,
      "loss": 7.3202,
      "step": 72
    },
    {
      "epoch": 0.4213564213564214,
      "grad_norm": 1.671875,
      "learning_rate": 0.000330823607189913,
      "loss": 7.4655,
      "step": 73
    },
    {
      "epoch": 0.42712842712842713,
      "grad_norm": 1.7421875,
      "learning_rate": 0.0003263591433882666,
      "loss": 7.4769,
      "step": 74
    },
    {
      "epoch": 0.4329004329004329,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00032186765773884244,
      "loss": 7.5285,
      "step": 75
    },
    {
      "epoch": 0.43867243867243866,
      "grad_norm": 1.921875,
      "learning_rate": 0.0003173507396811774,
      "loss": 7.4527,
      "step": 76
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.734375,
      "learning_rate": 0.00031280998765478727,
      "loss": 7.3743,
      "step": 77
    },
    {
      "epoch": 0.45021645021645024,
      "grad_norm": 2.109375,
      "learning_rate": 0.0003082470085335133,
      "loss": 7.4309,
      "step": 78
    },
    {
      "epoch": 0.455988455988456,
      "grad_norm": 1.828125,
      "learning_rate": 0.00030366341705688466,
      "loss": 7.3623,
      "step": 79
    },
    {
      "epoch": 0.46176046176046176,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002990608352586965,
      "loss": 7.4157,
      "step": 80
    },
    {
      "epoch": 0.4675324675324675,
      "grad_norm": 1.7578125,
      "learning_rate": 0.00029444089189300783,
      "loss": 7.4327,
      "step": 81
    },
    {
      "epoch": 0.4733044733044733,
      "grad_norm": 1.6875,
      "learning_rate": 0.00028980522185776065,
      "loss": 7.4362,
      "step": 82
    },
    {
      "epoch": 0.4790764790764791,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00028515546561622466,
      "loss": 7.4027,
      "step": 83
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 1.9609375,
      "learning_rate": 0.000280493268616473,
      "loss": 7.4137,
      "step": 84
    },
    {
      "epoch": 0.4906204906204906,
      "grad_norm": 1.8046875,
      "learning_rate": 0.0002758202807090941,
      "loss": 7.4065,
      "step": 85
    },
    {
      "epoch": 0.4963924963924964,
      "grad_norm": 1.59375,
      "learning_rate": 0.00027113815556334474,
      "loss": 7.4405,
      "step": 86
    },
    {
      "epoch": 0.5021645021645021,
      "grad_norm": 1.734375,
      "learning_rate": 0.00026644855008195267,
      "loss": 7.3764,
      "step": 87
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002617531238147744,
      "loss": 7.4357,
      "step": 88
    },
    {
      "epoch": 0.5137085137085137,
      "grad_norm": 1.890625,
      "learning_rate": 0.0002570535383715165,
      "loss": 7.429,
      "step": 89
    },
    {
      "epoch": 0.5194805194805194,
      "grad_norm": 1.671875,
      "learning_rate": 0.0002523514568337281,
      "loss": 7.4084,
      "step": 90
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 1.796875,
      "learning_rate": 0.000247648543166272,
      "loss": 7.4225,
      "step": 91
    },
    {
      "epoch": 0.5310245310245311,
      "grad_norm": 1.9140625,
      "learning_rate": 0.00024294646162848353,
      "loss": 7.46,
      "step": 92
    },
    {
      "epoch": 0.5367965367965368,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00023824687618522567,
      "loss": 7.4939,
      "step": 93
    },
    {
      "epoch": 0.5425685425685426,
      "grad_norm": 1.921875,
      "learning_rate": 0.00023355144991804737,
      "loss": 7.4228,
      "step": 94
    },
    {
      "epoch": 0.5483405483405484,
      "grad_norm": 1.5625,
      "learning_rate": 0.00022886184443665522,
      "loss": 7.4992,
      "step": 95
    },
    {
      "epoch": 0.5541125541125541,
      "grad_norm": 1.6953125,
      "learning_rate": 0.0002241797192909059,
      "loss": 7.3876,
      "step": 96
    },
    {
      "epoch": 0.5598845598845599,
      "grad_norm": 2.046875,
      "learning_rate": 0.000219506731383527,
      "loss": 7.4043,
      "step": 97
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 1.7734375,
      "learning_rate": 0.0002148445343837755,
      "loss": 7.457,
      "step": 98
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.890625,
      "learning_rate": 0.00021019477814223942,
      "loss": 7.3575,
      "step": 99
    },
    {
      "epoch": 0.5772005772005772,
      "grad_norm": 2.03125,
      "learning_rate": 0.0002055591081069922,
      "loss": 7.4039,
      "step": 100
    },
    {
      "epoch": 0.5829725829725829,
      "grad_norm": 1.734375,
      "learning_rate": 0.00020093916474130352,
      "loss": 7.4031,
      "step": 101
    },
    {
      "epoch": 0.5887445887445888,
      "grad_norm": 1.7421875,
      "learning_rate": 0.00019633658294311535,
      "loss": 7.3922,
      "step": 102
    },
    {
      "epoch": 0.5945165945165946,
      "grad_norm": 1.953125,
      "learning_rate": 0.0001917529914664867,
      "loss": 7.3967,
      "step": 103
    },
    {
      "epoch": 0.6002886002886003,
      "grad_norm": 1.6875,
      "learning_rate": 0.00018719001234521283,
      "loss": 7.402,
      "step": 104
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.65625,
      "learning_rate": 0.00018264926031882274,
      "loss": 7.3653,
      "step": 105
    },
    {
      "epoch": 0.6118326118326118,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00017813234226115766,
      "loss": 7.4276,
      "step": 106
    },
    {
      "epoch": 0.6176046176046176,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00017364085661173345,
      "loss": 7.4163,
      "step": 107
    },
    {
      "epoch": 0.6233766233766234,
      "grad_norm": 1.4921875,
      "learning_rate": 0.000169176392810087,
      "loss": 7.3949,
      "step": 108
    },
    {
      "epoch": 0.6291486291486291,
      "grad_norm": 1.453125,
      "learning_rate": 0.0001647405307333085,
      "loss": 7.3474,
      "step": 109
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00016033484013695687,
      "loss": 7.3913,
      "step": 110
    },
    {
      "epoch": 0.6406926406926406,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00015596088009955694,
      "loss": 7.3751,
      "step": 111
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 1.53125,
      "learning_rate": 0.00015162019847087617,
      "loss": 7.4067,
      "step": 112
    },
    {
      "epoch": 0.6522366522366523,
      "grad_norm": 1.421875,
      "learning_rate": 0.00014731433132417315,
      "loss": 7.3936,
      "step": 113
    },
    {
      "epoch": 0.658008658008658,
      "grad_norm": 1.4375,
      "learning_rate": 0.00014304480241261527,
      "loss": 7.3803,
      "step": 114
    },
    {
      "epoch": 0.6637806637806638,
      "grad_norm": 1.390625,
      "learning_rate": 0.0001388131226300552,
      "loss": 7.3383,
      "step": 115
    },
    {
      "epoch": 0.6695526695526696,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0001346207894763578,
      "loss": 7.3762,
      "step": 116
    },
    {
      "epoch": 0.6753246753246753,
      "grad_norm": 1.640625,
      "learning_rate": 0.0001304692865274683,
      "loss": 7.3567,
      "step": 117
    },
    {
      "epoch": 0.6810966810966811,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0001263600829104062,
      "loss": 7.4187,
      "step": 118
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00012229463278337307,
      "loss": 7.409,
      "step": 119
    },
    {
      "epoch": 0.6926406926406926,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00011827437482115758,
      "loss": 7.3818,
      "step": 120
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 1.640625,
      "learning_rate": 0.00011430073170601968,
      "loss": 7.4282,
      "step": 121
    },
    {
      "epoch": 0.7041847041847041,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00011037510962423425,
      "loss": 7.4073,
      "step": 122
    },
    {
      "epoch": 0.70995670995671,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0001064988977684716,
      "loss": 7.3963,
      "step": 123
    },
    {
      "epoch": 0.7157287157287158,
      "grad_norm": 1.625,
      "learning_rate": 0.00010267346784619325,
      "loss": 7.337,
      "step": 124
    },
    {
      "epoch": 0.7215007215007215,
      "grad_norm": 1.609375,
      "learning_rate": 9.890017359423326e-05,
      "loss": 7.3859,
      "step": 125
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.3671875,
      "learning_rate": 9.518035029974126e-05,
      "loss": 7.3205,
      "step": 126
    },
    {
      "epoch": 0.733044733044733,
      "grad_norm": 1.375,
      "learning_rate": 9.151531432765204e-05,
      "loss": 7.3241,
      "step": 127
    },
    {
      "epoch": 0.7388167388167388,
      "grad_norm": 1.5,
      "learning_rate": 8.790636265485333e-05,
      "loss": 7.4123,
      "step": 128
    },
    {
      "epoch": 0.7445887445887446,
      "grad_norm": 1.2578125,
      "learning_rate": 8.435477241121353e-05,
      "loss": 7.3948,
      "step": 129
    },
    {
      "epoch": 0.7503607503607503,
      "grad_norm": 1.390625,
      "learning_rate": 8.086180042763284e-05,
      "loss": 7.4035,
      "step": 130
    },
    {
      "epoch": 0.7561327561327561,
      "grad_norm": 1.53125,
      "learning_rate": 7.742868279127849e-05,
      "loss": 7.3424,
      "step": 131
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 1.3671875,
      "learning_rate": 7.405663440815968e-05,
      "loss": 7.416,
      "step": 132
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 1.4296875,
      "learning_rate": 7.074684857319927e-05,
      "loss": 7.3278,
      "step": 133
    },
    {
      "epoch": 0.7734487734487735,
      "grad_norm": 1.3203125,
      "learning_rate": 6.750049654795198e-05,
      "loss": 7.3875,
      "step": 134
    },
    {
      "epoch": 0.7792207792207793,
      "grad_norm": 1.3203125,
      "learning_rate": 6.431872714612072e-05,
      "loss": 7.3234,
      "step": 135
    },
    {
      "epoch": 0.784992784992785,
      "grad_norm": 1.296875,
      "learning_rate": 6.120266632701598e-05,
      "loss": 7.3634,
      "step": 136
    },
    {
      "epoch": 0.7907647907647908,
      "grad_norm": 1.3359375,
      "learning_rate": 5.815341679710326e-05,
      "loss": 7.3782,
      "step": 137
    },
    {
      "epoch": 0.7965367965367965,
      "grad_norm": 1.359375,
      "learning_rate": 5.517205761977939e-05,
      "loss": 7.3929,
      "step": 138
    },
    {
      "epoch": 0.8023088023088023,
      "grad_norm": 1.3203125,
      "learning_rate": 5.225964383351489e-05,
      "loss": 7.3549,
      "step": 139
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.296875,
      "learning_rate": 4.941720607849912e-05,
      "loss": 7.3895,
      "step": 140
    },
    {
      "epoch": 0.8138528138528138,
      "grad_norm": 1.328125,
      "learning_rate": 4.664575023191886e-05,
      "loss": 7.3694,
      "step": 141
    },
    {
      "epoch": 0.8196248196248196,
      "grad_norm": 1.2890625,
      "learning_rate": 4.394625705200012e-05,
      "loss": 7.3656,
      "step": 142
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 1.265625,
      "learning_rate": 4.131968183093912e-05,
      "loss": 7.3274,
      "step": 143
    },
    {
      "epoch": 0.8311688311688312,
      "grad_norm": 1.3671875,
      "learning_rate": 3.876695405684485e-05,
      "loss": 7.3736,
      "step": 144
    },
    {
      "epoch": 0.836940836940837,
      "grad_norm": 1.3125,
      "learning_rate": 3.628897708481377e-05,
      "loss": 7.352,
      "step": 145
    },
    {
      "epoch": 0.8427128427128427,
      "grad_norm": 1.2890625,
      "learning_rate": 3.388662781725141e-05,
      "loss": 7.3178,
      "step": 146
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 1.359375,
      "learning_rate": 3.1560756393556184e-05,
      "loss": 7.2835,
      "step": 147
    },
    {
      "epoch": 0.8542568542568543,
      "grad_norm": 1.3515625,
      "learning_rate": 2.9312185889273145e-05,
      "loss": 7.352,
      "step": 148
    },
    {
      "epoch": 0.86002886002886,
      "grad_norm": 1.2421875,
      "learning_rate": 2.7141712024825378e-05,
      "loss": 7.3817,
      "step": 149
    },
    {
      "epoch": 0.8658008658008658,
      "grad_norm": 1.296875,
      "learning_rate": 2.505010288392587e-05,
      "loss": 7.3627,
      "step": 150
    },
    {
      "epoch": 0.8715728715728716,
      "grad_norm": 1.265625,
      "learning_rate": 2.3038098641769088e-05,
      "loss": 7.3361,
      "step": 151
    },
    {
      "epoch": 0.8773448773448773,
      "grad_norm": 1.2109375,
      "learning_rate": 2.1106411303099453e-05,
      "loss": 7.3344,
      "step": 152
    },
    {
      "epoch": 0.8831168831168831,
      "grad_norm": 1.3203125,
      "learning_rate": 1.9255724450247676e-05,
      "loss": 7.3394,
      "step": 153
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.2578125,
      "learning_rate": 1.7486693001226267e-05,
      "loss": 7.4067,
      "step": 154
    },
    {
      "epoch": 0.8946608946608947,
      "grad_norm": 1.2890625,
      "learning_rate": 1.579994297796808e-05,
      "loss": 7.4078,
      "step": 155
    },
    {
      "epoch": 0.9004329004329005,
      "grad_norm": 1.2109375,
      "learning_rate": 1.4196071284790529e-05,
      "loss": 7.3871,
      "step": 156
    },
    {
      "epoch": 0.9062049062049062,
      "grad_norm": 1.3359375,
      "learning_rate": 1.2675645497164351e-05,
      "loss": 7.389,
      "step": 157
    },
    {
      "epoch": 0.911976911976912,
      "grad_norm": 1.2265625,
      "learning_rate": 1.1239203660860647e-05,
      "loss": 7.3978,
      "step": 158
    },
    {
      "epoch": 0.9177489177489178,
      "grad_norm": 1.3671875,
      "learning_rate": 9.88725410154842e-06,
      "loss": 7.3805,
      "step": 159
    },
    {
      "epoch": 0.9235209235209235,
      "grad_norm": 1.359375,
      "learning_rate": 8.620275244908826e-06,
      "loss": 7.3118,
      "step": 160
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 1.3046875,
      "learning_rate": 7.438715447331018e-06,
      "loss": 7.4196,
      "step": 161
    },
    {
      "epoch": 0.935064935064935,
      "grad_norm": 1.265625,
      "learning_rate": 6.342992837248235e-06,
      "loss": 7.3744,
      "step": 162
    },
    {
      "epoch": 0.9408369408369408,
      "grad_norm": 1.25,
      "learning_rate": 5.333495167171354e-06,
      "loss": 7.3615,
      "step": 163
    },
    {
      "epoch": 0.9466089466089466,
      "grad_norm": 1.234375,
      "learning_rate": 4.410579676471571e-06,
      "loss": 7.4075,
      "step": 164
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 1.2265625,
      "learning_rate": 3.5745729649613034e-06,
      "loss": 7.4288,
      "step": 165
    },
    {
      "epoch": 0.9581529581529582,
      "grad_norm": 1.1796875,
      "learning_rate": 2.8257708773173627e-06,
      "loss": 7.4002,
      "step": 166
    },
    {
      "epoch": 0.963924963924964,
      "grad_norm": 1.3203125,
      "learning_rate": 2.1644383983880357e-06,
      "loss": 7.3354,
      "step": 167
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.2109375,
      "learning_rate": 1.5908095594207582e-06,
      "loss": 7.3282,
      "step": 168
    },
    {
      "epoch": 0.9754689754689755,
      "grad_norm": 1.203125,
      "learning_rate": 1.1050873552433394e-06,
      "loss": 7.3628,
      "step": 169
    },
    {
      "epoch": 0.9812409812409812,
      "grad_norm": 1.234375,
      "learning_rate": 7.074436724286704e-07,
      "loss": 7.32,
      "step": 170
    },
    {
      "epoch": 0.987012987012987,
      "grad_norm": 1.328125,
      "learning_rate": 3.9801922846766094e-07,
      "loss": 7.3693,
      "step": 171
    },
    {
      "epoch": 0.9927849927849928,
      "grad_norm": 1.1015625,
      "learning_rate": 1.7692352197240524e-07,
      "loss": 7.413,
      "step": 172
    },
    {
      "epoch": 0.9985569985569985,
      "grad_norm": 1.2890625,
      "learning_rate": 4.423479392709484e-08,
      "loss": 7.3762,
      "step": 173
    },
    {
      "epoch": 1.0057720057720059,
      "grad_norm": 9.5625,
      "learning_rate": 0.0,
      "loss": 8.0537,
      "step": 174
    },
    {
      "epoch": 1.0115440115440115,
      "grad_norm": 9.3125,
      "learning_rate": 0.0002605469934405078,
      "loss": 8.0795,
      "step": 175
    },
    {
      "epoch": 1.0173160173160174,
      "grad_norm": 3.3125,
      "learning_rate": 0.0002582041791754375,
      "loss": 7.7628,
      "step": 176
    },
    {
      "epoch": 1.023088023088023,
      "grad_norm": 9.125,
      "learning_rate": 0.00025586064340081516,
      "loss": 8.0,
      "step": 177
    },
    {
      "epoch": 1.028860028860029,
      "grad_norm": 9.1875,
      "learning_rate": 0.00025351659221689896,
      "loss": 7.9595,
      "step": 178
    },
    {
      "epoch": 1.0346320346320346,
      "grad_norm": 4.65625,
      "learning_rate": 0.0002511722317692747,
      "loss": 7.7844,
      "step": 179
    },
    {
      "epoch": 1.0404040404040404,
      "grad_norm": 8.6875,
      "learning_rate": 0.0002488277682307254,
      "loss": 7.7455,
      "step": 180
    },
    {
      "epoch": 1.046176046176046,
      "grad_norm": 7.25,
      "learning_rate": 0.00024648340778310105,
      "loss": 7.7461,
      "step": 181
    },
    {
      "epoch": 1.051948051948052,
      "grad_norm": 4.21875,
      "learning_rate": 0.0002441393565991849,
      "loss": 7.6593,
      "step": 182
    },
    {
      "epoch": 1.0577200577200576,
      "grad_norm": 3.09375,
      "learning_rate": 0.00024179582082456253,
      "loss": 7.5647,
      "step": 183
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 3.890625,
      "learning_rate": 0.00023945300655949225,
      "loss": 7.5575,
      "step": 184
    },
    {
      "epoch": 1.0692640692640694,
      "grad_norm": 3.625,
      "learning_rate": 0.00023711111984077966,
      "loss": 7.4884,
      "step": 185
    },
    {
      "epoch": 1.075036075036075,
      "grad_norm": 2.96875,
      "learning_rate": 0.00023477036662365828,
      "loss": 7.4191,
      "step": 186
    },
    {
      "epoch": 1.0808080808080809,
      "grad_norm": 4.40625,
      "learning_rate": 0.00023243095276367684,
      "loss": 7.4441,
      "step": 187
    },
    {
      "epoch": 1.0865800865800865,
      "grad_norm": 4.1875,
      "learning_rate": 0.00023009308399859506,
      "loss": 7.4609,
      "step": 188
    },
    {
      "epoch": 1.0923520923520924,
      "grad_norm": 2.484375,
      "learning_rate": 0.00022775696593029104,
      "loss": 7.4875,
      "step": 189
    },
    {
      "epoch": 1.098124098124098,
      "grad_norm": 2.609375,
      "learning_rate": 0.00022542280400667918,
      "loss": 7.4308,
      "step": 190
    },
    {
      "epoch": 1.103896103896104,
      "grad_norm": 2.53125,
      "learning_rate": 0.00022309080350364253,
      "loss": 7.3175,
      "step": 191
    },
    {
      "epoch": 1.1096681096681096,
      "grad_norm": 2.640625,
      "learning_rate": 0.0002207611695069794,
      "loss": 7.3679,
      "step": 192
    },
    {
      "epoch": 1.1154401154401155,
      "grad_norm": 2.375,
      "learning_rate": 0.00021843410689436824,
      "loss": 7.3754,
      "step": 193
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 2.015625,
      "learning_rate": 0.0002161098203173492,
      "loss": 7.4065,
      "step": 194
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 1.84375,
      "learning_rate": 0.000213788514183326,
      "loss": 7.3632,
      "step": 195
    },
    {
      "epoch": 1.1327561327561328,
      "grad_norm": 2.53125,
      "learning_rate": 0.00021147039263759028,
      "loss": 7.3099,
      "step": 196
    },
    {
      "epoch": 1.1385281385281385,
      "grad_norm": 2.21875,
      "learning_rate": 0.00020915565954536742,
      "loss": 7.3534,
      "step": 197
    },
    {
      "epoch": 1.1443001443001444,
      "grad_norm": 2.265625,
      "learning_rate": 0.0002068445184738886,
      "loss": 7.3543,
      "step": 198
    },
    {
      "epoch": 1.15007215007215,
      "grad_norm": 2.078125,
      "learning_rate": 0.00020453717267448717,
      "loss": 7.3115,
      "step": 199
    },
    {
      "epoch": 1.155844155844156,
      "grad_norm": 1.8203125,
      "learning_rate": 0.00020223382506472505,
      "loss": 7.3154,
      "step": 200
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 1.7109375,
      "learning_rate": 0.00019993467821054645,
      "loss": 7.3584,
      "step": 201
    },
    {
      "epoch": 1.1673881673881674,
      "grad_norm": 2.09375,
      "learning_rate": 0.00019763993430846395,
      "loss": 7.3113,
      "step": 202
    },
    {
      "epoch": 1.173160173160173,
      "grad_norm": 1.609375,
      "learning_rate": 0.000195349795167776,
      "loss": 7.406,
      "step": 203
    },
    {
      "epoch": 1.178932178932179,
      "grad_norm": 1.6875,
      "learning_rate": 0.0001930644621928194,
      "loss": 7.3339,
      "step": 204
    },
    {
      "epoch": 1.1847041847041848,
      "grad_norm": 1.8046875,
      "learning_rate": 0.0001907841363652568,
      "loss": 7.3526,
      "step": 205
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00018850901822640146,
      "loss": 7.3234,
      "step": 206
    },
    {
      "epoch": 1.1962481962481963,
      "grad_norm": 1.65625,
      "learning_rate": 0.0001862393078595809,
      "loss": 7.4072,
      "step": 207
    },
    {
      "epoch": 1.202020202020202,
      "grad_norm": 1.9296875,
      "learning_rate": 0.0001839752048725408,
      "loss": 7.4083,
      "step": 208
    },
    {
      "epoch": 1.2077922077922079,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00018171690837989057,
      "loss": 7.3961,
      "step": 209
    },
    {
      "epoch": 1.2135642135642135,
      "grad_norm": 1.546875,
      "learning_rate": 0.00017946461698559237,
      "loss": 7.4036,
      "step": 210
    },
    {
      "epoch": 1.2193362193362194,
      "grad_norm": 1.6796875,
      "learning_rate": 0.00017721852876549508,
      "loss": 7.3232,
      "step": 211
    },
    {
      "epoch": 1.225108225108225,
      "grad_norm": 1.890625,
      "learning_rate": 0.00017497884124991487,
      "loss": 7.3948,
      "step": 212
    },
    {
      "epoch": 1.230880230880231,
      "grad_norm": 1.6640625,
      "learning_rate": 0.00017274575140626317,
      "loss": 7.4026,
      "step": 213
    },
    {
      "epoch": 1.2366522366522366,
      "grad_norm": 1.5625,
      "learning_rate": 0.00017051945562172494,
      "loss": 7.3974,
      "step": 214
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 1.546875,
      "learning_rate": 0.00016830014968598734,
      "loss": 7.3143,
      "step": 215
    },
    {
      "epoch": 1.248196248196248,
      "grad_norm": 1.734375,
      "learning_rate": 0.00016608802877402136,
      "loss": 7.3914,
      "step": 216
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 1.59375,
      "learning_rate": 0.00016388328742891677,
      "loss": 7.3759,
      "step": 217
    },
    {
      "epoch": 1.2597402597402598,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00016168611954477417,
      "loss": 7.3363,
      "step": 218
    },
    {
      "epoch": 1.2655122655122655,
      "grad_norm": 1.578125,
      "learning_rate": 0.00015949671834965222,
      "loss": 7.3725,
      "step": 219
    },
    {
      "epoch": 1.2712842712842713,
      "grad_norm": 1.78125,
      "learning_rate": 0.00015731527638857492,
      "loss": 7.3916,
      "step": 220
    },
    {
      "epoch": 1.277056277056277,
      "grad_norm": 1.84375,
      "learning_rate": 0.00015514198550659793,
      "loss": 7.3137,
      "step": 221
    },
    {
      "epoch": 1.2828282828282829,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00015297703683193753,
      "loss": 7.3567,
      "step": 222
    },
    {
      "epoch": 1.2886002886002885,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00015082062075916165,
      "loss": 7.3742,
      "step": 223
    },
    {
      "epoch": 1.2943722943722944,
      "grad_norm": 1.5,
      "learning_rate": 0.00014867292693244546,
      "loss": 7.3856,
      "step": 224
    },
    {
      "epoch": 1.3001443001443,
      "grad_norm": 1.6640625,
      "learning_rate": 0.000146534144228894,
      "loss": 7.3726,
      "step": 225
    },
    {
      "epoch": 1.305916305916306,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00014440446074193099,
      "loss": 7.3972,
      "step": 226
    },
    {
      "epoch": 1.3116883116883118,
      "grad_norm": 1.75,
      "learning_rate": 0.00014228406376475743,
      "loss": 7.333,
      "step": 227
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 1.71875,
      "learning_rate": 0.00014017313977387997,
      "loss": 7.2984,
      "step": 228
    },
    {
      "epoch": 1.3232323232323233,
      "grad_norm": 1.671875,
      "learning_rate": 0.00013807187441271156,
      "loss": 7.4107,
      "step": 229
    },
    {
      "epoch": 1.329004329004329,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00013598045247524554,
      "loss": 7.3404,
      "step": 230
    },
    {
      "epoch": 1.3347763347763348,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00013389905788980294,
      "loss": 7.3451,
      "step": 231
    },
    {
      "epoch": 1.3405483405483405,
      "grad_norm": 1.640625,
      "learning_rate": 0.00013182787370285865,
      "loss": 7.3895,
      "step": 232
    },
    {
      "epoch": 1.3463203463203464,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00012976708206294252,
      "loss": 7.3647,
      "step": 233
    },
    {
      "epoch": 1.352092352092352,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00012771686420462054,
      "loss": 7.3824,
      "step": 234
    },
    {
      "epoch": 1.3578643578643579,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0001256774004325565,
      "loss": 7.432,
      "step": 235
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 1.5625,
      "learning_rate": 0.00012364887010565535,
      "loss": 7.3783,
      "step": 236
    },
    {
      "epoch": 1.3694083694083694,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00012163145162128947,
      "loss": 7.3569,
      "step": 237
    },
    {
      "epoch": 1.375180375180375,
      "grad_norm": 1.6875,
      "learning_rate": 0.0001196253223996099,
      "loss": 7.3285,
      "step": 238
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 1.75,
      "learning_rate": 0.00011763065886794258,
      "loss": 7.3751,
      "step": 239
    },
    {
      "epoch": 1.3867243867243868,
      "grad_norm": 1.859375,
      "learning_rate": 0.00011564763644527357,
      "loss": 7.4235,
      "step": 240
    },
    {
      "epoch": 1.3924963924963925,
      "grad_norm": 1.578125,
      "learning_rate": 0.00011367642952682153,
      "loss": 7.4149,
      "step": 241
    },
    {
      "epoch": 1.3982683982683983,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00011171721146870015,
      "loss": 7.3641,
      "step": 242
    },
    {
      "epoch": 1.404040404040404,
      "grad_norm": 2.15625,
      "learning_rate": 0.00010977015457267365,
      "loss": 7.3095,
      "step": 243
    },
    {
      "epoch": 1.4098124098124099,
      "grad_norm": 1.9765625,
      "learning_rate": 0.00010783543007100266,
      "loss": 7.3397,
      "step": 244
    },
    {
      "epoch": 1.4155844155844157,
      "grad_norm": 2.375,
      "learning_rate": 0.00010591320811138636,
      "loss": 7.4186,
      "step": 245
    },
    {
      "epoch": 1.4213564213564214,
      "grad_norm": 1.9609375,
      "learning_rate": 0.00010400365774199818,
      "loss": 7.3828,
      "step": 246
    },
    {
      "epoch": 1.427128427128427,
      "grad_norm": 2.015625,
      "learning_rate": 0.0001021069468966194,
      "loss": 7.4065,
      "step": 247
    },
    {
      "epoch": 1.432900432900433,
      "grad_norm": 1.9765625,
      "learning_rate": 0.00010022324237987047,
      "loss": 7.3727,
      "step": 248
    },
    {
      "epoch": 1.4386724386724388,
      "grad_norm": 1.7421875,
      "learning_rate": 9.835270985254111e-05,
      "loss": 7.4207,
      "step": 249
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 1.6953125,
      "learning_rate": 9.649551381702168e-05,
      "loss": 7.3651,
      "step": 250
    },
    {
      "epoch": 1.4502164502164503,
      "grad_norm": 1.53125,
      "learning_rate": 9.46518176028364e-05,
      "loss": 7.3787,
      "step": 251
    },
    {
      "epoch": 1.455988455988456,
      "grad_norm": 1.71875,
      "learning_rate": 9.282178335227883e-05,
      "loss": 7.3673,
      "step": 252
    },
    {
      "epoch": 1.4617604617604618,
      "grad_norm": 1.6796875,
      "learning_rate": 9.100557200615292e-05,
      "loss": 7.3334,
      "step": 253
    },
    {
      "epoch": 1.4675324675324675,
      "grad_norm": 1.828125,
      "learning_rate": 8.920334328961918e-05,
      "loss": 7.3773,
      "step": 254
    },
    {
      "epoch": 1.4733044733044733,
      "grad_norm": 1.765625,
      "learning_rate": 8.74152556981474e-05,
      "loss": 7.3488,
      "step": 255
    },
    {
      "epoch": 1.479076479076479,
      "grad_norm": 1.984375,
      "learning_rate": 8.56414664835785e-05,
      "loss": 7.427,
      "step": 256
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 1.6015625,
      "learning_rate": 8.388213164029459e-05,
      "loss": 7.3515,
      "step": 257
    },
    {
      "epoch": 1.4906204906204907,
      "grad_norm": 1.5625,
      "learning_rate": 8.213740589150032e-05,
      "loss": 7.4119,
      "step": 258
    },
    {
      "epoch": 1.4963924963924964,
      "grad_norm": 1.671875,
      "learning_rate": 8.040744267561637e-05,
      "loss": 7.3912,
      "step": 259
    },
    {
      "epoch": 1.502164502164502,
      "grad_norm": 1.765625,
      "learning_rate": 7.869239413278442e-05,
      "loss": 7.3997,
      "step": 260
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 1.671875,
      "learning_rate": 7.699241109148844e-05,
      "loss": 7.3349,
      "step": 261
    },
    {
      "epoch": 1.5137085137085138,
      "grad_norm": 1.6015625,
      "learning_rate": 7.530764305528959e-05,
      "loss": 7.3493,
      "step": 262
    },
    {
      "epoch": 1.5194805194805194,
      "grad_norm": 1.8671875,
      "learning_rate": 7.363823818967824e-05,
      "loss": 7.3535,
      "step": 263
    },
    {
      "epoch": 1.5252525252525253,
      "grad_norm": 1.78125,
      "learning_rate": 7.198434330904388e-05,
      "loss": 7.3843,
      "step": 264
    },
    {
      "epoch": 1.531024531024531,
      "grad_norm": 1.71875,
      "learning_rate": 7.034610386376342e-05,
      "loss": 7.353,
      "step": 265
    },
    {
      "epoch": 1.5367965367965368,
      "grad_norm": 1.6328125,
      "learning_rate": 6.872366392741017e-05,
      "loss": 7.3932,
      "step": 266
    },
    {
      "epoch": 1.5425685425685427,
      "grad_norm": 1.7421875,
      "learning_rate": 6.711716618408281e-05,
      "loss": 7.4042,
      "step": 267
    },
    {
      "epoch": 1.5483405483405484,
      "grad_norm": 1.7265625,
      "learning_rate": 6.552675191585741e-05,
      "loss": 7.3581,
      "step": 268
    },
    {
      "epoch": 1.554112554112554,
      "grad_norm": 1.984375,
      "learning_rate": 6.395256099036278e-05,
      "loss": 7.3909,
      "step": 269
    },
    {
      "epoch": 1.5598845598845599,
      "grad_norm": 1.734375,
      "learning_rate": 6.239473184847941e-05,
      "loss": 7.4146,
      "step": 270
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 1.734375,
      "learning_rate": 6.085340149216467e-05,
      "loss": 7.3862,
      "step": 271
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.7421875,
      "learning_rate": 5.9328705472404546e-05,
      "loss": 7.3928,
      "step": 272
    },
    {
      "epoch": 1.577200577200577,
      "grad_norm": 1.6875,
      "learning_rate": 5.7820777877292065e-05,
      "loss": 7.4358,
      "step": 273
    },
    {
      "epoch": 1.582972582972583,
      "grad_norm": 1.8046875,
      "learning_rate": 5.632975132023585e-05,
      "loss": 7.3356,
      "step": 274
    },
    {
      "epoch": 1.5887445887445888,
      "grad_norm": 1.890625,
      "learning_rate": 5.485575692829678e-05,
      "loss": 7.3872,
      "step": 275
    },
    {
      "epoch": 1.5945165945165947,
      "grad_norm": 1.90625,
      "learning_rate": 5.339892433065654e-05,
      "loss": 7.2702,
      "step": 276
    },
    {
      "epoch": 1.6002886002886003,
      "grad_norm": 1.8671875,
      "learning_rate": 5.195938164721767e-05,
      "loss": 7.4592,
      "step": 277
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 1.5390625,
      "learning_rate": 5.0537255477335644e-05,
      "loss": 7.3226,
      "step": 278
    },
    {
      "epoch": 1.6118326118326118,
      "grad_norm": 2.125,
      "learning_rate": 4.913267088868553e-05,
      "loss": 7.2895,
      "step": 279
    },
    {
      "epoch": 1.6176046176046177,
      "grad_norm": 1.796875,
      "learning_rate": 4.7745751406263163e-05,
      "loss": 7.3925,
      "step": 280
    },
    {
      "epoch": 1.6233766233766234,
      "grad_norm": 1.8046875,
      "learning_rate": 4.637661900152143e-05,
      "loss": 7.3546,
      "step": 281
    },
    {
      "epoch": 1.629148629148629,
      "grad_norm": 1.8046875,
      "learning_rate": 4.5025394081643854e-05,
      "loss": 7.4133,
      "step": 282
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 1.59375,
      "learning_rate": 4.3692195478955615e-05,
      "loss": 7.264,
      "step": 283
    },
    {
      "epoch": 1.6406926406926408,
      "grad_norm": 1.640625,
      "learning_rate": 4.237714044047258e-05,
      "loss": 7.3105,
      "step": 284
    },
    {
      "epoch": 1.6464646464646466,
      "grad_norm": 1.8359375,
      "learning_rate": 4.108034461759036e-05,
      "loss": 7.3165,
      "step": 285
    },
    {
      "epoch": 1.6522366522366523,
      "grad_norm": 1.9296875,
      "learning_rate": 3.980192205591354e-05,
      "loss": 7.3313,
      "step": 286
    },
    {
      "epoch": 1.658008658008658,
      "grad_norm": 1.734375,
      "learning_rate": 3.8541985185225645e-05,
      "loss": 7.3548,
      "step": 287
    },
    {
      "epoch": 1.6637806637806638,
      "grad_norm": 1.71875,
      "learning_rate": 3.7300644809602155e-05,
      "loss": 7.3359,
      "step": 288
    },
    {
      "epoch": 1.6695526695526697,
      "grad_norm": 1.640625,
      "learning_rate": 3.6078010097665206e-05,
      "loss": 7.3164,
      "step": 289
    },
    {
      "epoch": 1.6753246753246753,
      "grad_norm": 1.6328125,
      "learning_rate": 3.487418857298366e-05,
      "loss": 7.4107,
      "step": 290
    },
    {
      "epoch": 1.681096681096681,
      "grad_norm": 1.5625,
      "learning_rate": 3.368928610461652e-05,
      "loss": 7.351,
      "step": 291
    },
    {
      "epoch": 1.6868686868686869,
      "grad_norm": 1.6484375,
      "learning_rate": 3.2523406897802446e-05,
      "loss": 7.341,
      "step": 292
    },
    {
      "epoch": 1.6926406926406927,
      "grad_norm": 1.84375,
      "learning_rate": 3.1376653484795545e-05,
      "loss": 7.3897,
      "step": 293
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 1.6328125,
      "learning_rate": 3.0249126715848258e-05,
      "loss": 7.3691,
      "step": 294
    },
    {
      "epoch": 1.704184704184704,
      "grad_norm": 1.578125,
      "learning_rate": 2.9140925750342357e-05,
      "loss": 7.3357,
      "step": 295
    },
    {
      "epoch": 1.70995670995671,
      "grad_norm": 1.6328125,
      "learning_rate": 2.8052148048068076e-05,
      "loss": 7.3513,
      "step": 296
    },
    {
      "epoch": 1.7157287157287158,
      "grad_norm": 1.6328125,
      "learning_rate": 2.698288936065338e-05,
      "loss": 7.3226,
      "step": 297
    },
    {
      "epoch": 1.7215007215007216,
      "grad_norm": 1.375,
      "learning_rate": 2.593324372314318e-05,
      "loss": 7.3975,
      "step": 298
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 1.5078125,
      "learning_rate": 2.4903303445729276e-05,
      "loss": 7.3231,
      "step": 299
    },
    {
      "epoch": 1.733044733044733,
      "grad_norm": 1.4765625,
      "learning_rate": 2.3893159105632362e-05,
      "loss": 7.3581,
      "step": 300
    },
    {
      "epoch": 1.7388167388167388,
      "grad_norm": 1.5859375,
      "learning_rate": 2.2902899539136435e-05,
      "loss": 7.3666,
      "step": 301
    },
    {
      "epoch": 1.7445887445887447,
      "grad_norm": 1.625,
      "learning_rate": 2.1932611833775846e-05,
      "loss": 7.3034,
      "step": 302
    },
    {
      "epoch": 1.7503607503607503,
      "grad_norm": 1.6484375,
      "learning_rate": 2.0982381320676647e-05,
      "loss": 7.3257,
      "step": 303
    },
    {
      "epoch": 1.756132756132756,
      "grad_norm": 1.5078125,
      "learning_rate": 2.0052291567052295e-05,
      "loss": 7.3548,
      "step": 304
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 1.5,
      "learning_rate": 1.9142424368854162e-05,
      "loss": 7.3287,
      "step": 305
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 1.53125,
      "learning_rate": 1.825285974357835e-05,
      "loss": 7.3127,
      "step": 306
    },
    {
      "epoch": 1.7734487734487736,
      "grad_norm": 1.671875,
      "learning_rate": 1.738367592322837e-05,
      "loss": 7.331,
      "step": 307
    },
    {
      "epoch": 1.7792207792207793,
      "grad_norm": 1.5078125,
      "learning_rate": 1.6534949347435185e-05,
      "loss": 7.3318,
      "step": 308
    },
    {
      "epoch": 1.784992784992785,
      "grad_norm": 1.5,
      "learning_rate": 1.5706754656734908e-05,
      "loss": 7.3386,
      "step": 309
    },
    {
      "epoch": 1.7907647907647908,
      "grad_norm": 1.453125,
      "learning_rate": 1.4899164686004413e-05,
      "loss": 7.35,
      "step": 310
    },
    {
      "epoch": 1.7965367965367967,
      "grad_norm": 1.4765625,
      "learning_rate": 1.4112250458055975e-05,
      "loss": 7.403,
      "step": 311
    },
    {
      "epoch": 1.8023088023088023,
      "grad_norm": 1.6328125,
      "learning_rate": 1.3346081177391473e-05,
      "loss": 7.264,
      "step": 312
    },
    {
      "epoch": 1.808080808080808,
      "grad_norm": 1.578125,
      "learning_rate": 1.2600724224115845e-05,
      "loss": 7.2868,
      "step": 313
    },
    {
      "epoch": 1.8138528138528138,
      "grad_norm": 1.6171875,
      "learning_rate": 1.1876245148011694e-05,
      "loss": 7.3578,
      "step": 314
    },
    {
      "epoch": 1.8196248196248197,
      "grad_norm": 1.4296875,
      "learning_rate": 1.1172707662774561e-05,
      "loss": 7.3388,
      "step": 315
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 1.484375,
      "learning_rate": 1.0490173640409468e-05,
      "loss": 7.2943,
      "step": 316
    },
    {
      "epoch": 1.8311688311688312,
      "grad_norm": 1.6328125,
      "learning_rate": 9.828703105789983e-06,
      "loss": 7.3905,
      "step": 317
    },
    {
      "epoch": 1.8369408369408369,
      "grad_norm": 1.5390625,
      "learning_rate": 9.188354231378899e-06,
      "loss": 7.3607,
      "step": 318
    },
    {
      "epoch": 1.8427128427128427,
      "grad_norm": 1.40625,
      "learning_rate": 8.569183332112846e-06,
      "loss": 7.338,
      "step": 319
    },
    {
      "epoch": 1.8484848484848486,
      "grad_norm": 1.5078125,
      "learning_rate": 7.971244860449395e-06,
      "loss": 7.3615,
      "step": 320
    },
    {
      "epoch": 1.8542568542568543,
      "grad_norm": 1.515625,
      "learning_rate": 7.394591401578166e-06,
      "loss": 7.3794,
      "step": 321
    },
    {
      "epoch": 1.86002886002886,
      "grad_norm": 1.484375,
      "learning_rate": 6.839273668796747e-06,
      "loss": 7.3836,
      "step": 322
    },
    {
      "epoch": 1.8658008658008658,
      "grad_norm": 1.4453125,
      "learning_rate": 6.3053404990502384e-06,
      "loss": 7.3192,
      "step": 323
    },
    {
      "epoch": 1.8715728715728717,
      "grad_norm": 1.5546875,
      "learning_rate": 5.7928388486366555e-06,
      "loss": 7.3722,
      "step": 324
    },
    {
      "epoch": 1.8773448773448773,
      "grad_norm": 1.5234375,
      "learning_rate": 5.301813789077264e-06,
      "loss": 7.2723,
      "step": 325
    },
    {
      "epoch": 1.883116883116883,
      "grad_norm": 1.5625,
      "learning_rate": 4.832308503152832e-06,
      "loss": 7.2657,
      "step": 326
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.7109375,
      "learning_rate": 4.384364281105973e-06,
      "loss": 7.3382,
      "step": 327
    },
    {
      "epoch": 1.8946608946608947,
      "grad_norm": 1.4765625,
      "learning_rate": 3.9580205170098856e-06,
      "loss": 7.3453,
      "step": 328
    },
    {
      "epoch": 1.9004329004329006,
      "grad_norm": 1.3828125,
      "learning_rate": 3.553314705303845e-06,
      "loss": 7.3368,
      "step": 329
    },
    {
      "epoch": 1.9062049062049062,
      "grad_norm": 1.6171875,
      "learning_rate": 3.1702824374959527e-06,
      "loss": 7.3414,
      "step": 330
    },
    {
      "epoch": 1.9119769119769119,
      "grad_norm": 1.40625,
      "learning_rate": 2.8089573990328076e-06,
      "loss": 7.2911,
      "step": 331
    },
    {
      "epoch": 1.9177489177489178,
      "grad_norm": 1.515625,
      "learning_rate": 2.469371366337264e-06,
      "loss": 7.3272,
      "step": 332
    },
    {
      "epoch": 1.9235209235209236,
      "grad_norm": 1.53125,
      "learning_rate": 2.1515542040138335e-06,
      "loss": 7.3588,
      "step": 333
    },
    {
      "epoch": 1.9292929292929293,
      "grad_norm": 1.4453125,
      "learning_rate": 1.8555338622222583e-06,
      "loss": 7.355,
      "step": 334
    },
    {
      "epoch": 1.935064935064935,
      "grad_norm": 1.578125,
      "learning_rate": 1.581336374219422e-06,
      "loss": 7.2745,
      "step": 335
    },
    {
      "epoch": 1.9408369408369408,
      "grad_norm": 1.46875,
      "learning_rate": 1.3289858540699584e-06,
      "loss": 7.3525,
      "step": 336
    },
    {
      "epoch": 1.9466089466089467,
      "grad_norm": 1.5703125,
      "learning_rate": 1.0985044945254763e-06,
      "loss": 7.3226,
      "step": 337
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 1.46875,
      "learning_rate": 8.899125650729256e-07,
      "loss": 7.2717,
      "step": 338
    },
    {
      "epoch": 1.9581529581529582,
      "grad_norm": 1.3046875,
      "learning_rate": 7.032284101518849e-07,
      "loss": 7.332,
      "step": 339
    },
    {
      "epoch": 1.9639249639249639,
      "grad_norm": 1.4140625,
      "learning_rate": 5.384684475414625e-07,
      "loss": 7.3699,
      "step": 340
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 1.3828125,
      "learning_rate": 3.9564716691622984e-07,
      "loss": 7.331,
      "step": 341
    },
    {
      "epoch": 1.9754689754689756,
      "grad_norm": 1.625,
      "learning_rate": 2.7477712857215677e-07,
      "loss": 7.3776,
      "step": 342
    },
    {
      "epoch": 1.9812409812409812,
      "grad_norm": 1.5,
      "learning_rate": 1.7586896232180128e-07,
      "loss": 7.377,
      "step": 343
    },
    {
      "epoch": 1.987012987012987,
      "grad_norm": 1.671875,
      "learning_rate": 9.89313665596403e-08,
      "loss": 7.2944,
      "step": 344
    },
    {
      "epoch": 1.9927849927849928,
      "grad_norm": 1.53125,
      "learning_rate": 4.3971107497042806e-08,
      "loss": 7.3404,
      "step": 345
    },
    {
      "epoch": 1.9985569985569986,
      "grad_norm": 1.6484375,
      "learning_rate": 1.0993018567162505e-08,
      "loss": 7.3742,
      "step": 346
    },
    {
      "epoch": 2.005772005772006,
      "grad_norm": 6.8125,
      "learning_rate": 0.0,
      "loss": 8.1356,
      "step": 347
    },
    {
      "epoch": 2.0115440115440117,
      "grad_norm": 6.59375,
      "learning_rate": 0.00013090475294165788,
      "loss": 8.1108,
      "step": 348
    },
    {
      "epoch": 2.017316017316017,
      "grad_norm": 3.515625,
      "learning_rate": 0.00012953421662274264,
      "loss": 7.9734,
      "step": 349
    },
    {
      "epoch": 2.023088023088023,
      "grad_norm": 3.53125,
      "learning_rate": 0.0001281683795270203,
      "loss": 7.8885,
      "step": 350
    },
    {
      "epoch": 2.028860028860029,
      "grad_norm": 5.3125,
      "learning_rate": 0.0001268072949341293,
      "loss": 7.8462,
      "step": 351
    },
    {
      "epoch": 2.034632034632035,
      "grad_norm": 5.75,
      "learning_rate": 0.0001254510159383185,
      "loss": 7.8353,
      "step": 352
    },
    {
      "epoch": 2.04040404040404,
      "grad_norm": 5.125,
      "learning_rate": 0.0001240995954463761,
      "loss": 7.7548,
      "step": 353
    },
    {
      "epoch": 2.046176046176046,
      "grad_norm": 4.03125,
      "learning_rate": 0.00012275308617556643,
      "loss": 7.6661,
      "step": 354
    },
    {
      "epoch": 2.051948051948052,
      "grad_norm": 3.5625,
      "learning_rate": 0.00012141154065157281,
      "loss": 7.616,
      "step": 355
    },
    {
      "epoch": 2.057720057720058,
      "grad_norm": 3.4375,
      "learning_rate": 0.00012007501120644901,
      "loss": 7.6385,
      "step": 356
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 3.109375,
      "learning_rate": 0.0001187435499765773,
      "loss": 7.5805,
      "step": 357
    },
    {
      "epoch": 2.069264069264069,
      "grad_norm": 2.609375,
      "learning_rate": 0.00011741720890063559,
      "loss": 7.5596,
      "step": 358
    },
    {
      "epoch": 2.075036075036075,
      "grad_norm": 2.84375,
      "learning_rate": 0.0001160960397175705,
      "loss": 7.5282,
      "step": 359
    },
    {
      "epoch": 2.080808080808081,
      "grad_norm": 2.5625,
      "learning_rate": 0.00011478009396457953,
      "loss": 7.5843,
      "step": 360
    },
    {
      "epoch": 2.0865800865800868,
      "grad_norm": 1.9921875,
      "learning_rate": 0.00011346942297510052,
      "loss": 7.5187,
      "step": 361
    },
    {
      "epoch": 2.092352092352092,
      "grad_norm": 2.671875,
      "learning_rate": 0.00011216407787680919,
      "loss": 7.4363,
      "step": 362
    },
    {
      "epoch": 2.098124098124098,
      "grad_norm": 3.015625,
      "learning_rate": 0.00011086410958962481,
      "loss": 7.4415,
      "step": 363
    },
    {
      "epoch": 2.103896103896104,
      "grad_norm": 2.375,
      "learning_rate": 0.00010956956882372377,
      "loss": 7.4549,
      "step": 364
    },
    {
      "epoch": 2.10966810966811,
      "grad_norm": 1.8671875,
      "learning_rate": 0.00010828050607756152,
      "loss": 7.4471,
      "step": 365
    },
    {
      "epoch": 2.1154401154401152,
      "grad_norm": 2.015625,
      "learning_rate": 0.00010699697163590256,
      "loss": 7.3998,
      "step": 366
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 2.09375,
      "learning_rate": 0.00010571901556785906,
      "loss": 7.3993,
      "step": 367
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 2.34375,
      "learning_rate": 0.00010444668772493762,
      "loss": 7.4536,
      "step": 368
    },
    {
      "epoch": 2.132756132756133,
      "grad_norm": 1.7578125,
      "learning_rate": 0.00010318003773909456,
      "loss": 7.3486,
      "step": 369
    },
    {
      "epoch": 2.1385281385281387,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00010191911502080026,
      "loss": 7.3766,
      "step": 370
    },
    {
      "epoch": 2.144300144300144,
      "grad_norm": 2.125,
      "learning_rate": 0.00010066396875711086,
      "loss": 7.3743,
      "step": 371
    },
    {
      "epoch": 2.15007215007215,
      "grad_norm": 1.9765625,
      "learning_rate": 9.941464790975035e-05,
      "loss": 7.3481,
      "step": 372
    },
    {
      "epoch": 2.155844155844156,
      "grad_norm": 1.6328125,
      "learning_rate": 9.817120121320044e-05,
      "loss": 7.4137,
      "step": 373
    },
    {
      "epoch": 2.1616161616161618,
      "grad_norm": 1.859375,
      "learning_rate": 9.693367717279932e-05,
      "loss": 7.3033,
      "step": 374
    },
    {
      "epoch": 2.167388167388167,
      "grad_norm": 1.8515625,
      "learning_rate": 9.570212406284931e-05,
      "loss": 7.3418,
      "step": 375
    },
    {
      "epoch": 2.173160173160173,
      "grad_norm": 1.6640625,
      "learning_rate": 9.447658992473426e-05,
      "loss": 7.3281,
      "step": 376
    },
    {
      "epoch": 2.178932178932179,
      "grad_norm": 2.03125,
      "learning_rate": 9.325712256504543e-05,
      "loss": 7.3892,
      "step": 377
    },
    {
      "epoch": 2.184704184704185,
      "grad_norm": 1.7578125,
      "learning_rate": 9.204376955371627e-05,
      "loss": 7.4028,
      "step": 378
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 1.875,
      "learning_rate": 9.083657822216681e-05,
      "loss": 7.4174,
      "step": 379
    },
    {
      "epoch": 2.196248196248196,
      "grad_norm": 1.78125,
      "learning_rate": 8.963559566145762e-05,
      "loss": 7.3563,
      "step": 380
    },
    {
      "epoch": 2.202020202020202,
      "grad_norm": 2.09375,
      "learning_rate": 8.844086872045293e-05,
      "loss": 7.3044,
      "step": 381
    },
    {
      "epoch": 2.207792207792208,
      "grad_norm": 1.9609375,
      "learning_rate": 8.725244400399255e-05,
      "loss": 7.266,
      "step": 382
    },
    {
      "epoch": 2.2135642135642137,
      "grad_norm": 2.15625,
      "learning_rate": 8.607036787107428e-05,
      "loss": 7.3624,
      "step": 383
    },
    {
      "epoch": 2.219336219336219,
      "grad_norm": 1.84375,
      "learning_rate": 8.489468643304551e-05,
      "loss": 7.3787,
      "step": 384
    },
    {
      "epoch": 2.225108225108225,
      "grad_norm": 2.203125,
      "learning_rate": 8.37254455518043e-05,
      "loss": 7.3489,
      "step": 385
    },
    {
      "epoch": 2.230880230880231,
      "grad_norm": 1.7890625,
      "learning_rate": 8.256269083801052e-05,
      "loss": 7.3041,
      "step": 386
    },
    {
      "epoch": 2.236652236652237,
      "grad_norm": 2.171875,
      "learning_rate": 8.140646764930651e-05,
      "loss": 7.3886,
      "step": 387
    },
    {
      "epoch": 2.242424242424242,
      "grad_norm": 1.921875,
      "learning_rate": 8.025682108854779e-05,
      "loss": 7.3738,
      "step": 388
    },
    {
      "epoch": 2.248196248196248,
      "grad_norm": 2.125,
      "learning_rate": 7.911379600204368e-05,
      "loss": 7.3288,
      "step": 389
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 1.90625,
      "learning_rate": 7.797743697780788e-05,
      "loss": 7.348,
      "step": 390
    },
    {
      "epoch": 2.25974025974026,
      "grad_norm": 1.8046875,
      "learning_rate": 7.684778834381906e-05,
      "loss": 7.3762,
      "step": 391
    },
    {
      "epoch": 2.2655122655122657,
      "grad_norm": 2.125,
      "learning_rate": 7.572489416629183e-05,
      "loss": 7.3625,
      "step": 392
    },
    {
      "epoch": 2.271284271284271,
      "grad_norm": 2.109375,
      "learning_rate": 7.460879824795768e-05,
      "loss": 7.3889,
      "step": 393
    },
    {
      "epoch": 2.277056277056277,
      "grad_norm": 1.859375,
      "learning_rate": 7.349954412635631e-05,
      "loss": 7.3027,
      "step": 394
    },
    {
      "epoch": 2.282828282828283,
      "grad_norm": 1.8046875,
      "learning_rate": 7.23971750721372e-05,
      "loss": 7.3769,
      "step": 395
    },
    {
      "epoch": 2.2886002886002887,
      "grad_norm": 1.953125,
      "learning_rate": 7.1301734087372e-05,
      "loss": 7.3195,
      "step": 396
    },
    {
      "epoch": 2.2943722943722946,
      "grad_norm": 2.078125,
      "learning_rate": 7.021326390387647e-05,
      "loss": 7.3431,
      "step": 397
    },
    {
      "epoch": 2.3001443001443,
      "grad_norm": 1.8828125,
      "learning_rate": 6.913180698154414e-05,
      "loss": 7.3301,
      "step": 398
    },
    {
      "epoch": 2.305916305916306,
      "grad_norm": 2.046875,
      "learning_rate": 6.805740550668971e-05,
      "loss": 7.3838,
      "step": 399
    },
    {
      "epoch": 2.311688311688312,
      "grad_norm": 1.8828125,
      "learning_rate": 6.69901013904037e-05,
      "loss": 7.3744,
      "step": 400
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 1.96875,
      "learning_rate": 6.592993626691693e-05,
      "loss": 7.3194,
      "step": 401
    },
    {
      "epoch": 2.323232323232323,
      "grad_norm": 2.046875,
      "learning_rate": 6.487695149197703e-05,
      "loss": 7.325,
      "step": 402
    },
    {
      "epoch": 2.329004329004329,
      "grad_norm": 1.828125,
      "learning_rate": 6.383118814123517e-05,
      "loss": 7.4354,
      "step": 403
    },
    {
      "epoch": 2.334776334776335,
      "grad_norm": 1.8984375,
      "learning_rate": 6.279268700864335e-05,
      "loss": 7.3983,
      "step": 404
    },
    {
      "epoch": 2.3405483405483407,
      "grad_norm": 2.03125,
      "learning_rate": 6.176148860486344e-05,
      "loss": 7.4168,
      "step": 405
    },
    {
      "epoch": 2.346320346320346,
      "grad_norm": 1.78125,
      "learning_rate": 6.073763315568642e-05,
      "loss": 7.3198,
      "step": 406
    },
    {
      "epoch": 2.352092352092352,
      "grad_norm": 1.65625,
      "learning_rate": 5.972116060046415e-05,
      "loss": 7.3607,
      "step": 407
    },
    {
      "epoch": 2.357864357864358,
      "grad_norm": 1.8203125,
      "learning_rate": 5.871211059055037e-05,
      "loss": 7.3622,
      "step": 408
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 1.640625,
      "learning_rate": 5.771052248775455e-05,
      "loss": 7.3243,
      "step": 409
    },
    {
      "epoch": 2.3694083694083696,
      "grad_norm": 1.875,
      "learning_rate": 5.671643536280624e-05,
      "loss": 7.3511,
      "step": 410
    },
    {
      "epoch": 2.375180375180375,
      "grad_norm": 1.6796875,
      "learning_rate": 5.572988799383097e-05,
      "loss": 7.3335,
      "step": 411
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 1.828125,
      "learning_rate": 5.4750918864837655e-05,
      "loss": 7.3317,
      "step": 412
    },
    {
      "epoch": 2.386724386724387,
      "grad_norm": 1.65625,
      "learning_rate": 5.377956616421728e-05,
      "loss": 7.345,
      "step": 413
    },
    {
      "epoch": 2.3924963924963927,
      "grad_norm": 1.875,
      "learning_rate": 5.281586778325323e-05,
      "loss": 7.3244,
      "step": 414
    },
    {
      "epoch": 2.398268398268398,
      "grad_norm": 1.90625,
      "learning_rate": 5.1859861314643264e-05,
      "loss": 7.345,
      "step": 415
    },
    {
      "epoch": 2.404040404040404,
      "grad_norm": 1.6875,
      "learning_rate": 5.091158405103302e-05,
      "loss": 7.3573,
      "step": 416
    },
    {
      "epoch": 2.40981240981241,
      "grad_norm": 1.71875,
      "learning_rate": 4.9971072983561265e-05,
      "loss": 7.3029,
      "step": 417
    },
    {
      "epoch": 2.4155844155844157,
      "grad_norm": 1.78125,
      "learning_rate": 4.903836480041696e-05,
      "loss": 7.2725,
      "step": 418
    },
    {
      "epoch": 2.421356421356421,
      "grad_norm": 1.7265625,
      "learning_rate": 4.8113495885408004e-05,
      "loss": 7.3683,
      "step": 419
    },
    {
      "epoch": 2.427128427128427,
      "grad_norm": 1.7578125,
      "learning_rate": 4.71965023165421e-05,
      "loss": 7.3018,
      "step": 420
    },
    {
      "epoch": 2.432900432900433,
      "grad_norm": 1.703125,
      "learning_rate": 4.6287419864619207e-05,
      "loss": 7.3445,
      "step": 421
    },
    {
      "epoch": 2.4386724386724388,
      "grad_norm": 1.6015625,
      "learning_rate": 4.5386283991836506e-05,
      "loss": 7.3258,
      "step": 422
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.71875,
      "learning_rate": 4.449312985040443e-05,
      "loss": 7.3699,
      "step": 423
    },
    {
      "epoch": 2.45021645021645,
      "grad_norm": 1.890625,
      "learning_rate": 4.360799228117615e-05,
      "loss": 7.3176,
      "step": 424
    },
    {
      "epoch": 2.455988455988456,
      "grad_norm": 1.734375,
      "learning_rate": 4.2730905812287915e-05,
      "loss": 7.3556,
      "step": 425
    },
    {
      "epoch": 2.461760461760462,
      "grad_norm": 1.8828125,
      "learning_rate": 4.18619046578127e-05,
      "loss": 7.3462,
      "step": 426
    },
    {
      "epoch": 2.4675324675324677,
      "grad_norm": 1.671875,
      "learning_rate": 4.100102271642478e-05,
      "loss": 7.3487,
      "step": 427
    },
    {
      "epoch": 2.473304473304473,
      "grad_norm": 1.828125,
      "learning_rate": 4.014829357007818e-05,
      "loss": 7.3398,
      "step": 428
    },
    {
      "epoch": 2.479076479076479,
      "grad_norm": 1.6171875,
      "learning_rate": 3.930375048269613e-05,
      "loss": 7.31,
      "step": 429
    },
    {
      "epoch": 2.484848484848485,
      "grad_norm": 1.65625,
      "learning_rate": 3.846742639887391e-05,
      "loss": 7.2804,
      "step": 430
    },
    {
      "epoch": 2.4906204906204907,
      "grad_norm": 1.5625,
      "learning_rate": 3.7639353942593404e-05,
      "loss": 7.2832,
      "step": 431
    },
    {
      "epoch": 2.496392496392496,
      "grad_norm": 1.6875,
      "learning_rate": 3.6819565415950355e-05,
      "loss": 7.3428,
      "step": 432
    },
    {
      "epoch": 2.502164502164502,
      "grad_norm": 1.7890625,
      "learning_rate": 3.600809279789488e-05,
      "loss": 7.2736,
      "step": 433
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 1.609375,
      "learning_rate": 3.5204967742983456e-05,
      "loss": 7.333,
      "step": 434
    },
    {
      "epoch": 2.513708513708514,
      "grad_norm": 1.640625,
      "learning_rate": 3.4410221580144394e-05,
      "loss": 7.2806,
      "step": 435
    },
    {
      "epoch": 2.5194805194805197,
      "grad_norm": 1.765625,
      "learning_rate": 3.362388531145544e-05,
      "loss": 7.2165,
      "step": 436
    },
    {
      "epoch": 2.525252525252525,
      "grad_norm": 1.640625,
      "learning_rate": 3.284598961093491e-05,
      "loss": 7.3242,
      "step": 437
    },
    {
      "epoch": 2.531024531024531,
      "grad_norm": 1.6796875,
      "learning_rate": 3.20765648233447e-05,
      "loss": 7.3048,
      "step": 438
    },
    {
      "epoch": 2.536796536796537,
      "grad_norm": 1.625,
      "learning_rate": 3.131564096300679e-05,
      "loss": 7.307,
      "step": 439
    },
    {
      "epoch": 2.5425685425685427,
      "grad_norm": 1.5859375,
      "learning_rate": 3.056324771263233e-05,
      "loss": 7.3535,
      "step": 440
    },
    {
      "epoch": 2.5483405483405486,
      "grad_norm": 1.6875,
      "learning_rate": 2.9819414422163792e-05,
      "loss": 7.2594,
      "step": 441
    },
    {
      "epoch": 2.554112554112554,
      "grad_norm": 1.890625,
      "learning_rate": 2.9084170107630064e-05,
      "loss": 7.361,
      "step": 442
    },
    {
      "epoch": 2.55988455988456,
      "grad_norm": 1.71875,
      "learning_rate": 2.8357543450014566e-05,
      "loss": 7.3175,
      "step": 443
    },
    {
      "epoch": 2.5656565656565657,
      "grad_norm": 1.5859375,
      "learning_rate": 2.763956279413643e-05,
      "loss": 7.2926,
      "step": 444
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.7109375,
      "learning_rate": 2.6930256147544823e-05,
      "loss": 7.3337,
      "step": 445
    },
    {
      "epoch": 2.577200577200577,
      "grad_norm": 1.7421875,
      "learning_rate": 2.6229651179426383e-05,
      "loss": 7.2869,
      "step": 446
    },
    {
      "epoch": 2.582972582972583,
      "grad_norm": 1.6953125,
      "learning_rate": 2.553777521952591e-05,
      "loss": 7.3315,
      "step": 447
    },
    {
      "epoch": 2.588744588744589,
      "grad_norm": 1.640625,
      "learning_rate": 2.4854655257080193e-05,
      "loss": 7.3513,
      "step": 448
    },
    {
      "epoch": 2.5945165945165947,
      "grad_norm": 1.4453125,
      "learning_rate": 2.4180317939765355e-05,
      "loss": 7.3528,
      "step": 449
    },
    {
      "epoch": 2.6002886002886,
      "grad_norm": 1.5703125,
      "learning_rate": 2.3514789572657126e-05,
      "loss": 7.3207,
      "step": 450
    },
    {
      "epoch": 2.606060606060606,
      "grad_norm": 1.5859375,
      "learning_rate": 2.285809611720488e-05,
      "loss": 7.2812,
      "step": 451
    },
    {
      "epoch": 2.611832611832612,
      "grad_norm": 1.578125,
      "learning_rate": 2.221026319021896e-05,
      "loss": 7.3321,
      "step": 452
    },
    {
      "epoch": 2.6176046176046177,
      "grad_norm": 1.578125,
      "learning_rate": 2.1571316062871242e-05,
      "loss": 7.2548,
      "step": 453
    },
    {
      "epoch": 2.6233766233766236,
      "grad_norm": 1.8046875,
      "learning_rate": 2.0941279659709295e-05,
      "loss": 7.3202,
      "step": 454
    },
    {
      "epoch": 2.629148629148629,
      "grad_norm": 1.75,
      "learning_rate": 2.032017855768431e-05,
      "loss": 7.3014,
      "step": 455
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 1.78125,
      "learning_rate": 1.9708036985192356e-05,
      "loss": 7.2923,
      "step": 456
    },
    {
      "epoch": 2.6406926406926408,
      "grad_norm": 1.5859375,
      "learning_rate": 1.9104878821129156e-05,
      "loss": 7.2865,
      "step": 457
    },
    {
      "epoch": 2.6464646464646466,
      "grad_norm": 1.5703125,
      "learning_rate": 1.8510727593958472e-05,
      "loss": 7.3449,
      "step": 458
    },
    {
      "epoch": 2.6522366522366525,
      "grad_norm": 1.6640625,
      "learning_rate": 1.7925606480794637e-05,
      "loss": 7.296,
      "step": 459
    },
    {
      "epoch": 2.658008658008658,
      "grad_norm": 1.6953125,
      "learning_rate": 1.7349538306498212e-05,
      "loss": 7.2598,
      "step": 460
    },
    {
      "epoch": 2.663780663780664,
      "grad_norm": 1.625,
      "learning_rate": 1.678254554278566e-05,
      "loss": 7.3797,
      "step": 461
    },
    {
      "epoch": 2.6695526695526697,
      "grad_norm": 1.8125,
      "learning_rate": 1.6224650307352594e-05,
      "loss": 7.2302,
      "step": 462
    },
    {
      "epoch": 2.675324675324675,
      "grad_norm": 1.5625,
      "learning_rate": 1.5675874363011462e-05,
      "loss": 7.3868,
      "step": 463
    },
    {
      "epoch": 2.681096681096681,
      "grad_norm": 1.5390625,
      "learning_rate": 1.5136239116842033e-05,
      "loss": 7.2577,
      "step": 464
    },
    {
      "epoch": 2.686868686868687,
      "grad_norm": 1.5625,
      "learning_rate": 1.4605765619356726e-05,
      "loss": 7.3482,
      "step": 465
    },
    {
      "epoch": 2.6926406926406927,
      "grad_norm": 1.6328125,
      "learning_rate": 1.4084474563679284e-05,
      "loss": 7.2526,
      "step": 466
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 1.7265625,
      "learning_rate": 1.3572386284737587e-05,
      "loss": 7.315,
      "step": 467
    },
    {
      "epoch": 2.704184704184704,
      "grad_norm": 1.65625,
      "learning_rate": 1.3069520758470454e-05,
      "loss": 7.293,
      "step": 468
    },
    {
      "epoch": 2.70995670995671,
      "grad_norm": 1.65625,
      "learning_rate": 1.2575897601048353e-05,
      "loss": 7.256,
      "step": 469
    },
    {
      "epoch": 2.7157287157287158,
      "grad_norm": 1.5859375,
      "learning_rate": 1.2091536068108255e-05,
      "loss": 7.2494,
      "step": 470
    },
    {
      "epoch": 2.7215007215007216,
      "grad_norm": 1.546875,
      "learning_rate": 1.161645505400244e-05,
      "loss": 7.2463,
      "step": 471
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 1.6796875,
      "learning_rate": 1.1150673091061491e-05,
      "loss": 7.3068,
      "step": 472
    },
    {
      "epoch": 2.733044733044733,
      "grad_norm": 1.734375,
      "learning_rate": 1.0694208348871304e-05,
      "loss": 7.2805,
      "step": 473
    },
    {
      "epoch": 2.738816738816739,
      "grad_norm": 1.6015625,
      "learning_rate": 1.0247078633564416e-05,
      "loss": 7.3481,
      "step": 474
    },
    {
      "epoch": 2.7445887445887447,
      "grad_norm": 1.5390625,
      "learning_rate": 9.809301387125408e-06,
      "loss": 7.3268,
      "step": 475
    },
    {
      "epoch": 2.75036075036075,
      "grad_norm": 1.515625,
      "learning_rate": 9.380893686710328e-06,
      "loss": 7.3259,
      "step": 476
    },
    {
      "epoch": 2.756132756132756,
      "grad_norm": 1.65625,
      "learning_rate": 8.961872243980734e-06,
      "loss": 7.337,
      "step": 477
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 1.5859375,
      "learning_rate": 8.55225340445176e-06,
      "loss": 7.3464,
      "step": 478
    },
    {
      "epoch": 2.7676767676767677,
      "grad_norm": 1.5625,
      "learning_rate": 8.152053146854465e-06,
      "loss": 7.3317,
      "step": 479
    },
    {
      "epoch": 2.7734487734487736,
      "grad_norm": 1.59375,
      "learning_rate": 7.761287082512447e-06,
      "loss": 7.3394,
      "step": 480
    },
    {
      "epoch": 2.779220779220779,
      "grad_norm": 1.4921875,
      "learning_rate": 7.379970454732959e-06,
      "loss": 7.2586,
      "step": 481
    },
    {
      "epoch": 2.784992784992785,
      "grad_norm": 1.546875,
      "learning_rate": 7.008118138212394e-06,
      "loss": 7.2969,
      "step": 482
    },
    {
      "epoch": 2.790764790764791,
      "grad_norm": 1.6328125,
      "learning_rate": 6.645744638455742e-06,
      "loss": 7.229,
      "step": 483
    },
    {
      "epoch": 2.7965367965367967,
      "grad_norm": 1.5,
      "learning_rate": 6.292864091210964e-06,
      "loss": 7.289,
      "step": 484
    },
    {
      "epoch": 2.8023088023088025,
      "grad_norm": 1.6015625,
      "learning_rate": 5.949490261917573e-06,
      "loss": 7.2469,
      "step": 485
    },
    {
      "epoch": 2.808080808080808,
      "grad_norm": 1.5859375,
      "learning_rate": 5.615636545169639e-06,
      "loss": 7.3073,
      "step": 486
    },
    {
      "epoch": 2.813852813852814,
      "grad_norm": 1.59375,
      "learning_rate": 5.291315964193161e-06,
      "loss": 7.2862,
      "step": 487
    },
    {
      "epoch": 2.8196248196248197,
      "grad_norm": 1.546875,
      "learning_rate": 4.9765411703382446e-06,
      "loss": 7.2848,
      "step": 488
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 1.484375,
      "learning_rate": 4.6713244425854175e-06,
      "loss": 7.3079,
      "step": 489
    },
    {
      "epoch": 2.8311688311688314,
      "grad_norm": 1.546875,
      "learning_rate": 4.375677687066731e-06,
      "loss": 7.3509,
      "step": 490
    },
    {
      "epoch": 2.836940836940837,
      "grad_norm": 1.5625,
      "learning_rate": 4.089612436601359e-06,
      "loss": 7.252,
      "step": 491
    },
    {
      "epoch": 2.8427128427128427,
      "grad_norm": 1.5078125,
      "learning_rate": 3.8131398502455615e-06,
      "loss": 7.2644,
      "step": 492
    },
    {
      "epoch": 2.8484848484848486,
      "grad_norm": 1.5078125,
      "learning_rate": 3.5462707128575687e-06,
      "loss": 7.3307,
      "step": 493
    },
    {
      "epoch": 2.854256854256854,
      "grad_norm": 1.640625,
      "learning_rate": 3.28901543467669e-06,
      "loss": 7.2691,
      "step": 494
    },
    {
      "epoch": 2.86002886002886,
      "grad_norm": 1.4296875,
      "learning_rate": 3.0413840509174173e-06,
      "loss": 7.2918,
      "step": 495
    },
    {
      "epoch": 2.865800865800866,
      "grad_norm": 1.6015625,
      "learning_rate": 2.803386221377796e-06,
      "loss": 7.3228,
      "step": 496
    },
    {
      "epoch": 2.8715728715728717,
      "grad_norm": 1.515625,
      "learning_rate": 2.5750312300627244e-06,
      "loss": 7.3752,
      "step": 497
    },
    {
      "epoch": 2.8773448773448775,
      "grad_norm": 1.625,
      "learning_rate": 2.3563279848216877e-06,
      "loss": 7.3198,
      "step": 498
    },
    {
      "epoch": 2.883116883116883,
      "grad_norm": 1.625,
      "learning_rate": 2.1472850170014268e-06,
      "loss": 7.3895,
      "step": 499
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.5703125,
      "learning_rate": 1.947910481112952e-06,
      "loss": 7.3638,
      "step": 500
    },
    {
      "epoch": 2.8946608946608947,
      "grad_norm": 1.5,
      "learning_rate": 1.7582121545136609e-06,
      "loss": 7.3253,
      "step": 501
    },
    {
      "epoch": 2.9004329004329006,
      "grad_norm": 1.5,
      "learning_rate": 1.5781974371037178e-06,
      "loss": 7.3448,
      "step": 502
    },
    {
      "epoch": 2.9062049062049065,
      "grad_norm": 1.453125,
      "learning_rate": 1.4078733510375364e-06,
      "loss": 7.2582,
      "step": 503
    },
    {
      "epoch": 2.911976911976912,
      "grad_norm": 1.515625,
      "learning_rate": 1.2472465404498867e-06,
      "loss": 7.3183,
      "step": 504
    },
    {
      "epoch": 2.9177489177489178,
      "grad_norm": 1.53125,
      "learning_rate": 1.0963232711966309e-06,
      "loss": 7.2997,
      "step": 505
    },
    {
      "epoch": 2.9235209235209236,
      "grad_norm": 1.4140625,
      "learning_rate": 9.551094306102793e-07,
      "loss": 7.2993,
      "step": 506
    },
    {
      "epoch": 2.929292929292929,
      "grad_norm": 1.5390625,
      "learning_rate": 8.236105272704242e-07,
      "loss": 7.3496,
      "step": 507
    },
    {
      "epoch": 2.935064935064935,
      "grad_norm": 1.6171875,
      "learning_rate": 7.018316907888289e-07,
      "loss": 7.3241,
      "step": 508
    },
    {
      "epoch": 2.940836940836941,
      "grad_norm": 1.5703125,
      "learning_rate": 5.897776716092818e-07,
      "loss": 7.3448,
      "step": 509
    },
    {
      "epoch": 2.9466089466089467,
      "grad_norm": 1.5078125,
      "learning_rate": 4.874528408223e-07,
      "loss": 7.3153,
      "step": 510
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 1.5,
      "learning_rate": 3.948611899946553e-07,
      "loss": 7.2916,
      "step": 511
    },
    {
      "epoch": 2.958152958152958,
      "grad_norm": 1.6015625,
      "learning_rate": 3.1200633101366447e-07,
      "loss": 7.2888,
      "step": 512
    },
    {
      "epoch": 2.963924963924964,
      "grad_norm": 1.5625,
      "learning_rate": 2.3889149594624736e-07,
      "loss": 7.3153,
      "step": 513
    },
    {
      "epoch": 2.9696969696969697,
      "grad_norm": 1.5859375,
      "learning_rate": 1.7551953691288813e-07,
      "loss": 7.3966,
      "step": 514
    },
    {
      "epoch": 2.9754689754689756,
      "grad_norm": 1.53125,
      "learning_rate": 1.2189292597639122e-07,
      "loss": 7.285,
      "step": 515
    },
    {
      "epoch": 2.9812409812409815,
      "grad_norm": 1.6015625,
      "learning_rate": 7.801375504537522e-08,
      "loss": 7.2693,
      "step": 516
    },
    {
      "epoch": 2.987012987012987,
      "grad_norm": 1.4609375,
      "learning_rate": 4.388373579275462e-08,
      "loss": 7.2985,
      "step": 517
    },
    {
      "epoch": 2.9927849927849928,
      "grad_norm": 1.53125,
      "learning_rate": 1.9504199588932235e-08,
      "loss": 7.3026,
      "step": 518
    },
    {
      "epoch": 2.9985569985569986,
      "grad_norm": 1.5234375,
      "learning_rate": 4.876097449896255e-09,
      "loss": 7.2832,
      "step": 519
    }
  ],
  "logging_steps": 1,
  "max_steps": 519,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.03982778662912e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

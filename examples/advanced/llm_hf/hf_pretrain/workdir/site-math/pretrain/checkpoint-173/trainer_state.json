{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9985569985569985,
  "eval_steps": 500,
  "global_step": 173,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005772005772005772,
      "grad_norm": 13.5,
      "learning_rate": 0.0,
      "loss": 8.5981,
      "step": 1
    },
    {
      "epoch": 0.011544011544011544,
      "grad_norm": 10.875,
      "learning_rate": 8.333333333333333e-05,
      "loss": 8.6744,
      "step": 2
    },
    {
      "epoch": 0.017316017316017316,
      "grad_norm": 7.625,
      "learning_rate": 0.00016666666666666666,
      "loss": 7.9187,
      "step": 3
    },
    {
      "epoch": 0.023088023088023088,
      "grad_norm": 16.625,
      "learning_rate": 0.00025,
      "loss": 7.9568,
      "step": 4
    },
    {
      "epoch": 0.02886002886002886,
      "grad_norm": 101.0,
      "learning_rate": 0.0003333333333333333,
      "loss": 11.9362,
      "step": 5
    },
    {
      "epoch": 0.03463203463203463,
      "grad_norm": 28.5,
      "learning_rate": 0.0004166666666666667,
      "loss": 9.6598,
      "step": 6
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 35.0,
      "learning_rate": 0.0005,
      "loss": 9.4126,
      "step": 7
    },
    {
      "epoch": 0.046176046176046176,
      "grad_norm": 38.5,
      "learning_rate": 0.0004999557652060729,
      "loss": 9.1622,
      "step": 8
    },
    {
      "epoch": 0.05194805194805195,
      "grad_norm": 8.25,
      "learning_rate": 0.0004998230764780276,
      "loss": 8.5241,
      "step": 9
    },
    {
      "epoch": 0.05772005772005772,
      "grad_norm": 10.375,
      "learning_rate": 0.0004996019807715324,
      "loss": 8.3689,
      "step": 10
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 8.125,
      "learning_rate": 0.0004992925563275714,
      "loss": 7.9521,
      "step": 11
    },
    {
      "epoch": 0.06926406926406926,
      "grad_norm": 14.375,
      "learning_rate": 0.0004988949126447567,
      "loss": 8.529,
      "step": 12
    },
    {
      "epoch": 0.07503607503607504,
      "grad_norm": 11.3125,
      "learning_rate": 0.0004984091904405792,
      "loss": 7.954,
      "step": 13
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 7.0625,
      "learning_rate": 0.000497835561601612,
      "loss": 7.7266,
      "step": 14
    },
    {
      "epoch": 0.08658008658008658,
      "grad_norm": 10.4375,
      "learning_rate": 0.0004971742291226826,
      "loss": 8.1219,
      "step": 15
    },
    {
      "epoch": 0.09235209235209235,
      "grad_norm": 23.125,
      "learning_rate": 0.0004964254270350387,
      "loss": 8.0693,
      "step": 16
    },
    {
      "epoch": 0.09812409812409813,
      "grad_norm": 8.375,
      "learning_rate": 0.0004955894203235284,
      "loss": 7.7709,
      "step": 17
    },
    {
      "epoch": 0.1038961038961039,
      "grad_norm": 5.375,
      "learning_rate": 0.0004946665048328287,
      "loss": 7.7282,
      "step": 18
    },
    {
      "epoch": 0.10966810966810966,
      "grad_norm": 5.21875,
      "learning_rate": 0.0004936570071627517,
      "loss": 7.6397,
      "step": 19
    },
    {
      "epoch": 0.11544011544011544,
      "grad_norm": 8.6875,
      "learning_rate": 0.0004925612845526691,
      "loss": 7.5681,
      "step": 20
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 5.5,
      "learning_rate": 0.0004913797247550911,
      "loss": 7.5938,
      "step": 21
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 4.34375,
      "learning_rate": 0.0004901127458984516,
      "loss": 7.5745,
      "step": 22
    },
    {
      "epoch": 0.13275613275613277,
      "grad_norm": 4.5625,
      "learning_rate": 0.0004887607963391394,
      "loss": 7.5515,
      "step": 23
    },
    {
      "epoch": 0.13852813852813853,
      "grad_norm": 3.515625,
      "learning_rate": 0.00048732435450283564,
      "loss": 7.471,
      "step": 24
    },
    {
      "epoch": 0.1443001443001443,
      "grad_norm": 5.59375,
      "learning_rate": 0.00048580392871520943,
      "loss": 7.5775,
      "step": 25
    },
    {
      "epoch": 0.15007215007215008,
      "grad_norm": 3.015625,
      "learning_rate": 0.00048420005702203196,
      "loss": 7.4914,
      "step": 26
    },
    {
      "epoch": 0.15584415584415584,
      "grad_norm": 3.140625,
      "learning_rate": 0.00048251330699877374,
      "loss": 7.5697,
      "step": 27
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 3.046875,
      "learning_rate": 0.00048074427554975236,
      "loss": 7.5102,
      "step": 28
    },
    {
      "epoch": 0.1673881673881674,
      "grad_norm": 3.0,
      "learning_rate": 0.00047889358869690056,
      "loss": 7.5294,
      "step": 29
    },
    {
      "epoch": 0.17316017316017315,
      "grad_norm": 3.15625,
      "learning_rate": 0.0004769619013582309,
      "loss": 7.5263,
      "step": 30
    },
    {
      "epoch": 0.17893217893217894,
      "grad_norm": 2.53125,
      "learning_rate": 0.00047494989711607415,
      "loss": 7.5215,
      "step": 31
    },
    {
      "epoch": 0.1847041847041847,
      "grad_norm": 2.609375,
      "learning_rate": 0.0004728582879751746,
      "loss": 7.524,
      "step": 32
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.078125,
      "learning_rate": 0.00047068781411072687,
      "loss": 7.4817,
      "step": 33
    },
    {
      "epoch": 0.19624819624819625,
      "grad_norm": 2.828125,
      "learning_rate": 0.00046843924360644385,
      "loss": 7.4674,
      "step": 34
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 2.78125,
      "learning_rate": 0.00046611337218274864,
      "loss": 7.5368,
      "step": 35
    },
    {
      "epoch": 0.2077922077922078,
      "grad_norm": 2.734375,
      "learning_rate": 0.0004637110229151863,
      "loss": 7.5882,
      "step": 36
    },
    {
      "epoch": 0.21356421356421357,
      "grad_norm": 2.09375,
      "learning_rate": 0.00046123304594315517,
      "loss": 7.4634,
      "step": 37
    },
    {
      "epoch": 0.21933621933621933,
      "grad_norm": 2.875,
      "learning_rate": 0.0004586803181690609,
      "loss": 7.5288,
      "step": 38
    },
    {
      "epoch": 0.22510822510822512,
      "grad_norm": 2.640625,
      "learning_rate": 0.0004560537429479998,
      "loss": 7.5046,
      "step": 39
    },
    {
      "epoch": 0.23088023088023088,
      "grad_norm": 2.78125,
      "learning_rate": 0.00045335424976808116,
      "loss": 7.5484,
      "step": 40
    },
    {
      "epoch": 0.23665223665223664,
      "grad_norm": 2.03125,
      "learning_rate": 0.0004505827939215009,
      "loss": 7.5354,
      "step": 41
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 2.75,
      "learning_rate": 0.00044774035616648516,
      "loss": 7.4529,
      "step": 42
    },
    {
      "epoch": 0.2481962481962482,
      "grad_norm": 2.515625,
      "learning_rate": 0.0004448279423802207,
      "loss": 7.5861,
      "step": 43
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 2.1875,
      "learning_rate": 0.0004418465832028967,
      "loss": 7.5338,
      "step": 44
    },
    {
      "epoch": 0.2597402597402597,
      "grad_norm": 2.109375,
      "learning_rate": 0.00043879733367298404,
      "loss": 7.4979,
      "step": 45
    },
    {
      "epoch": 0.26551226551226553,
      "grad_norm": 2.109375,
      "learning_rate": 0.00043568127285387924,
      "loss": 7.5978,
      "step": 46
    },
    {
      "epoch": 0.2712842712842713,
      "grad_norm": 2.234375,
      "learning_rate": 0.00043249950345204804,
      "loss": 7.5724,
      "step": 47
    },
    {
      "epoch": 0.27705627705627706,
      "grad_norm": 2.234375,
      "learning_rate": 0.0004292531514268008,
      "loss": 7.5828,
      "step": 48
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 1.828125,
      "learning_rate": 0.00042594336559184035,
      "loss": 7.4608,
      "step": 49
    },
    {
      "epoch": 0.2886002886002886,
      "grad_norm": 2.296875,
      "learning_rate": 0.0004225713172087216,
      "loss": 7.4937,
      "step": 50
    },
    {
      "epoch": 0.2943722943722944,
      "grad_norm": 2.40625,
      "learning_rate": 0.0004191381995723672,
      "loss": 7.4861,
      "step": 51
    },
    {
      "epoch": 0.30014430014430016,
      "grad_norm": 2.40625,
      "learning_rate": 0.00041564522758878654,
      "loss": 7.4319,
      "step": 52
    },
    {
      "epoch": 0.3059163059163059,
      "grad_norm": 2.078125,
      "learning_rate": 0.0004120936373451467,
      "loss": 7.5124,
      "step": 53
    },
    {
      "epoch": 0.3116883116883117,
      "grad_norm": 2.046875,
      "learning_rate": 0.000408484685672348,
      "loss": 7.5071,
      "step": 54
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 2.28125,
      "learning_rate": 0.0004048196497002588,
      "loss": 7.5044,
      "step": 55
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 1.859375,
      "learning_rate": 0.0004010998264057667,
      "loss": 7.4585,
      "step": 56
    },
    {
      "epoch": 0.329004329004329,
      "grad_norm": 1.984375,
      "learning_rate": 0.0003973265321538069,
      "loss": 7.4923,
      "step": 57
    },
    {
      "epoch": 0.3347763347763348,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0003935011022315284,
      "loss": 7.4486,
      "step": 58
    },
    {
      "epoch": 0.34054834054834054,
      "grad_norm": 1.875,
      "learning_rate": 0.00038962489037576583,
      "loss": 7.4381,
      "step": 59
    },
    {
      "epoch": 0.3463203463203463,
      "grad_norm": 2.125,
      "learning_rate": 0.0003856992682939803,
      "loss": 7.5355,
      "step": 60
    },
    {
      "epoch": 0.35209235209235207,
      "grad_norm": 1.75,
      "learning_rate": 0.0003817256251788425,
      "loss": 7.4856,
      "step": 61
    },
    {
      "epoch": 0.3578643578643579,
      "grad_norm": 1.9609375,
      "learning_rate": 0.00037770536721662694,
      "loss": 7.4742,
      "step": 62
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 1.9296875,
      "learning_rate": 0.0003736399170895938,
      "loss": 7.4431,
      "step": 63
    },
    {
      "epoch": 0.3694083694083694,
      "grad_norm": 2.21875,
      "learning_rate": 0.0003695307134725316,
      "loss": 7.5604,
      "step": 64
    },
    {
      "epoch": 0.37518037518037517,
      "grad_norm": 1.9453125,
      "learning_rate": 0.0003653792105236422,
      "loss": 7.4036,
      "step": 65
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 1.8828125,
      "learning_rate": 0.00036118687736994487,
      "loss": 7.4354,
      "step": 66
    },
    {
      "epoch": 0.38672438672438675,
      "grad_norm": 1.8515625,
      "learning_rate": 0.0003569551975873847,
      "loss": 7.495,
      "step": 67
    },
    {
      "epoch": 0.3924963924963925,
      "grad_norm": 1.921875,
      "learning_rate": 0.00035268566867582683,
      "loss": 7.4922,
      "step": 68
    },
    {
      "epoch": 0.39826839826839827,
      "grad_norm": 1.7109375,
      "learning_rate": 0.0003483798015291239,
      "loss": 7.3793,
      "step": 69
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 1.71875,
      "learning_rate": 0.00034403911990044307,
      "loss": 7.3737,
      "step": 70
    },
    {
      "epoch": 0.4098124098124098,
      "grad_norm": 2.15625,
      "learning_rate": 0.00033966515986304317,
      "loss": 7.4197,
      "step": 71
    },
    {
      "epoch": 0.4155844155844156,
      "grad_norm": 2.15625,
      "learning_rate": 0.0003352594692666915,
      "loss": 7.3202,
      "step": 72
    },
    {
      "epoch": 0.4213564213564214,
      "grad_norm": 1.671875,
      "learning_rate": 0.000330823607189913,
      "loss": 7.4655,
      "step": 73
    },
    {
      "epoch": 0.42712842712842713,
      "grad_norm": 1.7421875,
      "learning_rate": 0.0003263591433882666,
      "loss": 7.4769,
      "step": 74
    },
    {
      "epoch": 0.4329004329004329,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00032186765773884244,
      "loss": 7.5285,
      "step": 75
    },
    {
      "epoch": 0.43867243867243866,
      "grad_norm": 1.921875,
      "learning_rate": 0.0003173507396811774,
      "loss": 7.4527,
      "step": 76
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.734375,
      "learning_rate": 0.00031280998765478727,
      "loss": 7.3743,
      "step": 77
    },
    {
      "epoch": 0.45021645021645024,
      "grad_norm": 2.109375,
      "learning_rate": 0.0003082470085335133,
      "loss": 7.4309,
      "step": 78
    },
    {
      "epoch": 0.455988455988456,
      "grad_norm": 1.828125,
      "learning_rate": 0.00030366341705688466,
      "loss": 7.3623,
      "step": 79
    },
    {
      "epoch": 0.46176046176046176,
      "grad_norm": 1.5625,
      "learning_rate": 0.0002990608352586965,
      "loss": 7.4157,
      "step": 80
    },
    {
      "epoch": 0.4675324675324675,
      "grad_norm": 1.7578125,
      "learning_rate": 0.00029444089189300783,
      "loss": 7.4327,
      "step": 81
    },
    {
      "epoch": 0.4733044733044733,
      "grad_norm": 1.6875,
      "learning_rate": 0.00028980522185776065,
      "loss": 7.4362,
      "step": 82
    },
    {
      "epoch": 0.4790764790764791,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00028515546561622466,
      "loss": 7.4027,
      "step": 83
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 1.9609375,
      "learning_rate": 0.000280493268616473,
      "loss": 7.4137,
      "step": 84
    },
    {
      "epoch": 0.4906204906204906,
      "grad_norm": 1.8046875,
      "learning_rate": 0.0002758202807090941,
      "loss": 7.4065,
      "step": 85
    },
    {
      "epoch": 0.4963924963924964,
      "grad_norm": 1.59375,
      "learning_rate": 0.00027113815556334474,
      "loss": 7.4405,
      "step": 86
    },
    {
      "epoch": 0.5021645021645021,
      "grad_norm": 1.734375,
      "learning_rate": 0.00026644855008195267,
      "loss": 7.3764,
      "step": 87
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 1.6484375,
      "learning_rate": 0.0002617531238147744,
      "loss": 7.4357,
      "step": 88
    },
    {
      "epoch": 0.5137085137085137,
      "grad_norm": 1.890625,
      "learning_rate": 0.0002570535383715165,
      "loss": 7.429,
      "step": 89
    },
    {
      "epoch": 0.5194805194805194,
      "grad_norm": 1.671875,
      "learning_rate": 0.0002523514568337281,
      "loss": 7.4084,
      "step": 90
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 1.796875,
      "learning_rate": 0.000247648543166272,
      "loss": 7.4225,
      "step": 91
    },
    {
      "epoch": 0.5310245310245311,
      "grad_norm": 1.9140625,
      "learning_rate": 0.00024294646162848353,
      "loss": 7.46,
      "step": 92
    },
    {
      "epoch": 0.5367965367965368,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00023824687618522567,
      "loss": 7.4939,
      "step": 93
    },
    {
      "epoch": 0.5425685425685426,
      "grad_norm": 1.921875,
      "learning_rate": 0.00023355144991804737,
      "loss": 7.4228,
      "step": 94
    },
    {
      "epoch": 0.5483405483405484,
      "grad_norm": 1.5625,
      "learning_rate": 0.00022886184443665522,
      "loss": 7.4992,
      "step": 95
    },
    {
      "epoch": 0.5541125541125541,
      "grad_norm": 1.6953125,
      "learning_rate": 0.0002241797192909059,
      "loss": 7.3876,
      "step": 96
    },
    {
      "epoch": 0.5598845598845599,
      "grad_norm": 2.046875,
      "learning_rate": 0.000219506731383527,
      "loss": 7.4043,
      "step": 97
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 1.7734375,
      "learning_rate": 0.0002148445343837755,
      "loss": 7.457,
      "step": 98
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.890625,
      "learning_rate": 0.00021019477814223942,
      "loss": 7.3575,
      "step": 99
    },
    {
      "epoch": 0.5772005772005772,
      "grad_norm": 2.03125,
      "learning_rate": 0.0002055591081069922,
      "loss": 7.4039,
      "step": 100
    },
    {
      "epoch": 0.5829725829725829,
      "grad_norm": 1.734375,
      "learning_rate": 0.00020093916474130352,
      "loss": 7.4031,
      "step": 101
    },
    {
      "epoch": 0.5887445887445888,
      "grad_norm": 1.7421875,
      "learning_rate": 0.00019633658294311535,
      "loss": 7.3922,
      "step": 102
    },
    {
      "epoch": 0.5945165945165946,
      "grad_norm": 1.953125,
      "learning_rate": 0.0001917529914664867,
      "loss": 7.3967,
      "step": 103
    },
    {
      "epoch": 0.6002886002886003,
      "grad_norm": 1.6875,
      "learning_rate": 0.00018719001234521283,
      "loss": 7.402,
      "step": 104
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.65625,
      "learning_rate": 0.00018264926031882274,
      "loss": 7.3653,
      "step": 105
    },
    {
      "epoch": 0.6118326118326118,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00017813234226115766,
      "loss": 7.4276,
      "step": 106
    },
    {
      "epoch": 0.6176046176046176,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00017364085661173345,
      "loss": 7.4163,
      "step": 107
    },
    {
      "epoch": 0.6233766233766234,
      "grad_norm": 1.4921875,
      "learning_rate": 0.000169176392810087,
      "loss": 7.3949,
      "step": 108
    },
    {
      "epoch": 0.6291486291486291,
      "grad_norm": 1.453125,
      "learning_rate": 0.0001647405307333085,
      "loss": 7.3474,
      "step": 109
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00016033484013695687,
      "loss": 7.3913,
      "step": 110
    },
    {
      "epoch": 0.6406926406926406,
      "grad_norm": 1.5078125,
      "learning_rate": 0.00015596088009955694,
      "loss": 7.3751,
      "step": 111
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 1.53125,
      "learning_rate": 0.00015162019847087617,
      "loss": 7.4067,
      "step": 112
    },
    {
      "epoch": 0.6522366522366523,
      "grad_norm": 1.421875,
      "learning_rate": 0.00014731433132417315,
      "loss": 7.3936,
      "step": 113
    },
    {
      "epoch": 0.658008658008658,
      "grad_norm": 1.4375,
      "learning_rate": 0.00014304480241261527,
      "loss": 7.3803,
      "step": 114
    },
    {
      "epoch": 0.6637806637806638,
      "grad_norm": 1.390625,
      "learning_rate": 0.0001388131226300552,
      "loss": 7.3383,
      "step": 115
    },
    {
      "epoch": 0.6695526695526696,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0001346207894763578,
      "loss": 7.3762,
      "step": 116
    },
    {
      "epoch": 0.6753246753246753,
      "grad_norm": 1.640625,
      "learning_rate": 0.0001304692865274683,
      "loss": 7.3567,
      "step": 117
    },
    {
      "epoch": 0.6810966810966811,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0001263600829104062,
      "loss": 7.4187,
      "step": 118
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00012229463278337307,
      "loss": 7.409,
      "step": 119
    },
    {
      "epoch": 0.6926406926406926,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00011827437482115758,
      "loss": 7.3818,
      "step": 120
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 1.640625,
      "learning_rate": 0.00011430073170601968,
      "loss": 7.4282,
      "step": 121
    },
    {
      "epoch": 0.7041847041847041,
      "grad_norm": 1.7265625,
      "learning_rate": 0.00011037510962423425,
      "loss": 7.4073,
      "step": 122
    },
    {
      "epoch": 0.70995670995671,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0001064988977684716,
      "loss": 7.3963,
      "step": 123
    },
    {
      "epoch": 0.7157287157287158,
      "grad_norm": 1.625,
      "learning_rate": 0.00010267346784619325,
      "loss": 7.337,
      "step": 124
    },
    {
      "epoch": 0.7215007215007215,
      "grad_norm": 1.609375,
      "learning_rate": 9.890017359423326e-05,
      "loss": 7.3859,
      "step": 125
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.3671875,
      "learning_rate": 9.518035029974126e-05,
      "loss": 7.3205,
      "step": 126
    },
    {
      "epoch": 0.733044733044733,
      "grad_norm": 1.375,
      "learning_rate": 9.151531432765204e-05,
      "loss": 7.3241,
      "step": 127
    },
    {
      "epoch": 0.7388167388167388,
      "grad_norm": 1.5,
      "learning_rate": 8.790636265485333e-05,
      "loss": 7.4123,
      "step": 128
    },
    {
      "epoch": 0.7445887445887446,
      "grad_norm": 1.2578125,
      "learning_rate": 8.435477241121353e-05,
      "loss": 7.3948,
      "step": 129
    },
    {
      "epoch": 0.7503607503607503,
      "grad_norm": 1.390625,
      "learning_rate": 8.086180042763284e-05,
      "loss": 7.4035,
      "step": 130
    },
    {
      "epoch": 0.7561327561327561,
      "grad_norm": 1.53125,
      "learning_rate": 7.742868279127849e-05,
      "loss": 7.3424,
      "step": 131
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 1.3671875,
      "learning_rate": 7.405663440815968e-05,
      "loss": 7.416,
      "step": 132
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 1.4296875,
      "learning_rate": 7.074684857319927e-05,
      "loss": 7.3278,
      "step": 133
    },
    {
      "epoch": 0.7734487734487735,
      "grad_norm": 1.3203125,
      "learning_rate": 6.750049654795198e-05,
      "loss": 7.3875,
      "step": 134
    },
    {
      "epoch": 0.7792207792207793,
      "grad_norm": 1.3203125,
      "learning_rate": 6.431872714612072e-05,
      "loss": 7.3234,
      "step": 135
    },
    {
      "epoch": 0.784992784992785,
      "grad_norm": 1.296875,
      "learning_rate": 6.120266632701598e-05,
      "loss": 7.3634,
      "step": 136
    },
    {
      "epoch": 0.7907647907647908,
      "grad_norm": 1.3359375,
      "learning_rate": 5.815341679710326e-05,
      "loss": 7.3782,
      "step": 137
    },
    {
      "epoch": 0.7965367965367965,
      "grad_norm": 1.359375,
      "learning_rate": 5.517205761977939e-05,
      "loss": 7.3929,
      "step": 138
    },
    {
      "epoch": 0.8023088023088023,
      "grad_norm": 1.3203125,
      "learning_rate": 5.225964383351489e-05,
      "loss": 7.3549,
      "step": 139
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.296875,
      "learning_rate": 4.941720607849912e-05,
      "loss": 7.3895,
      "step": 140
    },
    {
      "epoch": 0.8138528138528138,
      "grad_norm": 1.328125,
      "learning_rate": 4.664575023191886e-05,
      "loss": 7.3694,
      "step": 141
    },
    {
      "epoch": 0.8196248196248196,
      "grad_norm": 1.2890625,
      "learning_rate": 4.394625705200012e-05,
      "loss": 7.3656,
      "step": 142
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 1.265625,
      "learning_rate": 4.131968183093912e-05,
      "loss": 7.3274,
      "step": 143
    },
    {
      "epoch": 0.8311688311688312,
      "grad_norm": 1.3671875,
      "learning_rate": 3.876695405684485e-05,
      "loss": 7.3736,
      "step": 144
    },
    {
      "epoch": 0.836940836940837,
      "grad_norm": 1.3125,
      "learning_rate": 3.628897708481377e-05,
      "loss": 7.352,
      "step": 145
    },
    {
      "epoch": 0.8427128427128427,
      "grad_norm": 1.2890625,
      "learning_rate": 3.388662781725141e-05,
      "loss": 7.3178,
      "step": 146
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 1.359375,
      "learning_rate": 3.1560756393556184e-05,
      "loss": 7.2835,
      "step": 147
    },
    {
      "epoch": 0.8542568542568543,
      "grad_norm": 1.3515625,
      "learning_rate": 2.9312185889273145e-05,
      "loss": 7.352,
      "step": 148
    },
    {
      "epoch": 0.86002886002886,
      "grad_norm": 1.2421875,
      "learning_rate": 2.7141712024825378e-05,
      "loss": 7.3817,
      "step": 149
    },
    {
      "epoch": 0.8658008658008658,
      "grad_norm": 1.296875,
      "learning_rate": 2.505010288392587e-05,
      "loss": 7.3627,
      "step": 150
    },
    {
      "epoch": 0.8715728715728716,
      "grad_norm": 1.265625,
      "learning_rate": 2.3038098641769088e-05,
      "loss": 7.3361,
      "step": 151
    },
    {
      "epoch": 0.8773448773448773,
      "grad_norm": 1.2109375,
      "learning_rate": 2.1106411303099453e-05,
      "loss": 7.3344,
      "step": 152
    },
    {
      "epoch": 0.8831168831168831,
      "grad_norm": 1.3203125,
      "learning_rate": 1.9255724450247676e-05,
      "loss": 7.3394,
      "step": 153
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.2578125,
      "learning_rate": 1.7486693001226267e-05,
      "loss": 7.4067,
      "step": 154
    },
    {
      "epoch": 0.8946608946608947,
      "grad_norm": 1.2890625,
      "learning_rate": 1.579994297796808e-05,
      "loss": 7.4078,
      "step": 155
    },
    {
      "epoch": 0.9004329004329005,
      "grad_norm": 1.2109375,
      "learning_rate": 1.4196071284790529e-05,
      "loss": 7.3871,
      "step": 156
    },
    {
      "epoch": 0.9062049062049062,
      "grad_norm": 1.3359375,
      "learning_rate": 1.2675645497164351e-05,
      "loss": 7.389,
      "step": 157
    },
    {
      "epoch": 0.911976911976912,
      "grad_norm": 1.2265625,
      "learning_rate": 1.1239203660860647e-05,
      "loss": 7.3978,
      "step": 158
    },
    {
      "epoch": 0.9177489177489178,
      "grad_norm": 1.3671875,
      "learning_rate": 9.88725410154842e-06,
      "loss": 7.3805,
      "step": 159
    },
    {
      "epoch": 0.9235209235209235,
      "grad_norm": 1.359375,
      "learning_rate": 8.620275244908826e-06,
      "loss": 7.3118,
      "step": 160
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 1.3046875,
      "learning_rate": 7.438715447331018e-06,
      "loss": 7.4196,
      "step": 161
    },
    {
      "epoch": 0.935064935064935,
      "grad_norm": 1.265625,
      "learning_rate": 6.342992837248235e-06,
      "loss": 7.3744,
      "step": 162
    },
    {
      "epoch": 0.9408369408369408,
      "grad_norm": 1.25,
      "learning_rate": 5.333495167171354e-06,
      "loss": 7.3615,
      "step": 163
    },
    {
      "epoch": 0.9466089466089466,
      "grad_norm": 1.234375,
      "learning_rate": 4.410579676471571e-06,
      "loss": 7.4075,
      "step": 164
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 1.2265625,
      "learning_rate": 3.5745729649613034e-06,
      "loss": 7.4288,
      "step": 165
    },
    {
      "epoch": 0.9581529581529582,
      "grad_norm": 1.1796875,
      "learning_rate": 2.8257708773173627e-06,
      "loss": 7.4002,
      "step": 166
    },
    {
      "epoch": 0.963924963924964,
      "grad_norm": 1.3203125,
      "learning_rate": 2.1644383983880357e-06,
      "loss": 7.3354,
      "step": 167
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.2109375,
      "learning_rate": 1.5908095594207582e-06,
      "loss": 7.3282,
      "step": 168
    },
    {
      "epoch": 0.9754689754689755,
      "grad_norm": 1.203125,
      "learning_rate": 1.1050873552433394e-06,
      "loss": 7.3628,
      "step": 169
    },
    {
      "epoch": 0.9812409812409812,
      "grad_norm": 1.234375,
      "learning_rate": 7.074436724286704e-07,
      "loss": 7.32,
      "step": 170
    },
    {
      "epoch": 0.987012987012987,
      "grad_norm": 1.328125,
      "learning_rate": 3.9801922846766094e-07,
      "loss": 7.3693,
      "step": 171
    },
    {
      "epoch": 0.9927849927849928,
      "grad_norm": 1.1015625,
      "learning_rate": 1.7692352197240524e-07,
      "loss": 7.413,
      "step": 172
    },
    {
      "epoch": 0.9985569985569985,
      "grad_norm": 1.2890625,
      "learning_rate": 4.423479392709484e-08,
      "loss": 7.3762,
      "step": 173
    }
  ],
  "logging_steps": 1,
  "max_steps": 173,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6799425955430400.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

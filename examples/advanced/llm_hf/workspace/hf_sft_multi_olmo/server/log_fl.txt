2025-05-12 22:04:26,028 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-12 22:04:26,038 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-12 22:04:29,728 - IntimeModelSelector - INFO - model selection weights control: {}
2025-05-12 22:04:29,803 - ModelQuantizer - INFO - Using model quantizator.
2025-05-12 22:04:29,804 - ModelDequantizer - INFO - Using model dequantizator.
2025-05-12 22:04:29,807 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Initializing BaseModelController workflow.
2025-05-12 22:04:29,807 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Beginning model controller run.
2025-05-12 22:04:29,808 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Start FedAvg.
2025-05-12 22:04:29,808 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - loading initial model from persistor
2025-05-12 22:04:29,808 - PTFileModelPersistor - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Both source_ckpt_file_full_name and ckpt_preload_path are not provided. Using the default model weights initialized on the persistor side.
2025-05-12 22:04:29,809 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Round 0 started.
2025-05-12 22:04:29,809 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sampled clients: ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-12 22:04:29,810 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sending task train to ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-12 22:04:35,426 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Running quantization...
2025-05-12 22:04:35,427 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Running quantization on 179 variables
2025-05-12 22:04:35,436 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running quantization...
2025-05-12 22:04:35,437 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running quantization on 179 variables
2025-05-12 22:04:35,616 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,618 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,619 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,664 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,664 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,665 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,665 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,666 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,670 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,670 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,706 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,707 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,707 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,710 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,713 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,714 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,759 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,760 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,760 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,760 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,761 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,761 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,761 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,761 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,794 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,794 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,795 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,797 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,798 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.4.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,801 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,801 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,845 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,846 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,846 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,847 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,847 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,847 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,848 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,849 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,869 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,884 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,884 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,885 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,886 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,886 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,886 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,887 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,888 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,939 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,940 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,941 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,941 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,941 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,941 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,942 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,942 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,961 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,975 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,976 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,976 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,977 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,977 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,978 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,978 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:35,979 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,026 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,027 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,028 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,029 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,029 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,029 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,030 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,030 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,051 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,066 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,067 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,068 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,068 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,069 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,069 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,069 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,070 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,115 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,115 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,115 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.11.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,116 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.11.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,118 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.11.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,118 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,118 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,155 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,155 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.12.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,163 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,164 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,199 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.12.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,199 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,200 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,200 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,201 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,202 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,202 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,203 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,203 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,237 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,237 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,238 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,238 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,239 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,239 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,240 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,240 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,241 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,274 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,274 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,275 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,275 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,276 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.15.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,276 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.15.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,279 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,279 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,326 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,327 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,328 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:36,432 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-12 22:04:36,433 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-12 22:04:36,489 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Quantized 56/179 params. Before quantization: 5008.25 MB. After quantization: 2176.00 MB with meta: 0.00 MB.
2025-05-12 22:04:36,544 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float16', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float16', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float16', 'model.model.layers.5.self_attn.o_proj.weight': 'float16', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float16', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float16', 'model.model.layers.6.self_attn.o_proj.weight': 'float16', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float16', 'model.model.layers.7.self_attn.v_proj.weight': 'float16', 'model.model.layers.7.self_attn.o_proj.weight': 'float16', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float16', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float16', 'model.model.layers.8.self_attn.k_proj.weight': 'float16', 'model.model.layers.8.self_attn.v_proj.weight': 'float16', 'model.model.layers.8.self_attn.o_proj.weight': 'float16', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float16', 'model.model.layers.9.self_attn.v_proj.weight': 'float16', 'model.model.layers.9.self_attn.o_proj.weight': 'float16', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float16', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float16', 'model.model.layers.10.self_attn.k_proj.weight': 'float16', 'model.model.layers.10.self_attn.v_proj.weight': 'float16', 'model.model.layers.10.self_attn.o_proj.weight': 'float16', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float16', 'model.model.layers.11.self_attn.k_proj.weight': 'float16', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float16', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float16', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float16', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float16', 'model.model.layers.13.self_attn.v_proj.weight': 'float16', 'model.model.layers.13.self_attn.o_proj.weight': 'float16', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float16', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float16', 'model.model.layers.14.self_attn.k_proj.weight': 'float16', 'model.model.layers.14.self_attn.v_proj.weight': 'float16', 'model.model.layers.14.self_attn.o_proj.weight': 'float16', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float16', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float16', 'model.model.layers.15.self_attn.v_proj.weight': 'float16', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-12 22:12:16,239 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Running dequantization...
2025-05-12 22:12:16,239 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Running dequantization on 179 variables
2025-05-12 22:12:20,300 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 22:12:20,303 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9c56a780-d874-47d8-8cf4-0ecd7a65c74b] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 22:12:25,369 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=3d585145-6558-43ec-aaea-3e541fc7c026] - Running quantization...
2025-05-12 22:12:25,369 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=3d585145-6558-43ec-aaea-3e541fc7c026] - Already quantized, skip quantization
2025-05-12 22:17:21,953 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=3d585145-6558-43ec-aaea-3e541fc7c026] - Running dequantization...
2025-05-12 22:17:21,953 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=3d585145-6558-43ec-aaea-3e541fc7c026] - Running dequantization on 179 variables
2025-05-12 22:17:25,927 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=3d585145-6558-43ec-aaea-3e541fc7c026] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 22:17:25,928 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=3d585145-6558-43ec-aaea-3e541fc7c026] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 22:23:06,043 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running dequantization...
2025-05-12 22:23:06,043 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running dequantization on 179 variables
2025-05-12 22:23:10,081 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 22:23:10,082 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 22:23:10,428 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - aggregating 3 update(s) at round 0
2025-05-12 22:23:22,478 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Start persist model on server.
2025-05-12 22:24:20,341 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - End persist model on server.
2025-05-12 22:24:20,343 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Round 1 started.
2025-05-12 22:24:20,344 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Sampled clients: ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-12 22:24:20,345 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Sending task train to ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-12 22:24:22,350 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running quantization...
2025-05-12 22:24:22,351 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running quantization on 179 variables
2025-05-12 22:24:25,145 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Running quantization...
2025-05-12 22:24:25,145 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Running quantization on 179 variables
2025-05-12 22:24:25,146 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.embed_tokens.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,146 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,146 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,146 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,147 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,147 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,147 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,147 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,148 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,148 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,148 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,148 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,149 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,149 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,149 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,151 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,151 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,151 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,151 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,152 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,152 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,152 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,152 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,153 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,153 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,153 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,153 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,154 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,154 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,154 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,154 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,155 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,155 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,155 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,155 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,156 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,156 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,156 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,156 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,157 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,157 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,157 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,157 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,157 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,158 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,158 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,158 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,158 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,159 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,159 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,159 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,159 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,161 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,161 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,161 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,161 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,162 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.6.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,201 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,202 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,437 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,437 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,437 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,438 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.7.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,485 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,486 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,723 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,724 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,748 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.8.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,797 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:25,797 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,080 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,081 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,082 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,083 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,319 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,319 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,391 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.10.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,392 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,392 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,630 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,631 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,655 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.11.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,703 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,704 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,940 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,940 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:26,941 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.12.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,012 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,012 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,247 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,248 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,248 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,296 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.13.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,296 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,297 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,585 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.14.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,611 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,611 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,851 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,852 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,852 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,919 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:27,919 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:28,159 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:28,159 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:28,160 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:29,091 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-12 22:24:29,092 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-12 22:24:29,148 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Quantized 54/179 params. Before quantization: 4368.25 MB. After quantization: 1536.00 MB with meta: 0.00 MB.
2025-05-12 22:24:29,201 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Quantized from {'model.model.embed_tokens.weight': 'float16', 'model.model.layers.0.self_attn.q_proj.weight': 'float16', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float16', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float16', 'model.model.layers.0.mlp.up_proj.weight': 'float16', 'model.model.layers.0.mlp.down_proj.weight': 'float16', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float16', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float16', 'model.model.layers.1.mlp.up_proj.weight': 'float16', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float16', 'model.model.layers.2.self_attn.o_proj.weight': 'float16', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float16', 'model.model.layers.2.mlp.up_proj.weight': 'float16', 'model.model.layers.2.mlp.down_proj.weight': 'float16', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float16', 'model.model.layers.3.mlp.up_proj.weight': 'float16', 'model.model.layers.3.mlp.down_proj.weight': 'float16', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float16', 'model.model.layers.4.self_attn.o_proj.weight': 'float16', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float16', 'model.model.layers.4.mlp.up_proj.weight': 'float16', 'model.model.layers.4.mlp.down_proj.weight': 'float16', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float16', 'model.model.layers.5.self_attn.o_proj.weight': 'float16', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float16', 'model.model.layers.5.mlp.up_proj.weight': 'float16', 'model.model.layers.5.mlp.down_proj.weight': 'float16', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float16', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float16', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float16', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float16', 'model.model.layers.9.self_attn.v_proj.weight': 'float16', 'model.model.layers.9.self_attn.o_proj.weight': 'float16', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float16', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float16', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float16', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float16', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float16', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-12 22:29:45,502 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Running dequantization...
2025-05-12 22:29:45,502 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Running dequantization on 179 variables
2025-05-12 22:29:49,410 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 22:29:49,411 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 22:29:49,412 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=c1f36130-f96b-4db2-9731-589b8becbde1] - validation metric -2.753997802734375 from client site-oasst1
2025-05-12 22:29:54,504 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=853b5a78-5fe3-4fd9-861d-db666bfaa029] - Running quantization...
2025-05-12 22:29:54,505 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=853b5a78-5fe3-4fd9-861d-db666bfaa029] - Already quantized, skip quantization
2025-05-12 22:37:49,995 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=853b5a78-5fe3-4fd9-861d-db666bfaa029] - Running dequantization...
2025-05-12 22:37:49,995 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=853b5a78-5fe3-4fd9-861d-db666bfaa029] - Running dequantization on 179 variables
2025-05-12 22:37:53,909 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=853b5a78-5fe3-4fd9-861d-db666bfaa029] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 22:37:53,911 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=853b5a78-5fe3-4fd9-861d-db666bfaa029] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 22:37:53,912 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=853b5a78-5fe3-4fd9-861d-db666bfaa029] - validation metric -3.0295259952545166 from client site-dolly
2025-05-12 22:43:06,013 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running dequantization...
2025-05-12 22:43:06,014 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running dequantization on 179 variables
2025-05-12 22:43:09,953 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 22:43:09,954 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 22:43:09,955 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - validation metric -1.7901140451431274 from client site-alpaca
2025-05-12 22:43:10,403 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - new best validation metric at round 1: -2.524545947710673
2025-05-12 22:43:39,474 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - aggregating 3 update(s) at round 1
2025-05-12 22:43:48,983 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Start persist model on server.
2025-05-12 22:45:02,667 - SimulatorClientRunner - ERROR - run_client_thread error: EOFError: 
2025-05-12 22:45:02,669 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - End persist model on server.
2025-05-12 22:45:02,672 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Round 2 started.
2025-05-12 22:45:02,674 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Sampled clients: ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-12 22:45:02,675 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Sending task train to ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-12 22:45:03,178 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running quantization...
2025-05-12 22:45:03,178 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running quantization on 179 variables
2025-05-12 22:45:11,253 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-12 22:45:11,254 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-12 23:03:45,560 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running dequantization...
2025-05-12 23:03:45,561 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running dequantization on 179 variables
2025-05-12 23:03:49,543 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 23:03:49,544 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 23:03:49,545 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - validation metric -1.7826472520828247 from client site-alpaca
2025-05-13 17:57:44,593 - SimulatorClientRunner - ERROR - run_client_thread error: EOFError: 
2025-05-13 17:57:44,764 - FedAvg - WARNING - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Number of results (1) is different from number of expected responses (3).
2025-05-13 17:57:44,798 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - new best validation metric at round 2: -1.7826472520828247
2025-05-13 17:58:22,293 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - aggregating 1 update(s) at round 2
2025-05-13 17:58:26,895 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Start persist model on server.
2025-05-13 17:59:21,956 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - End persist model on server.
2025-05-13 17:59:21,957 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Finished FedAvg.

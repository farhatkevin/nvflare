2025-05-12 22:04:34,918 - ModelDequantizer - INFO - Using model dequantizator.
2025-05-12 22:04:34,919 - ModelQuantizer - INFO - Using model quantizator.
2025-05-12 22:04:35,429 - TaskScriptRunner - INFO - start task run() with full path: /workspace/NVFlare/examples/advanced/llm_hf/workspace/hf_sft_multi_olmo/site-alpaca/simulate_job/app_site-alpaca/custom/src/hf_sft_peft_fl.py
2025-05-12 22:04:37,737 - TaskScriptRunner - INFO - Dataset size: training 41602, validation 5200
2025-05-12 22:04:37,738 - TaskScriptRunner - INFO - logging_steps: 52
2025-05-12 22:04:39,871 - TaskScriptRunner - INFO -  38           0 BUILD_LIST               0
2025-05-12 22:04:39,872 - TaskScriptRunner - INFO -               2 STORE_FAST               1 (output_texts)
2025-05-12 22:04:39,872 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,872 - TaskScriptRunner - INFO -  39           4 LOAD_GLOBAL              0 (range)
2025-05-12 22:04:39,872 - TaskScriptRunner - INFO -               6 LOAD_GLOBAL              1 (len)
2025-05-12 22:04:39,872 - TaskScriptRunner - INFO -               8 LOAD_FAST                0 (example)
2025-05-12 22:04:39,873 - TaskScriptRunner - INFO -              10 LOAD_CONST               1 ('input')
2025-05-12 22:04:39,873 - TaskScriptRunner - INFO -              12 BINARY_SUBSCR
2025-05-12 22:04:39,873 - TaskScriptRunner - INFO -              14 CALL_FUNCTION            1
2025-05-12 22:04:39,873 - TaskScriptRunner - INFO -              16 CALL_FUNCTION            1
2025-05-12 22:04:39,873 - TaskScriptRunner - INFO -              18 GET_ITER
2025-05-12 22:04:39,873 - TaskScriptRunner - INFO -         >>   20 FOR_ITER                23 (to 68)
2025-05-12 22:04:39,874 - TaskScriptRunner - INFO -              22 STORE_FAST               2 (i)
2025-05-12 22:04:39,874 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,874 - TaskScriptRunner - INFO -  40          24 LOAD_CONST               2 ('### Instruction: Generate Output according to the information and question given by Input. ### Input:')
2025-05-12 22:04:39,874 - TaskScriptRunner - INFO -              26 LOAD_FAST                0 (example)
2025-05-12 22:04:39,874 - TaskScriptRunner - INFO -              28 LOAD_CONST               1 ('input')
2025-05-12 22:04:39,874 - TaskScriptRunner - INFO -              30 BINARY_SUBSCR
2025-05-12 22:04:39,875 - TaskScriptRunner - INFO -              32 LOAD_FAST                2 (i)
2025-05-12 22:04:39,875 - TaskScriptRunner - INFO -              34 BINARY_SUBSCR
2025-05-12 22:04:39,875 - TaskScriptRunner - INFO -              36 FORMAT_VALUE             0
2025-05-12 22:04:39,875 - TaskScriptRunner - INFO -              38 LOAD_CONST               3 (' ### Response: ')
2025-05-12 22:04:39,875 - TaskScriptRunner - INFO -              40 LOAD_FAST                0 (example)
2025-05-12 22:04:39,875 - TaskScriptRunner - INFO -              42 LOAD_CONST               4 ('output')
2025-05-12 22:04:39,876 - TaskScriptRunner - INFO -              44 BINARY_SUBSCR
2025-05-12 22:04:39,876 - TaskScriptRunner - INFO -              46 LOAD_FAST                2 (i)
2025-05-12 22:04:39,876 - TaskScriptRunner - INFO -              48 BINARY_SUBSCR
2025-05-12 22:04:39,876 - TaskScriptRunner - INFO -              50 FORMAT_VALUE             0
2025-05-12 22:04:39,876 - TaskScriptRunner - INFO -              52 BUILD_STRING             4
2025-05-12 22:04:39,876 - TaskScriptRunner - INFO -              54 STORE_FAST               3 (text)
2025-05-12 22:04:39,877 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,877 - TaskScriptRunner - INFO -  41          56 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:39,877 - TaskScriptRunner - INFO -              58 LOAD_METHOD              2 (append)
2025-05-12 22:04:39,877 - TaskScriptRunner - INFO -              60 LOAD_FAST                3 (text)
2025-05-12 22:04:39,877 - TaskScriptRunner - INFO -              62 CALL_METHOD              1
2025-05-12 22:04:39,877 - TaskScriptRunner - INFO -              64 POP_TOP
2025-05-12 22:04:39,878 - TaskScriptRunner - INFO -              66 JUMP_ABSOLUTE           10 (to 20)
2025-05-12 22:04:39,878 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,878 - TaskScriptRunner - INFO -  42     >>   68 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:39,878 - TaskScriptRunner - INFO -              70 RETURN_VALUE
2025-05-12 22:04:39,878 - TaskScriptRunner - INFO - 415           0 LOAD_DEREF               4 (processing_class)
2025-05-12 22:04:39,879 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,879 - TaskScriptRunner - INFO - 416           2 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,879 - TaskScriptRunner - INFO -               4 LOAD_CONST               0 (None)
2025-05-12 22:04:39,879 - TaskScriptRunner - INFO -               6 IS_OP                    0
2025-05-12 22:04:39,879 - TaskScriptRunner - INFO -               8 POP_JUMP_IF_FALSE        9 (to 18)
2025-05-12 22:04:39,879 - TaskScriptRunner - INFO -              10 LOAD_FAST                0 (element)
2025-05-12 22:04:39,880 - TaskScriptRunner - INFO -              12 LOAD_DEREF               1 (dataset_text_field)
2025-05-12 22:04:39,880 - TaskScriptRunner - INFO -              14 BINARY_SUBSCR
2025-05-12 22:04:39,880 - TaskScriptRunner - INFO -              16 JUMP_FORWARD             3 (to 24)
2025-05-12 22:04:39,880 - TaskScriptRunner - INFO -         >>   18 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,880 - TaskScriptRunner - INFO -              20 LOAD_FAST                0 (element)
2025-05-12 22:04:39,880 - TaskScriptRunner - INFO -              22 CALL_FUNCTION            1
2025-05-12 22:04:39,881 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,881 - TaskScriptRunner - INFO - 417     >>   24 LOAD_DEREF               0 (add_special_tokens)
2025-05-12 22:04:39,881 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,881 - TaskScriptRunner - INFO - 418          26 LOAD_CONST               1 (True)
2025-05-12 22:04:39,881 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,881 - TaskScriptRunner - INFO - 419          28 LOAD_CONST               2 (False)
2025-05-12 22:04:39,882 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,882 - TaskScriptRunner - INFO - 420          30 LOAD_DEREF               3 (max_seq_length)
2025-05-12 22:04:39,882 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,882 - TaskScriptRunner - INFO - 421          32 LOAD_CONST               2 (False)
2025-05-12 22:04:39,882 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,882 - TaskScriptRunner - INFO - 422          34 LOAD_CONST               2 (False)
2025-05-12 22:04:39,883 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,883 - TaskScriptRunner - INFO - 415          36 LOAD_CONST               3 (('add_special_tokens', 'truncation', 'padding', 'max_length', 'return_overflowing_tokens', 'return_length'))
2025-05-12 22:04:39,883 - TaskScriptRunner - INFO -              38 CALL_FUNCTION_KW         7
2025-05-12 22:04:39,883 - TaskScriptRunner - INFO -              40 STORE_FAST               1 (outputs)
2025-05-12 22:04:39,883 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,883 - TaskScriptRunner - INFO - 425          42 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,884 - TaskScriptRunner - INFO -              44 LOAD_CONST               0 (None)
2025-05-12 22:04:39,884 - TaskScriptRunner - INFO -              46 IS_OP                    1
2025-05-12 22:04:39,884 - TaskScriptRunner - INFO -              48 POP_JUMP_IF_FALSE       36 (to 72)
2025-05-12 22:04:39,884 - TaskScriptRunner - INFO -              50 LOAD_GLOBAL              0 (isinstance)
2025-05-12 22:04:39,884 - TaskScriptRunner - INFO -              52 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,884 - TaskScriptRunner - INFO -              54 LOAD_FAST                0 (element)
2025-05-12 22:04:39,885 - TaskScriptRunner - INFO -              56 CALL_FUNCTION            1
2025-05-12 22:04:39,885 - TaskScriptRunner - INFO -              58 LOAD_GLOBAL              1 (list)
2025-05-12 22:04:39,885 - TaskScriptRunner - INFO -              60 CALL_FUNCTION            2
2025-05-12 22:04:39,885 - TaskScriptRunner - INFO -              62 POP_JUMP_IF_TRUE        36 (to 72)
2025-05-12 22:04:39,885 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,885 - TaskScriptRunner - INFO - 426          64 LOAD_GLOBAL              2 (ValueError)
2025-05-12 22:04:39,886 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,886 - TaskScriptRunner - INFO - 427          66 LOAD_CONST               4 ('The `formatting_func` should return a list of processed strings since it can lead to silent bugs.')
2025-05-12 22:04:39,886 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,886 - TaskScriptRunner - INFO - 426          68 CALL_FUNCTION            1
2025-05-12 22:04:39,886 - TaskScriptRunner - INFO -              70 RAISE_VARARGS            1
2025-05-12 22:04:39,886 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,887 - TaskScriptRunner - INFO - 430     >>   72 LOAD_FAST                1 (outputs)
2025-05-12 22:04:39,887 - TaskScriptRunner - INFO -              74 LOAD_CONST               5 ('input_ids')
2025-05-12 22:04:39,887 - TaskScriptRunner - INFO -              76 BINARY_SUBSCR
2025-05-12 22:04:39,887 - TaskScriptRunner - INFO -              78 LOAD_FAST                1 (outputs)
2025-05-12 22:04:39,887 - TaskScriptRunner - INFO -              80 LOAD_CONST               6 ('attention_mask')
2025-05-12 22:04:39,887 - TaskScriptRunner - INFO -              82 BINARY_SUBSCR
2025-05-12 22:04:39,888 - TaskScriptRunner - INFO -              84 LOAD_CONST               7 (('input_ids', 'attention_mask'))
2025-05-12 22:04:39,888 - TaskScriptRunner - INFO -              86 BUILD_CONST_KEY_MAP      2
2025-05-12 22:04:39,888 - TaskScriptRunner - INFO -              88 RETURN_VALUE
2025-05-12 22:04:39,931 - TaskScriptRunner - INFO -  38           0 BUILD_LIST               0
2025-05-12 22:04:39,932 - TaskScriptRunner - INFO -               2 STORE_FAST               1 (output_texts)
2025-05-12 22:04:39,932 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,932 - TaskScriptRunner - INFO -  39           4 LOAD_GLOBAL              0 (range)
2025-05-12 22:04:39,932 - TaskScriptRunner - INFO -               6 LOAD_GLOBAL              1 (len)
2025-05-12 22:04:39,932 - TaskScriptRunner - INFO -               8 LOAD_FAST                0 (example)
2025-05-12 22:04:39,933 - TaskScriptRunner - INFO -              10 LOAD_CONST               1 ('input')
2025-05-12 22:04:39,933 - TaskScriptRunner - INFO -              12 BINARY_SUBSCR
2025-05-12 22:04:39,933 - TaskScriptRunner - INFO -              14 CALL_FUNCTION            1
2025-05-12 22:04:39,933 - TaskScriptRunner - INFO -              16 CALL_FUNCTION            1
2025-05-12 22:04:39,933 - TaskScriptRunner - INFO -              18 GET_ITER
2025-05-12 22:04:39,933 - TaskScriptRunner - INFO -         >>   20 FOR_ITER                23 (to 68)
2025-05-12 22:04:39,934 - TaskScriptRunner - INFO -              22 STORE_FAST               2 (i)
2025-05-12 22:04:39,934 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,934 - TaskScriptRunner - INFO -  40          24 LOAD_CONST               2 ('### Instruction: Generate Output according to the information and question given by Input. ### Input:')
2025-05-12 22:04:39,934 - TaskScriptRunner - INFO -              26 LOAD_FAST                0 (example)
2025-05-12 22:04:39,934 - TaskScriptRunner - INFO -              28 LOAD_CONST               1 ('input')
2025-05-12 22:04:39,934 - TaskScriptRunner - INFO -              30 BINARY_SUBSCR
2025-05-12 22:04:39,935 - TaskScriptRunner - INFO -              32 LOAD_FAST                2 (i)
2025-05-12 22:04:39,935 - TaskScriptRunner - INFO -              34 BINARY_SUBSCR
2025-05-12 22:04:39,935 - TaskScriptRunner - INFO -              36 FORMAT_VALUE             0
2025-05-12 22:04:39,935 - TaskScriptRunner - INFO -              38 LOAD_CONST               3 (' ### Response: ')
2025-05-12 22:04:39,935 - TaskScriptRunner - INFO -              40 LOAD_FAST                0 (example)
2025-05-12 22:04:39,935 - TaskScriptRunner - INFO -              42 LOAD_CONST               4 ('output')
2025-05-12 22:04:39,936 - TaskScriptRunner - INFO -              44 BINARY_SUBSCR
2025-05-12 22:04:39,936 - TaskScriptRunner - INFO -              46 LOAD_FAST                2 (i)
2025-05-12 22:04:39,936 - TaskScriptRunner - INFO -              48 BINARY_SUBSCR
2025-05-12 22:04:39,936 - TaskScriptRunner - INFO -              50 FORMAT_VALUE             0
2025-05-12 22:04:39,936 - TaskScriptRunner - INFO -              52 BUILD_STRING             4
2025-05-12 22:04:39,936 - TaskScriptRunner - INFO -              54 STORE_FAST               3 (text)
2025-05-12 22:04:39,937 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,937 - TaskScriptRunner - INFO -  41          56 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:39,937 - TaskScriptRunner - INFO -              58 LOAD_METHOD              2 (append)
2025-05-12 22:04:39,937 - TaskScriptRunner - INFO -              60 LOAD_FAST                3 (text)
2025-05-12 22:04:39,937 - TaskScriptRunner - INFO -              62 CALL_METHOD              1
2025-05-12 22:04:39,937 - TaskScriptRunner - INFO -              64 POP_TOP
2025-05-12 22:04:39,938 - TaskScriptRunner - INFO -              66 JUMP_ABSOLUTE           10 (to 20)
2025-05-12 22:04:39,938 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,938 - TaskScriptRunner - INFO -  42     >>   68 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:39,938 - TaskScriptRunner - INFO -              70 RETURN_VALUE
2025-05-12 22:04:39,941 - TaskScriptRunner - INFO -  38           0 BUILD_LIST               0
2025-05-12 22:04:39,941 - TaskScriptRunner - INFO -               2 STORE_FAST               1 (output_texts)
2025-05-12 22:04:39,941 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,941 - TaskScriptRunner - INFO -  39           4 LOAD_GLOBAL              0 (range)
2025-05-12 22:04:39,942 - TaskScriptRunner - INFO -               6 LOAD_GLOBAL              1 (len)
2025-05-12 22:04:39,942 - TaskScriptRunner - INFO -               8 LOAD_FAST                0 (example)
2025-05-12 22:04:39,942 - TaskScriptRunner - INFO -              10 LOAD_CONST               1 ('input')
2025-05-12 22:04:39,942 - TaskScriptRunner - INFO -              12 BINARY_SUBSCR
2025-05-12 22:04:39,942 - TaskScriptRunner - INFO -              14 CALL_FUNCTION            1
2025-05-12 22:04:39,942 - TaskScriptRunner - INFO -              16 CALL_FUNCTION            1
2025-05-12 22:04:39,943 - TaskScriptRunner - INFO -              18 GET_ITER
2025-05-12 22:04:39,943 - TaskScriptRunner - INFO -         >>   20 FOR_ITER                23 (to 68)
2025-05-12 22:04:39,943 - TaskScriptRunner - INFO -              22 STORE_FAST               2 (i)
2025-05-12 22:04:39,943 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,943 - TaskScriptRunner - INFO -  40          24 LOAD_CONST               2 ('### Instruction: Generate Output according to the information and question given by Input. ### Input:')
2025-05-12 22:04:39,943 - TaskScriptRunner - INFO -              26 LOAD_FAST                0 (example)
2025-05-12 22:04:39,944 - TaskScriptRunner - INFO -              28 LOAD_CONST               1 ('input')
2025-05-12 22:04:39,944 - TaskScriptRunner - INFO -              30 BINARY_SUBSCR
2025-05-12 22:04:39,944 - TaskScriptRunner - INFO -              32 LOAD_FAST                2 (i)
2025-05-12 22:04:39,944 - TaskScriptRunner - INFO -              34 BINARY_SUBSCR
2025-05-12 22:04:39,944 - TaskScriptRunner - INFO -              36 FORMAT_VALUE             0
2025-05-12 22:04:39,944 - TaskScriptRunner - INFO -              38 LOAD_CONST               3 (' ### Response: ')
2025-05-12 22:04:39,945 - TaskScriptRunner - INFO -              40 LOAD_FAST                0 (example)
2025-05-12 22:04:39,945 - TaskScriptRunner - INFO -              42 LOAD_CONST               4 ('output')
2025-05-12 22:04:39,945 - TaskScriptRunner - INFO -              44 BINARY_SUBSCR
2025-05-12 22:04:39,945 - TaskScriptRunner - INFO -              46 LOAD_FAST                2 (i)
2025-05-12 22:04:39,945 - TaskScriptRunner - INFO -              48 BINARY_SUBSCR
2025-05-12 22:04:39,945 - TaskScriptRunner - INFO -              50 FORMAT_VALUE             0
2025-05-12 22:04:39,946 - TaskScriptRunner - INFO -              52 BUILD_STRING             4
2025-05-12 22:04:39,946 - TaskScriptRunner - INFO -              54 STORE_FAST               3 (text)
2025-05-12 22:04:39,946 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,946 - TaskScriptRunner - INFO -  41          56 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:39,946 - TaskScriptRunner - INFO -              58 LOAD_METHOD              2 (append)
2025-05-12 22:04:39,946 - TaskScriptRunner - INFO -              60 LOAD_FAST                3 (text)
2025-05-12 22:04:39,947 - TaskScriptRunner - INFO -              62 CALL_METHOD              1
2025-05-12 22:04:39,947 - TaskScriptRunner - INFO -              64 POP_TOP
2025-05-12 22:04:39,947 - TaskScriptRunner - INFO -              66 JUMP_ABSOLUTE           10 (to 20)
2025-05-12 22:04:39,947 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,947 - TaskScriptRunner - INFO -  42     >>   68 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:39,947 - TaskScriptRunner - INFO -              70 RETURN_VALUE
2025-05-12 22:04:39,948 - TaskScriptRunner - INFO - 415           0 LOAD_DEREF               4 (processing_class)
2025-05-12 22:04:39,948 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,948 - TaskScriptRunner - INFO - 416           2 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,948 - TaskScriptRunner - INFO -               4 LOAD_CONST               0 (None)
2025-05-12 22:04:39,948 - TaskScriptRunner - INFO -               6 IS_OP                    0
2025-05-12 22:04:39,949 - TaskScriptRunner - INFO -               8 POP_JUMP_IF_FALSE        9 (to 18)
2025-05-12 22:04:39,949 - TaskScriptRunner - INFO -              10 LOAD_FAST                0 (element)
2025-05-12 22:04:39,949 - TaskScriptRunner - INFO -              12 LOAD_DEREF               1 (dataset_text_field)
2025-05-12 22:04:39,949 - TaskScriptRunner - INFO -              14 BINARY_SUBSCR
2025-05-12 22:04:39,949 - TaskScriptRunner - INFO -              16 JUMP_FORWARD             3 (to 24)
2025-05-12 22:04:39,949 - TaskScriptRunner - INFO -         >>   18 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,949 - TaskScriptRunner - INFO -              20 LOAD_FAST                0 (element)
2025-05-12 22:04:39,950 - TaskScriptRunner - INFO -              22 CALL_FUNCTION            1
2025-05-12 22:04:39,950 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,950 - TaskScriptRunner - INFO - 417     >>   24 LOAD_DEREF               0 (add_special_tokens)
2025-05-12 22:04:39,950 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,950 - TaskScriptRunner - INFO - 418          26 LOAD_CONST               1 (True)
2025-05-12 22:04:39,950 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,951 - TaskScriptRunner - INFO - 419          28 LOAD_CONST               2 (False)
2025-05-12 22:04:39,951 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,951 - TaskScriptRunner - INFO - 420          30 LOAD_DEREF               3 (max_seq_length)
2025-05-12 22:04:39,951 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,951 - TaskScriptRunner - INFO - 421          32 LOAD_CONST               2 (False)
2025-05-12 22:04:39,951 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,952 - TaskScriptRunner - INFO - 422          34 LOAD_CONST               2 (False)
2025-05-12 22:04:39,952 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,952 - TaskScriptRunner - INFO - 415          36 LOAD_CONST               3 (('add_special_tokens', 'truncation', 'padding', 'max_length', 'return_overflowing_tokens', 'return_length'))
2025-05-12 22:04:39,952 - TaskScriptRunner - INFO -              38 CALL_FUNCTION_KW         7
2025-05-12 22:04:39,952 - TaskScriptRunner - INFO -              40 STORE_FAST               1 (outputs)
2025-05-12 22:04:39,952 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,953 - TaskScriptRunner - INFO - 425          42 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,953 - TaskScriptRunner - INFO -              44 LOAD_CONST               0 (None)
2025-05-12 22:04:39,953 - TaskScriptRunner - INFO -              46 IS_OP                    1
2025-05-12 22:04:39,953 - TaskScriptRunner - INFO -              48 POP_JUMP_IF_FALSE       36 (to 72)
2025-05-12 22:04:39,953 - TaskScriptRunner - INFO -              50 LOAD_GLOBAL              0 (isinstance)
2025-05-12 22:04:39,953 - TaskScriptRunner - INFO -              52 LOAD_DEREF               2 (formatting_func)
2025-05-12 22:04:39,954 - TaskScriptRunner - INFO -              54 LOAD_FAST                0 (element)
2025-05-12 22:04:39,954 - TaskScriptRunner - INFO -              56 CALL_FUNCTION            1
2025-05-12 22:04:39,954 - TaskScriptRunner - INFO -              58 LOAD_GLOBAL              1 (list)
2025-05-12 22:04:39,954 - TaskScriptRunner - INFO -              60 CALL_FUNCTION            2
2025-05-12 22:04:39,954 - TaskScriptRunner - INFO -              62 POP_JUMP_IF_TRUE        36 (to 72)
2025-05-12 22:04:39,954 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,954 - TaskScriptRunner - INFO - 426          64 LOAD_GLOBAL              2 (ValueError)
2025-05-12 22:04:39,955 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,955 - TaskScriptRunner - INFO - 427          66 LOAD_CONST               4 ('The `formatting_func` should return a list of processed strings since it can lead to silent bugs.')
2025-05-12 22:04:39,955 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,955 - TaskScriptRunner - INFO - 426          68 CALL_FUNCTION            1
2025-05-12 22:04:39,955 - TaskScriptRunner - INFO -              70 RAISE_VARARGS            1
2025-05-12 22:04:39,955 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,956 - TaskScriptRunner - INFO - 430     >>   72 LOAD_FAST                1 (outputs)
2025-05-12 22:04:39,956 - TaskScriptRunner - INFO -              74 LOAD_CONST               5 ('input_ids')
2025-05-12 22:04:39,956 - TaskScriptRunner - INFO -              76 BINARY_SUBSCR
2025-05-12 22:04:39,956 - TaskScriptRunner - INFO -              78 LOAD_FAST                1 (outputs)
2025-05-12 22:04:39,956 - TaskScriptRunner - INFO -              80 LOAD_CONST               6 ('attention_mask')
2025-05-12 22:04:39,956 - TaskScriptRunner - INFO -              82 BINARY_SUBSCR
2025-05-12 22:04:39,957 - TaskScriptRunner - INFO -              84 LOAD_CONST               7 (('input_ids', 'attention_mask'))
2025-05-12 22:04:39,957 - TaskScriptRunner - INFO -              86 BUILD_CONST_KEY_MAP      2
2025-05-12 22:04:39,957 - TaskScriptRunner - INFO -              88 RETURN_VALUE
2025-05-12 22:04:39,998 - TaskScriptRunner - INFO -  38           0 BUILD_LIST               0
2025-05-12 22:04:39,999 - TaskScriptRunner - INFO -               2 STORE_FAST               1 (output_texts)
2025-05-12 22:04:39,999 - TaskScriptRunner - INFO - 
2025-05-12 22:04:39,999 - TaskScriptRunner - INFO -  39           4 LOAD_GLOBAL              0 (range)
2025-05-12 22:04:39,999 - TaskScriptRunner - INFO -               6 LOAD_GLOBAL              1 (len)
2025-05-12 22:04:39,999 - TaskScriptRunner - INFO -               8 LOAD_FAST                0 (example)
2025-05-12 22:04:39,999 - TaskScriptRunner - INFO -              10 LOAD_CONST               1 ('input')
2025-05-12 22:04:40,000 - TaskScriptRunner - INFO -              12 BINARY_SUBSCR
2025-05-12 22:04:40,000 - TaskScriptRunner - INFO -              14 CALL_FUNCTION            1
2025-05-12 22:04:40,000 - TaskScriptRunner - INFO -              16 CALL_FUNCTION            1
2025-05-12 22:04:40,000 - TaskScriptRunner - INFO -              18 GET_ITER
2025-05-12 22:04:40,000 - TaskScriptRunner - INFO -         >>   20 FOR_ITER                23 (to 68)
2025-05-12 22:04:40,000 - TaskScriptRunner - INFO -              22 STORE_FAST               2 (i)
2025-05-12 22:04:40,001 - TaskScriptRunner - INFO - 
2025-05-12 22:04:40,001 - TaskScriptRunner - INFO -  40          24 LOAD_CONST               2 ('### Instruction: Generate Output according to the information and question given by Input. ### Input:')
2025-05-12 22:04:40,001 - TaskScriptRunner - INFO -              26 LOAD_FAST                0 (example)
2025-05-12 22:04:40,001 - TaskScriptRunner - INFO -              28 LOAD_CONST               1 ('input')
2025-05-12 22:04:40,001 - TaskScriptRunner - INFO -              30 BINARY_SUBSCR
2025-05-12 22:04:40,001 - TaskScriptRunner - INFO -              32 LOAD_FAST                2 (i)
2025-05-12 22:04:40,002 - TaskScriptRunner - INFO -              34 BINARY_SUBSCR
2025-05-12 22:04:40,002 - TaskScriptRunner - INFO -              36 FORMAT_VALUE             0
2025-05-12 22:04:40,002 - TaskScriptRunner - INFO -              38 LOAD_CONST               3 (' ### Response: ')
2025-05-12 22:04:40,002 - TaskScriptRunner - INFO -              40 LOAD_FAST                0 (example)
2025-05-12 22:04:40,002 - TaskScriptRunner - INFO -              42 LOAD_CONST               4 ('output')
2025-05-12 22:04:40,002 - TaskScriptRunner - INFO -              44 BINARY_SUBSCR
2025-05-12 22:04:40,003 - TaskScriptRunner - INFO -              46 LOAD_FAST                2 (i)
2025-05-12 22:04:40,003 - TaskScriptRunner - INFO -              48 BINARY_SUBSCR
2025-05-12 22:04:40,003 - TaskScriptRunner - INFO -              50 FORMAT_VALUE             0
2025-05-12 22:04:40,003 - TaskScriptRunner - INFO -              52 BUILD_STRING             4
2025-05-12 22:04:40,003 - TaskScriptRunner - INFO -              54 STORE_FAST               3 (text)
2025-05-12 22:04:40,003 - TaskScriptRunner - INFO - 
2025-05-12 22:04:40,004 - TaskScriptRunner - INFO -  41          56 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:40,004 - TaskScriptRunner - INFO -              58 LOAD_METHOD              2 (append)
2025-05-12 22:04:40,004 - TaskScriptRunner - INFO -              60 LOAD_FAST                3 (text)
2025-05-12 22:04:40,004 - TaskScriptRunner - INFO -              62 CALL_METHOD              1
2025-05-12 22:04:40,004 - TaskScriptRunner - INFO -              64 POP_TOP
2025-05-12 22:04:40,004 - TaskScriptRunner - INFO -              66 JUMP_ABSOLUTE           10 (to 20)
2025-05-12 22:04:40,005 - TaskScriptRunner - INFO - 
2025-05-12 22:04:40,005 - TaskScriptRunner - INFO -  42     >>   68 LOAD_FAST                1 (output_texts)
2025-05-12 22:04:40,005 - TaskScriptRunner - INFO -              70 RETURN_VALUE
2025-05-12 22:04:47,887 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running dequantization...
2025-05-12 22:04:47,888 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running dequantization on 179 variables
2025-05-12 22:04:47,928 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,929 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,930 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,940 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,941 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,941 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,942 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,943 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,945 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,946 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,952 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,953 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,954 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,956 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,958 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,959 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,968 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,969 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,970 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,970 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,971 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,971 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,972 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,973 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,980 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,981 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,982 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,984 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,985 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.4.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,987 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,988 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,997 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,998 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,998 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:47,999 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,000 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,001 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,002 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,003 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,007 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,010 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,011 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,012 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,013 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,014 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,015 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,016 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,016 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,028 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,029 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,030 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,031 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,031 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,032 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,033 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,034 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,038 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,041 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,042 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,043 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,044 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,045 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,046 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,047 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,047 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,057 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,058 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,059 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,059 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,059 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,060 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,061 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,062 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,066 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,070 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,071 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,072 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,072 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,073 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,074 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,075 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,076 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,085 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,086 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,087 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.11.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,088 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.11.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,090 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.11.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,091 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,091 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,100 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,101 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,105 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.12.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,107 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,108 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,114 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.12.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,115 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,116 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,117 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,118 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,119 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,120 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,122 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,122 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,129 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,130 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,130 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,131 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,131 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,132 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,132 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,133 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,134 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,140 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,141 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,142 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,143 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,144 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.15.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,144 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.15.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,146 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,147 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,155 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,156 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,158 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Skipping dequantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:04:48,190 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Dequantized 56/179 params. Before dequantization: 2176.00 MB with meta: 0.00 MB. After dequantization: 4352.00 MB.
2025-05-12 22:04:48,193 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float16', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float16', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float16', 'model.model.layers.5.self_attn.o_proj.weight': 'float16', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float16', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float16', 'model.model.layers.6.self_attn.o_proj.weight': 'float16', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float16', 'model.model.layers.7.self_attn.v_proj.weight': 'float16', 'model.model.layers.7.self_attn.o_proj.weight': 'float16', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float16', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float16', 'model.model.layers.8.self_attn.k_proj.weight': 'float16', 'model.model.layers.8.self_attn.v_proj.weight': 'float16', 'model.model.layers.8.self_attn.o_proj.weight': 'float16', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float16', 'model.model.layers.9.self_attn.v_proj.weight': 'float16', 'model.model.layers.9.self_attn.o_proj.weight': 'float16', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float16', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float16', 'model.model.layers.10.self_attn.k_proj.weight': 'float16', 'model.model.layers.10.self_attn.v_proj.weight': 'float16', 'model.model.layers.10.self_attn.o_proj.weight': 'float16', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float16', 'model.model.layers.11.self_attn.k_proj.weight': 'float16', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float16', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float16', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float16', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float16', 'model.model.layers.13.self_attn.v_proj.weight': 'float16', 'model.model.layers.13.self_attn.o_proj.weight': 'float16', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float16', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float16', 'model.model.layers.14.self_attn.k_proj.weight': 'float16', 'model.model.layers.14.self_attn.v_proj.weight': 'float16', 'model.model.layers.14.self_attn.o_proj.weight': 'float16', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float16', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float16', 'model.model.layers.15.self_attn.v_proj.weight': 'float16', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'}
2025-05-12 22:04:48,195 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - execute for task (train)
2025-05-12 22:04:48,197 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - send data to peer
2025-05-12 22:04:48,197 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - sending payload to peer
2025-05-12 22:04:48,199 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Waiting for result from peer
2025-05-12 22:04:48,659 - TaskScriptRunner - INFO - current_round=0
2025-05-12 22:05:15,187 - TaskScriptRunner - INFO - {'eval_loss': 2.793596029281616, 'eval_model_preparation_time': 0.0023, 'eval_runtime': 25.7808, 'eval_samples_per_second': 201.7, 'eval_steps_per_second': 25.213}
2025-05-12 22:05:15,188 - TaskScriptRunner - INFO - Evaluation metric score: {'eval_loss': 2.793596029281616, 'eval_model_preparation_time': 0.0023, 'eval_runtime': 25.7808, 'eval_samples_per_second': 201.7, 'eval_steps_per_second': 25.213}
2025-05-12 22:06:07,718 - TaskScriptRunner - INFO - {'loss': 2.2603, 'grad_norm': 1.8984375, 'learning_rate': 0.0005, 'epoch': 0.049997596269410124}
2025-05-12 22:06:59,114 - TaskScriptRunner - INFO - {'loss': 2.1828, 'grad_norm': 2.046875, 'learning_rate': 0.0005, 'epoch': 0.09999519253882025}
2025-05-12 22:07:50,488 - TaskScriptRunner - INFO - {'loss': 2.2098, 'grad_norm': 2.546875, 'learning_rate': 0.0005, 'epoch': 0.1499927888082304}
2025-05-12 22:08:42,152 - TaskScriptRunner - INFO - {'loss': 2.2048, 'grad_norm': 1.5, 'learning_rate': 0.0005, 'epoch': 0.1999903850776405}
2025-05-12 22:09:33,780 - TaskScriptRunner - INFO - {'loss': 2.1925, 'grad_norm': 1.421875, 'learning_rate': 0.0005, 'epoch': 0.24998798134705064}
2025-05-12 22:10:24,998 - TaskScriptRunner - INFO - {'loss': 2.1853, 'grad_norm': 1.5546875, 'learning_rate': 0.0005, 'epoch': 0.2999855776164608}
2025-05-12 22:11:16,329 - TaskScriptRunner - INFO - {'loss': 2.1697, 'grad_norm': 2.921875, 'learning_rate': 0.0005, 'epoch': 0.34998317388587086}
2025-05-12 22:12:08,520 - TaskScriptRunner - INFO - {'loss': 2.17, 'grad_norm': 1.4765625, 'learning_rate': 0.0005, 'epoch': 0.399980770155281}
2025-05-12 22:13:01,088 - TaskScriptRunner - INFO - {'loss': 2.1632, 'grad_norm': 1.265625, 'learning_rate': 0.0005, 'epoch': 0.44997836642469113}
2025-05-12 22:13:52,447 - TaskScriptRunner - INFO - {'loss': 2.1394, 'grad_norm': 1.21875, 'learning_rate': 0.0005, 'epoch': 0.4999759626941013}
2025-05-12 22:14:43,928 - TaskScriptRunner - INFO - {'loss': 2.1166, 'grad_norm': 2.25, 'learning_rate': 0.0005, 'epoch': 0.5499735589635114}
2025-05-12 22:15:35,090 - TaskScriptRunner - INFO - {'loss': 2.1231, 'grad_norm': 1.2734375, 'learning_rate': 0.0005, 'epoch': 0.5999711552329215}
2025-05-12 22:16:26,346 - TaskScriptRunner - INFO - {'loss': 2.1179, 'grad_norm': 1.2109375, 'learning_rate': 0.0005, 'epoch': 0.6499687515023316}
2025-05-12 22:17:18,837 - TaskScriptRunner - INFO - {'loss': 2.0974, 'grad_norm': 1.2734375, 'learning_rate': 0.0005, 'epoch': 0.6999663477717417}
2025-05-12 22:18:11,122 - TaskScriptRunner - INFO - {'loss': 2.0937, 'grad_norm': 1.2421875, 'learning_rate': 0.0005, 'epoch': 0.7499639440411519}
2025-05-12 22:19:03,038 - TaskScriptRunner - INFO - {'loss': 2.0681, 'grad_norm': 1.1640625, 'learning_rate': 0.0005, 'epoch': 0.799961540310562}
2025-05-12 22:19:55,243 - TaskScriptRunner - INFO - {'loss': 2.0757, 'grad_norm': 1.2421875, 'learning_rate': 0.0005, 'epoch': 0.8499591365799721}
2025-05-12 22:20:47,590 - TaskScriptRunner - INFO - {'loss': 2.0879, 'grad_norm': 1.390625, 'learning_rate': 0.0005, 'epoch': 0.8999567328493823}
2025-05-12 22:21:40,084 - TaskScriptRunner - INFO - {'loss': 2.0585, 'grad_norm': 1.1640625, 'learning_rate': 0.0005, 'epoch': 0.9499543291187924}
2025-05-12 22:22:32,470 - TaskScriptRunner - INFO - {'loss': 2.0673, 'grad_norm': 1.1796875, 'learning_rate': 0.0005, 'epoch': 0.9999519253882025}
2025-05-12 22:22:48,005 - TaskScriptRunner - INFO - {'train_runtime': 1052.6333, 'train_samples_per_second': 39.522, 'train_steps_per_second': 0.988, 'train_loss': 2.1392099087054914, 'epoch': 0.9999519253882025}
2025-05-12 22:22:49,786 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running quantization...
2025-05-12 22:22:49,787 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Running quantization on 179 variables
2025-05-12 22:22:56,469 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-12 22:22:56,470 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=e2bdc33d-21b4-40c6-9027-454abbfc2c48] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-12 22:24:40,832 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running dequantization...
2025-05-12 22:24:40,833 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running dequantization on 179 variables
2025-05-12 22:24:40,834 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.embed_tokens.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,834 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,834 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,834 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,835 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,835 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,835 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,836 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,836 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,836 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,837 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,837 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,837 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,837 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,838 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,838 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,838 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,839 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,839 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,839 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,839 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,840 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,840 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,840 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,841 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,841 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,841 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,842 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,842 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,842 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,842 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,843 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,843 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,843 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,844 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,844 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,844 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,844 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,845 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,845 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,845 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,846 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,846 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,846 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,847 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,847 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,847 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,847 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,848 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,848 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,848 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,849 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,849 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,849 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,849 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,850 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,850 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,850 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,851 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,851 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,851 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,851 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,852 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,852 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,852 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,853 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,853 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,853 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,853 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,854 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.6.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,863 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:40,863 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,000 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,001 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,001 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,001 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.7.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,026 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,027 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,165 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,166 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,175 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.8.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,199 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,225 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,364 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,365 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,365 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,366 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,366 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,366 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,366 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,367 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,506 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,507 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,538 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.10.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,538 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,538 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,678 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,679 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,692 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.11.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,716 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,716 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,853 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,854 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,854 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.12.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,884 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:41,884 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,020 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,021 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,021 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,045 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.13.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,045 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,046 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,182 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,183 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,204 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.14.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,213 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,214 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,350 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,351 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,351 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,384 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,384 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,521 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,521 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:42,522 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Skipping dequantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-12 22:24:43,073 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Dequantized 54/179 params. Before dequantization: 1536.00 MB with meta: 0.00 MB. After dequantization: 3072.00 MB.
2025-05-12 22:24:43,074 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Dequantized back to {'model.model.embed_tokens.weight': 'float16', 'model.model.layers.0.self_attn.q_proj.weight': 'float16', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float16', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float16', 'model.model.layers.0.mlp.up_proj.weight': 'float16', 'model.model.layers.0.mlp.down_proj.weight': 'float16', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float16', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float16', 'model.model.layers.1.mlp.up_proj.weight': 'float16', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float16', 'model.model.layers.2.self_attn.o_proj.weight': 'float16', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float16', 'model.model.layers.2.mlp.up_proj.weight': 'float16', 'model.model.layers.2.mlp.down_proj.weight': 'float16', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float16', 'model.model.layers.3.mlp.up_proj.weight': 'float16', 'model.model.layers.3.mlp.down_proj.weight': 'float16', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float16', 'model.model.layers.4.self_attn.o_proj.weight': 'float16', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float16', 'model.model.layers.4.mlp.up_proj.weight': 'float16', 'model.model.layers.4.mlp.down_proj.weight': 'float16', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float16', 'model.model.layers.5.self_attn.o_proj.weight': 'float16', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float16', 'model.model.layers.5.mlp.up_proj.weight': 'float16', 'model.model.layers.5.mlp.down_proj.weight': 'float16', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float16', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float16', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float16', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float16', 'model.model.layers.9.self_attn.v_proj.weight': 'float16', 'model.model.layers.9.self_attn.o_proj.weight': 'float16', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float16', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float16', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float16', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float16', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float16', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'}
2025-05-12 22:24:43,076 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - execute for task (train)
2025-05-12 22:24:43,078 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - send data to peer
2025-05-12 22:24:43,079 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - sending payload to peer
2025-05-12 22:24:43,080 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Waiting for result from peer
2025-05-12 22:24:43,599 - TaskScriptRunner - INFO - current_round=1
2025-05-12 22:25:10,615 - TaskScriptRunner - INFO - {'eval_loss': 1.7901140451431274, 'eval_model_preparation_time': 0.0023, 'eval_runtime': 25.9605, 'eval_samples_per_second': 200.304, 'eval_steps_per_second': 25.038, 'epoch': 0.9999519253882025}
2025-05-12 22:25:10,616 - TaskScriptRunner - INFO - Evaluation metric score: {'eval_loss': 1.7901140451431274, 'eval_model_preparation_time': 0.0023, 'eval_runtime': 25.9605, 'eval_samples_per_second': 200.304, 'eval_steps_per_second': 25.038, 'epoch': 0.9999519253882025}
2025-05-12 22:25:20,072 - TaskScriptRunner - INFO - Increment num_train_epochs to 2
2025-05-12 22:26:22,334 - TaskScriptRunner - INFO - {'loss': 1.3872, 'grad_norm': 1.3125, 'learning_rate': 0.0005, 'epoch': 1.0499975962694101}
2025-05-12 22:27:13,138 - TaskScriptRunner - INFO - {'loss': 1.5091, 'grad_norm': 1.2265625, 'learning_rate': 0.0005, 'epoch': 1.0999951925388203}
2025-05-12 22:28:03,655 - TaskScriptRunner - INFO - {'loss': 1.5642, 'grad_norm': 1.1875, 'learning_rate': 0.0005, 'epoch': 1.1499927888082304}
2025-05-12 22:28:54,036 - TaskScriptRunner - INFO - {'loss': 1.5917, 'grad_norm': 1.1796875, 'learning_rate': 0.0005, 'epoch': 1.1999903850776406}
2025-05-12 22:29:45,612 - TaskScriptRunner - INFO - {'loss': 1.6352, 'grad_norm': 1.171875, 'learning_rate': 0.0005, 'epoch': 1.2499879813470507}
2025-05-12 22:30:37,293 - TaskScriptRunner - INFO - {'loss': 1.688, 'grad_norm': 1.15625, 'learning_rate': 0.0005, 'epoch': 1.2999855776164608}
2025-05-12 22:31:28,209 - TaskScriptRunner - INFO - {'loss': 1.6827, 'grad_norm': 1.1953125, 'learning_rate': 0.0005, 'epoch': 1.349983173885871}
2025-05-12 22:32:19,091 - TaskScriptRunner - INFO - {'loss': 1.6823, 'grad_norm': 1.1328125, 'learning_rate': 0.0005, 'epoch': 1.399980770155281}
2025-05-12 22:33:09,526 - TaskScriptRunner - INFO - {'loss': 1.7103, 'grad_norm': 1.875, 'learning_rate': 0.0005, 'epoch': 1.4499783664246912}
2025-05-12 22:34:00,201 - TaskScriptRunner - INFO - {'loss': 1.7189, 'grad_norm': 1.0703125, 'learning_rate': 0.0005, 'epoch': 1.4999759626941014}
2025-05-12 22:34:50,637 - TaskScriptRunner - INFO - {'loss': 1.7092, 'grad_norm': 1.078125, 'learning_rate': 0.0005, 'epoch': 1.5499735589635115}
2025-05-12 22:35:41,272 - TaskScriptRunner - INFO - {'loss': 1.7316, 'grad_norm': 1.2265625, 'learning_rate': 0.0005, 'epoch': 1.5999711552329217}
2025-05-12 22:36:31,987 - TaskScriptRunner - INFO - {'loss': 1.7216, 'grad_norm': 1.1171875, 'learning_rate': 0.0005, 'epoch': 1.6499687515023316}
2025-05-12 22:37:22,524 - TaskScriptRunner - INFO - {'loss': 1.7255, 'grad_norm': 1.125, 'learning_rate': 0.0005, 'epoch': 1.6999663477717417}
2025-05-12 22:38:13,830 - TaskScriptRunner - INFO - {'loss': 1.7407, 'grad_norm': 1.1484375, 'learning_rate': 0.0005, 'epoch': 1.7499639440411519}
2025-05-12 22:39:05,450 - TaskScriptRunner - INFO - {'loss': 1.7448, 'grad_norm': 1.0390625, 'learning_rate': 0.0005, 'epoch': 1.799961540310562}
2025-05-12 22:39:56,995 - TaskScriptRunner - INFO - {'loss': 1.7338, 'grad_norm': 1.109375, 'learning_rate': 0.0005, 'epoch': 1.8499591365799721}
2025-05-12 22:40:48,432 - TaskScriptRunner - INFO - {'loss': 1.7702, 'grad_norm': 1.0703125, 'learning_rate': 0.0005, 'epoch': 1.8999567328493823}
2025-05-12 22:41:39,895 - TaskScriptRunner - INFO - {'loss': 1.7768, 'grad_norm': 1.0625, 'learning_rate': 0.0005, 'epoch': 1.9499543291187924}
2025-05-12 22:42:31,354 - TaskScriptRunner - INFO - {'loss': 1.7475, 'grad_norm': 1.3515625, 'learning_rate': 0.0005, 'epoch': 1.9999519253882025}
2025-05-12 22:42:48,671 - TaskScriptRunner - INFO - {'train_runtime': 1037.53, 'train_samples_per_second': 80.194, 'train_steps_per_second': 2.005, 'train_loss': 0.8392827144035926, 'epoch': 1.9999519253882025}
2025-05-12 22:42:50,579 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running quantization...
2025-05-12 22:42:50,580 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Running quantization on 179 variables
2025-05-12 22:42:56,969 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-12 22:42:56,970 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2c243371-f687-441e-a290-b19c69267782] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-12 22:45:22,023 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running dequantization...
2025-05-12 22:45:22,024 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running dequantization on 179 variables
2025-05-12 22:45:26,247 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-12 22:45:26,249 - ModelDequantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-12 22:45:26,251 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - execute for task (train)
2025-05-12 22:45:26,252 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - send data to peer
2025-05-12 22:45:26,253 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - sending payload to peer
2025-05-12 22:45:26,254 - PTInProcessClientAPIExecutor - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Waiting for result from peer
2025-05-12 22:45:26,624 - TaskScriptRunner - INFO - current_round=2
2025-05-12 22:45:53,514 - TaskScriptRunner - INFO - {'eval_loss': 1.7826472520828247, 'eval_model_preparation_time': 0.0023, 'eval_runtime': 26.124, 'eval_samples_per_second': 199.051, 'eval_steps_per_second': 24.881, 'epoch': 1.9999519253882025}
2025-05-12 22:45:53,514 - TaskScriptRunner - INFO - Evaluation metric score: {'eval_loss': 1.7826472520828247, 'eval_model_preparation_time': 0.0023, 'eval_runtime': 26.124, 'eval_samples_per_second': 199.051, 'eval_steps_per_second': 24.881, 'epoch': 1.9999519253882025}
2025-05-12 22:46:01,706 - TaskScriptRunner - INFO - Increment num_train_epochs to 3
2025-05-12 22:47:02,492 - TaskScriptRunner - INFO - {'loss': 0.9892, 'grad_norm': 1.0, 'learning_rate': 0.0005, 'epoch': 2.04999759626941}
2025-05-12 22:47:52,807 - TaskScriptRunner - INFO - {'loss': 1.0955, 'grad_norm': 0.96484375, 'learning_rate': 0.0005, 'epoch': 2.0999951925388203}
2025-05-12 22:48:43,392 - TaskScriptRunner - INFO - {'loss': 1.171, 'grad_norm': 0.96484375, 'learning_rate': 0.0005, 'epoch': 2.1499927888082304}
2025-05-12 22:49:33,802 - TaskScriptRunner - INFO - {'loss': 1.2065, 'grad_norm': 0.984375, 'learning_rate': 0.0005, 'epoch': 2.1999903850776406}
2025-05-12 22:50:24,710 - TaskScriptRunner - INFO - {'loss': 1.262, 'grad_norm': 1.0625, 'learning_rate': 0.0005, 'epoch': 2.2499879813470507}
2025-05-12 22:51:15,728 - TaskScriptRunner - INFO - {'loss': 1.2807, 'grad_norm': 1.1015625, 'learning_rate': 0.0005, 'epoch': 2.299985577616461}
2025-05-12 22:52:06,323 - TaskScriptRunner - INFO - {'loss': 1.2911, 'grad_norm': 1.015625, 'learning_rate': 0.0005, 'epoch': 2.349983173885871}
2025-05-12 22:52:56,828 - TaskScriptRunner - INFO - {'loss': 1.3192, 'grad_norm': 1.109375, 'learning_rate': 0.0005, 'epoch': 2.399980770155281}
2025-05-12 22:53:47,471 - TaskScriptRunner - INFO - {'loss': 1.3268, 'grad_norm': 1.046875, 'learning_rate': 0.0005, 'epoch': 2.4499783664246912}
2025-05-12 22:54:38,255 - TaskScriptRunner - INFO - {'loss': 1.3426, 'grad_norm': 1.078125, 'learning_rate': 0.0005, 'epoch': 2.4999759626941014}
2025-05-12 22:55:29,280 - TaskScriptRunner - INFO - {'loss': 1.3803, 'grad_norm': 1.15625, 'learning_rate': 0.0005, 'epoch': 2.5499735589635115}
2025-05-12 22:56:20,118 - TaskScriptRunner - INFO - {'loss': 1.3757, 'grad_norm': 1.15625, 'learning_rate': 0.0005, 'epoch': 2.5999711552329217}
2025-05-12 22:57:10,600 - TaskScriptRunner - INFO - {'loss': 1.3957, 'grad_norm': 1.1015625, 'learning_rate': 0.0005, 'epoch': 2.649968751502332}
2025-05-12 22:58:01,285 - TaskScriptRunner - INFO - {'loss': 1.4074, 'grad_norm': 1.1484375, 'learning_rate': 0.0005, 'epoch': 2.699966347771742}
2025-05-12 22:58:52,424 - TaskScriptRunner - INFO - {'loss': 1.4274, 'grad_norm': 1.15625, 'learning_rate': 0.0005, 'epoch': 2.749963944041152}
2025-05-12 22:59:43,076 - TaskScriptRunner - INFO - {'loss': 1.4266, 'grad_norm': 1.078125, 'learning_rate': 0.0005, 'epoch': 2.799961540310562}
2025-05-12 23:00:34,062 - TaskScriptRunner - INFO - {'loss': 1.404, 'grad_norm': 1.1328125, 'learning_rate': 0.0005, 'epoch': 2.8499591365799724}
2025-05-12 23:01:24,457 - TaskScriptRunner - INFO - {'loss': 1.4405, 'grad_norm': 1.0234375, 'learning_rate': 0.0005, 'epoch': 2.8999567328493825}
2025-05-12 23:02:14,904 - TaskScriptRunner - INFO - {'loss': 1.4411, 'grad_norm': 1.0625, 'learning_rate': 0.0005, 'epoch': 2.9499543291187926}
2025-05-12 23:03:05,458 - TaskScriptRunner - INFO - {'loss': 1.4354, 'grad_norm': 1.109375, 'learning_rate': 0.0005, 'epoch': 2.9999519253882028}
2025-05-12 23:03:26,362 - TaskScriptRunner - INFO - {'train_runtime': 1035.0576, 'train_samples_per_second': 120.579, 'train_steps_per_second': 3.014, 'train_loss': 0.4403130164513221, 'epoch': 2.9999519253882028}
2025-05-12 23:03:28,511 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running quantization...
2025-05-12 23:03:28,512 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Running quantization on 179 variables
2025-05-12 23:03:35,153 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-12 23:03:35,154 - ModelQuantizer - INFO - [identity=site-alpaca, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=d14b5cb6-a703-450d-ac55-13fe4886ec2c] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16

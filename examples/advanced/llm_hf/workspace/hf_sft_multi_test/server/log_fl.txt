2025-05-13 17:59:39,239 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-13 17:59:39,249 - driver_manager - WARNING - Driver ignored. Error loading nvflare.fuel.f3.drivers.aio_http_driver: [Errno 2] No such file or directory
2025-05-13 17:59:43,140 - IntimeModelSelector - INFO - model selection weights control: {}
2025-05-13 17:59:43,194 - ModelQuantizer - INFO - Using model quantizator.
2025-05-13 17:59:43,195 - ModelDequantizer - INFO - Using model dequantizator.
2025-05-13 17:59:43,198 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Initializing BaseModelController workflow.
2025-05-13 17:59:43,198 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Beginning model controller run.
2025-05-13 17:59:43,199 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Start FedAvg.
2025-05-13 17:59:43,199 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - loading initial model from persistor
2025-05-13 17:59:43,199 - PTFileModelPersistor - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Both source_ckpt_file_full_name and ckpt_preload_path are not provided. Using the default model weights initialized on the persistor side.
2025-05-13 17:59:43,200 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Round 0 started.
2025-05-13 17:59:43,200 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sampled clients: ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-13 17:59:43,201 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller] - Sending task train to ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-13 17:59:48,592 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Running quantization...
2025-05-13 17:59:48,592 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Running quantization on 179 variables
2025-05-13 17:59:48,606 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Running quantization...
2025-05-13 17:59:48,607 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Running quantization on 179 variables
2025-05-13 17:59:48,815 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,816 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,869 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,870 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,870 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,871 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,871 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,875 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,875 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,916 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,917 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,917 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,923 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.2.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,924 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,925 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,965 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.2.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,966 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,966 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,966 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:48,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,009 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,010 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,010 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,010 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,011 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,011 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,014 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,015 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,053 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,054 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,054 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,054 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,055 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,055 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,055 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,055 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,056 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,105 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,106 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,106 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,107 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,107 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,107 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,108 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,108 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,148 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,149 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,149 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,152 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,153 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,154 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,196 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,197 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,197 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,197 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,198 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,198 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,198 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,199 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,199 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,246 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,247 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,247 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,248 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.9.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,248 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.9.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,248 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.9.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,249 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,298 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,299 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,300 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,300 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,300 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,301 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,301 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,302 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,347 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,347 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,348 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.11.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,348 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.11.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,352 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,352 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,401 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,401 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,402 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,402 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,402 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,402 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,403 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,403 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,442 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,443 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,443 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,443 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,445 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.13.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,447 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,447 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,492 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,492 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,493 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,493 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,493 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,493 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,494 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,494 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,539 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,539 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,540 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,540 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.15.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,540 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.15.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,542 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,542 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,591 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,592 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,592 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 17:59:49,730 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Quantized 177/179 params. Before quantization: 5664.50 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-13 17:59:49,731 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-13 17:59:49,845 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Quantized 61/179 params. Before quantization: 5048.27 MB. After quantization: 2216.01 MB with meta: 0.00 MB.
2025-05-13 17:59:49,935 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float16', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float16', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float16', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float16', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float16', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float16', 'model.model.layers.5.self_attn.o_proj.weight': 'float16', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float16', 'model.model.layers.6.self_attn.o_proj.weight': 'float16', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float16', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float16', 'model.model.layers.7.self_attn.k_proj.weight': 'float16', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float16', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float16', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float16', 'model.model.layers.8.self_attn.k_proj.weight': 'float16', 'model.model.layers.8.self_attn.v_proj.weight': 'float16', 'model.model.layers.8.self_attn.o_proj.weight': 'float16', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float16', 'model.model.layers.9.self_attn.v_proj.weight': 'float16', 'model.model.layers.9.self_attn.o_proj.weight': 'float16', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float16', 'model.model.layers.10.self_attn.k_proj.weight': 'float16', 'model.model.layers.10.self_attn.v_proj.weight': 'float16', 'model.model.layers.10.self_attn.o_proj.weight': 'float16', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float16', 'model.model.layers.11.self_attn.k_proj.weight': 'float16', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float16', 'model.model.layers.12.self_attn.k_proj.weight': 'float16', 'model.model.layers.12.self_attn.v_proj.weight': 'float16', 'model.model.layers.12.self_attn.o_proj.weight': 'float16', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float16', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float16', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float16', 'model.model.layers.14.self_attn.k_proj.weight': 'float16', 'model.model.layers.14.self_attn.v_proj.weight': 'float16', 'model.model.layers.14.self_attn.o_proj.weight': 'float16', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float16', 'model.model.layers.15.self_attn.v_proj.weight': 'float16', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-13 18:07:33,063 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Running dequantization...
2025-05-13 18:07:33,064 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Running dequantization on 179 variables
2025-05-13 18:07:36,986 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:07:36,987 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=9a5e61b2-5ea6-4039-9988-5c74287040cb] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:07:42,172 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=703ed024-7eed-4358-91b8-0e46153225fe] - Running quantization...
2025-05-13 18:07:42,172 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=703ed024-7eed-4358-91b8-0e46153225fe] - Already quantized, skip quantization
2025-05-13 18:12:38,930 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=703ed024-7eed-4358-91b8-0e46153225fe] - Running dequantization...
2025-05-13 18:12:38,931 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=703ed024-7eed-4358-91b8-0e46153225fe] - Running dequantization on 179 variables
2025-05-13 18:12:42,887 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=703ed024-7eed-4358-91b8-0e46153225fe] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:12:42,889 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=703ed024-7eed-4358-91b8-0e46153225fe] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:18:24,028 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Running dequantization...
2025-05-13 18:18:24,028 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Running dequantization on 179 variables
2025-05-13 18:18:27,986 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:18:27,988 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:18:28,386 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - aggregating 3 update(s) at round 0
2025-05-13 18:18:41,160 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Start persist model on server.
2025-05-13 18:19:42,517 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - End persist model on server.
2025-05-13 18:19:42,525 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Round 1 started.
2025-05-13 18:19:42,525 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Sampled clients: ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-13 18:19:42,526 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=f4aeca21-1f4b-4377-8126-e81ab7ac9d17] - Sending task train to ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-13 18:19:42,545 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Running quantization...
2025-05-13 18:19:42,545 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Running quantization on 179 variables
2025-05-13 18:19:44,526 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Running quantization...
2025-05-13 18:19:44,526 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Running quantization on 179 variables
2025-05-13 18:19:44,527 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.embed_tokens.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,527 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,527 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,527 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,528 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,528 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,528 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,528 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,529 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,529 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,529 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,529 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,530 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,530 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,530 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,530 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,531 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,531 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,531 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,531 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,531 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,532 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,532 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,532 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,532 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,533 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,533 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,533 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,533 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,533 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.mlp.gate_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,534 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.mlp.up_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,534 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.mlp.down_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,534 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,534 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,535 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,536 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,536 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,769 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,770 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,770 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,770 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.4.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,808 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.4.self_attn.o_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,809 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:44,809 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,047 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,048 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,072 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.5.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,119 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,119 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,359 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,359 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,383 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.6.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,418 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,419 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,659 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,659 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,728 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,729 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:45,967 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,036 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,037 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,271 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,272 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,272 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,324 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,324 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,560 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,560 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,629 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,629 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,867 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,868 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,990 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:46,990 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,227 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,227 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,296 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,296 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,530 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,530 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,565 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.13.self_attn.v_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,583 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,583 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,819 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,819 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,888 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:47,888 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:48,125 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:48,126 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:48,196 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:48,196 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:48,432 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:48,433 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:48,433 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:19:49,392 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Quantized 179/179 params. Before quantization: 5664.51 MB. After quantization: 2832.25 MB with meta: 0.00 MB.
2025-05-13 18:19:49,392 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-13 18:19:49,405 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Quantized 81/179 params. Before quantization: 4800.25 MB. After quantization: 1968.00 MB with meta: 0.00 MB.
2025-05-13 18:19:49,442 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Quantized from {'model.model.embed_tokens.weight': 'float16', 'model.model.layers.0.self_attn.q_proj.weight': 'float16', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float16', 'model.model.layers.0.self_attn.o_proj.weight': 'float16', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float16', 'model.model.layers.0.mlp.up_proj.weight': 'float16', 'model.model.layers.0.mlp.down_proj.weight': 'float16', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float16', 'model.model.layers.1.self_attn.v_proj.weight': 'float16', 'model.model.layers.1.self_attn.o_proj.weight': 'float16', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float16', 'model.model.layers.1.mlp.up_proj.weight': 'float16', 'model.model.layers.1.mlp.down_proj.weight': 'float16', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float16', 'model.model.layers.2.self_attn.v_proj.weight': 'float16', 'model.model.layers.2.self_attn.o_proj.weight': 'float16', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float16', 'model.model.layers.2.mlp.up_proj.weight': 'float16', 'model.model.layers.2.mlp.down_proj.weight': 'float16', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float16', 'model.model.layers.3.self_attn.k_proj.weight': 'float16', 'model.model.layers.3.self_attn.v_proj.weight': 'float16', 'model.model.layers.3.self_attn.o_proj.weight': 'float16', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float16', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float16', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float16', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float16', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float16', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-13 18:27:46,148 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Running dequantization...
2025-05-13 18:27:46,149 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Running dequantization on 179 variables
2025-05-13 18:27:54,236 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:27:54,237 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:27:54,239 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=4513e98d-593a-4eb4-9683-14ac3a319301] - validation metric -3.0295448303222656 from client site-dolly
2025-05-13 18:27:59,354 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=0efdb3e4-53fb-4169-9b20-1dbc5cdb6950] - Running quantization...
2025-05-13 18:27:59,354 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=0efdb3e4-53fb-4169-9b20-1dbc5cdb6950] - Already quantized, skip quantization
2025-05-13 18:33:13,723 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0efdb3e4-53fb-4169-9b20-1dbc5cdb6950] - Running dequantization...
2025-05-13 18:33:13,724 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0efdb3e4-53fb-4169-9b20-1dbc5cdb6950] - Running dequantization on 179 variables
2025-05-13 18:33:17,615 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0efdb3e4-53fb-4169-9b20-1dbc5cdb6950] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:33:17,617 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0efdb3e4-53fb-4169-9b20-1dbc5cdb6950] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:33:17,619 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0efdb3e4-53fb-4169-9b20-1dbc5cdb6950] - validation metric -2.7538247108459473 from client site-oasst1
2025-05-13 18:38:31,999 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Running dequantization...
2025-05-13 18:38:32,000 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Running dequantization on 179 variables
2025-05-13 18:38:35,948 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:38:35,950 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:38:35,951 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - validation metric -1.7901445627212524 from client site-alpaca
2025-05-13 18:38:36,370 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - new best validation metric at round 1: -2.5245047012964883
2025-05-13 18:39:05,555 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - aggregating 3 update(s) at round 1
2025-05-13 18:39:19,144 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Start persist model on server.
2025-05-13 18:40:34,405 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - End persist model on server.
2025-05-13 18:40:34,411 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Round 2 started.
2025-05-13 18:40:34,412 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Sampled clients: ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-13 18:40:34,413 - FedAvg - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=bdb3b4e2-6871-4c3d-89c5-967c7dcafb76] - Sending task train to ['site-dolly', 'site-alpaca', 'site-oasst1']
2025-05-13 18:40:34,917 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Running quantization...
2025-05-13 18:40:34,918 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Running quantization on 179 variables
2025-05-13 18:40:34,955 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Running quantization...
2025-05-13 18:40:34,955 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Running quantization on 179 variables
2025-05-13 18:40:35,942 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.0.self_attn.k_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:35,977 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.0.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:35,978 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.0.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,213 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.0.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,214 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.0.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,214 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.1.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,269 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.1.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,270 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.1.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,505 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.1.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,506 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.1.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,506 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.2.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,559 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Skipping quantization for model.model.layers.2.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,560 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Skipping quantization for model.model.layers.2.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,797 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.2.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,798 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.2.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,867 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.3.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:36,867 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.3.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,104 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.3.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,105 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.3.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,105 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.4.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,157 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.4.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,158 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.4.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,394 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.4.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,394 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.4.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,394 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.5.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,446 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.5.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,447 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.5.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,682 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.5.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,682 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.5.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,683 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.6.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,735 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.6.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,735 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.6.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,970 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.6.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:37,971 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.6.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,040 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.7.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,041 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.7.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,279 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.7.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,279 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.7.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,279 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.8.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,331 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.8.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,332 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.8.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,570 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.8.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,571 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.8.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,571 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.9.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,623 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.9.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,623 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.9.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,859 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.9.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,860 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.9.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,860 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.10.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,912 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.10.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:38,912 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.10.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,150 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.10.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,151 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.10.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,151 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.11.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,204 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.11.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,204 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.11.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,442 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.11.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,442 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.11.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,443 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.12.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,494 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.12.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,495 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.12.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,732 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.12.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,732 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.12.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,733 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.13.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,784 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.13.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:39,785 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.13.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,023 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.13.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,023 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.13.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,023 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.14.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,077 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Skipping quantization for model.model.layers.14.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,078 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Skipping quantization for model.model.layers.14.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,316 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.14.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,317 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.14.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,317 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.15.self_attn.q_proj.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,373 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Skipping quantization for model.model.layers.15.self_attn.q_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,373 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Skipping quantization for model.model.layers.15.self_attn.k_norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,631 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.15.post_attention_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,631 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.layers.15.post_feedforward_layernorm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:40,631 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Skipping quantization for model.model.norm.weight, quantization bit float16 >= source data bit float16
2025-05-13 18:40:41,586 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Quantized 173/179 params. Before quantization: 5664.48 MB. After quantization: 2832.23 MB with meta: 0.00 MB.
2025-05-13 18:40:41,586 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float16', 'model.model.layers.2.self_attn.k_norm.weight': 'float16', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float16', 'model.model.layers.14.self_attn.k_norm.weight': 'float16', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float16', 'model.model.layers.15.self_attn.k_norm.weight': 'float16', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'} to float16
2025-05-13 18:40:41,639 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Quantized 106/179 params. Before quantization: 5552.28 MB. After quantization: 2720.02 MB with meta: 0.00 MB.
2025-05-13 18:40:41,678 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-alpaca, peer_run=simulate_job, task_name=train, task_id=9461cf67-c688-44dc-9ccd-022b266ad0ff] - Quantized from {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float16', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float16', 'model.model.layers.0.self_attn.k_norm.weight': 'float16', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float16', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.1.self_attn.q_proj.weight': 'float16', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float16', 'model.model.layers.1.self_attn.k_norm.weight': 'float16', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float16', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.2.self_attn.q_proj.weight': 'float16', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float16', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float16', 'model.model.layers.3.self_attn.k_norm.weight': 'float16', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float16', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.4.self_attn.q_proj.weight': 'float16', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float16', 'model.model.layers.4.self_attn.k_norm.weight': 'float16', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float16', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.5.self_attn.q_proj.weight': 'float16', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float16', 'model.model.layers.5.self_attn.k_norm.weight': 'float16', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float16', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.6.self_attn.q_proj.weight': 'float16', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float16', 'model.model.layers.6.self_attn.k_norm.weight': 'float16', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float16', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float16', 'model.model.layers.7.self_attn.k_norm.weight': 'float16', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float16', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.8.self_attn.q_proj.weight': 'float16', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float16', 'model.model.layers.8.self_attn.k_norm.weight': 'float16', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float16', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.9.self_attn.q_proj.weight': 'float16', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float16', 'model.model.layers.9.self_attn.k_norm.weight': 'float16', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float16', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.10.self_attn.q_proj.weight': 'float16', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float16', 'model.model.layers.10.self_attn.k_norm.weight': 'float16', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float16', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.11.self_attn.q_proj.weight': 'float16', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float16', 'model.model.layers.11.self_attn.k_norm.weight': 'float16', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float16', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.12.self_attn.q_proj.weight': 'float16', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float16', 'model.model.layers.12.self_attn.k_norm.weight': 'float16', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float16', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.13.self_attn.q_proj.weight': 'float16', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float16', 'model.model.layers.13.self_attn.k_norm.weight': 'float16', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float16', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.14.self_attn.q_proj.weight': 'float16', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float16', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float16', 'model.model.layers.15.self_attn.q_proj.weight': 'float16', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float16', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float16', 'model.model.norm.weight': 'float16', 'model.lm_head.weight': 'float32'} to float16
2025-05-13 18:41:50,226 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Running dequantization...
2025-05-13 18:41:50,226 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Running dequantization on 179 variables
2025-05-13 18:41:54,092 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:41:54,094 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:41:54,095 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-dolly, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5bbf376b-070b-4fb6-92ba-7806eb81cc34] - validation metric -3.099393606185913 from client site-dolly
2025-05-13 18:41:59,360 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=0e77d03f-a3ac-49ef-9254-02d0b6a04ff7] - Running quantization...
2025-05-13 18:41:59,360 - ModelQuantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, task_name=train, task_id=0e77d03f-a3ac-49ef-9254-02d0b6a04ff7] - Already quantized, skip quantization
2025-05-13 18:43:01,316 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0e77d03f-a3ac-49ef-9254-02d0b6a04ff7] - Running dequantization...
2025-05-13 18:43:01,317 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0e77d03f-a3ac-49ef-9254-02d0b6a04ff7] - Running dequantization on 179 variables
2025-05-13 18:43:05,254 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0e77d03f-a3ac-49ef-9254-02d0b6a04ff7] - Dequantized 179/179 params. Before dequantization: 2832.25 MB with meta: 0.00 MB. After dequantization: 5664.51 MB.
2025-05-13 18:43:05,255 - ModelDequantizer - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0e77d03f-a3ac-49ef-9254-02d0b6a04ff7] - Dequantized back to {'model.model.embed_tokens.weight': 'float32', 'model.model.layers.0.self_attn.q_proj.weight': 'float32', 'model.model.layers.0.self_attn.k_proj.weight': 'float32', 'model.model.layers.0.self_attn.v_proj.weight': 'float32', 'model.model.layers.0.self_attn.o_proj.weight': 'float32', 'model.model.layers.0.self_attn.q_norm.weight': 'float32', 'model.model.layers.0.self_attn.k_norm.weight': 'float32', 'model.model.layers.0.mlp.gate_proj.weight': 'float32', 'model.model.layers.0.mlp.up_proj.weight': 'float32', 'model.model.layers.0.mlp.down_proj.weight': 'float32', 'model.model.layers.0.post_attention_layernorm.weight': 'float32', 'model.model.layers.0.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.1.self_attn.q_proj.weight': 'float32', 'model.model.layers.1.self_attn.k_proj.weight': 'float32', 'model.model.layers.1.self_attn.v_proj.weight': 'float32', 'model.model.layers.1.self_attn.o_proj.weight': 'float32', 'model.model.layers.1.self_attn.q_norm.weight': 'float32', 'model.model.layers.1.self_attn.k_norm.weight': 'float32', 'model.model.layers.1.mlp.gate_proj.weight': 'float32', 'model.model.layers.1.mlp.up_proj.weight': 'float32', 'model.model.layers.1.mlp.down_proj.weight': 'float32', 'model.model.layers.1.post_attention_layernorm.weight': 'float32', 'model.model.layers.1.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.2.self_attn.q_proj.weight': 'float32', 'model.model.layers.2.self_attn.k_proj.weight': 'float32', 'model.model.layers.2.self_attn.v_proj.weight': 'float32', 'model.model.layers.2.self_attn.o_proj.weight': 'float32', 'model.model.layers.2.self_attn.q_norm.weight': 'float32', 'model.model.layers.2.self_attn.k_norm.weight': 'float32', 'model.model.layers.2.mlp.gate_proj.weight': 'float32', 'model.model.layers.2.mlp.up_proj.weight': 'float32', 'model.model.layers.2.mlp.down_proj.weight': 'float32', 'model.model.layers.2.post_attention_layernorm.weight': 'float32', 'model.model.layers.2.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.3.self_attn.q_proj.weight': 'float32', 'model.model.layers.3.self_attn.k_proj.weight': 'float32', 'model.model.layers.3.self_attn.v_proj.weight': 'float32', 'model.model.layers.3.self_attn.o_proj.weight': 'float32', 'model.model.layers.3.self_attn.q_norm.weight': 'float32', 'model.model.layers.3.self_attn.k_norm.weight': 'float32', 'model.model.layers.3.mlp.gate_proj.weight': 'float32', 'model.model.layers.3.mlp.up_proj.weight': 'float32', 'model.model.layers.3.mlp.down_proj.weight': 'float32', 'model.model.layers.3.post_attention_layernorm.weight': 'float32', 'model.model.layers.3.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.4.self_attn.q_proj.weight': 'float32', 'model.model.layers.4.self_attn.k_proj.weight': 'float32', 'model.model.layers.4.self_attn.v_proj.weight': 'float32', 'model.model.layers.4.self_attn.o_proj.weight': 'float32', 'model.model.layers.4.self_attn.q_norm.weight': 'float32', 'model.model.layers.4.self_attn.k_norm.weight': 'float32', 'model.model.layers.4.mlp.gate_proj.weight': 'float32', 'model.model.layers.4.mlp.up_proj.weight': 'float32', 'model.model.layers.4.mlp.down_proj.weight': 'float32', 'model.model.layers.4.post_attention_layernorm.weight': 'float32', 'model.model.layers.4.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.5.self_attn.q_proj.weight': 'float32', 'model.model.layers.5.self_attn.k_proj.weight': 'float32', 'model.model.layers.5.self_attn.v_proj.weight': 'float32', 'model.model.layers.5.self_attn.o_proj.weight': 'float32', 'model.model.layers.5.self_attn.q_norm.weight': 'float32', 'model.model.layers.5.self_attn.k_norm.weight': 'float32', 'model.model.layers.5.mlp.gate_proj.weight': 'float32', 'model.model.layers.5.mlp.up_proj.weight': 'float32', 'model.model.layers.5.mlp.down_proj.weight': 'float32', 'model.model.layers.5.post_attention_layernorm.weight': 'float32', 'model.model.layers.5.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.6.self_attn.q_proj.weight': 'float32', 'model.model.layers.6.self_attn.k_proj.weight': 'float32', 'model.model.layers.6.self_attn.v_proj.weight': 'float32', 'model.model.layers.6.self_attn.o_proj.weight': 'float32', 'model.model.layers.6.self_attn.q_norm.weight': 'float32', 'model.model.layers.6.self_attn.k_norm.weight': 'float32', 'model.model.layers.6.mlp.gate_proj.weight': 'float32', 'model.model.layers.6.mlp.up_proj.weight': 'float32', 'model.model.layers.6.mlp.down_proj.weight': 'float32', 'model.model.layers.6.post_attention_layernorm.weight': 'float32', 'model.model.layers.6.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.7.self_attn.q_proj.weight': 'float32', 'model.model.layers.7.self_attn.k_proj.weight': 'float32', 'model.model.layers.7.self_attn.v_proj.weight': 'float32', 'model.model.layers.7.self_attn.o_proj.weight': 'float32', 'model.model.layers.7.self_attn.q_norm.weight': 'float32', 'model.model.layers.7.self_attn.k_norm.weight': 'float32', 'model.model.layers.7.mlp.gate_proj.weight': 'float32', 'model.model.layers.7.mlp.up_proj.weight': 'float32', 'model.model.layers.7.mlp.down_proj.weight': 'float32', 'model.model.layers.7.post_attention_layernorm.weight': 'float32', 'model.model.layers.7.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.8.self_attn.q_proj.weight': 'float32', 'model.model.layers.8.self_attn.k_proj.weight': 'float32', 'model.model.layers.8.self_attn.v_proj.weight': 'float32', 'model.model.layers.8.self_attn.o_proj.weight': 'float32', 'model.model.layers.8.self_attn.q_norm.weight': 'float32', 'model.model.layers.8.self_attn.k_norm.weight': 'float32', 'model.model.layers.8.mlp.gate_proj.weight': 'float32', 'model.model.layers.8.mlp.up_proj.weight': 'float32', 'model.model.layers.8.mlp.down_proj.weight': 'float32', 'model.model.layers.8.post_attention_layernorm.weight': 'float32', 'model.model.layers.8.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.9.self_attn.q_proj.weight': 'float32', 'model.model.layers.9.self_attn.k_proj.weight': 'float32', 'model.model.layers.9.self_attn.v_proj.weight': 'float32', 'model.model.layers.9.self_attn.o_proj.weight': 'float32', 'model.model.layers.9.self_attn.q_norm.weight': 'float32', 'model.model.layers.9.self_attn.k_norm.weight': 'float32', 'model.model.layers.9.mlp.gate_proj.weight': 'float32', 'model.model.layers.9.mlp.up_proj.weight': 'float32', 'model.model.layers.9.mlp.down_proj.weight': 'float32', 'model.model.layers.9.post_attention_layernorm.weight': 'float32', 'model.model.layers.9.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.10.self_attn.q_proj.weight': 'float32', 'model.model.layers.10.self_attn.k_proj.weight': 'float32', 'model.model.layers.10.self_attn.v_proj.weight': 'float32', 'model.model.layers.10.self_attn.o_proj.weight': 'float32', 'model.model.layers.10.self_attn.q_norm.weight': 'float32', 'model.model.layers.10.self_attn.k_norm.weight': 'float32', 'model.model.layers.10.mlp.gate_proj.weight': 'float32', 'model.model.layers.10.mlp.up_proj.weight': 'float32', 'model.model.layers.10.mlp.down_proj.weight': 'float32', 'model.model.layers.10.post_attention_layernorm.weight': 'float32', 'model.model.layers.10.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.11.self_attn.q_proj.weight': 'float32', 'model.model.layers.11.self_attn.k_proj.weight': 'float32', 'model.model.layers.11.self_attn.v_proj.weight': 'float32', 'model.model.layers.11.self_attn.o_proj.weight': 'float32', 'model.model.layers.11.self_attn.q_norm.weight': 'float32', 'model.model.layers.11.self_attn.k_norm.weight': 'float32', 'model.model.layers.11.mlp.gate_proj.weight': 'float32', 'model.model.layers.11.mlp.up_proj.weight': 'float32', 'model.model.layers.11.mlp.down_proj.weight': 'float32', 'model.model.layers.11.post_attention_layernorm.weight': 'float32', 'model.model.layers.11.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.12.self_attn.q_proj.weight': 'float32', 'model.model.layers.12.self_attn.k_proj.weight': 'float32', 'model.model.layers.12.self_attn.v_proj.weight': 'float32', 'model.model.layers.12.self_attn.o_proj.weight': 'float32', 'model.model.layers.12.self_attn.q_norm.weight': 'float32', 'model.model.layers.12.self_attn.k_norm.weight': 'float32', 'model.model.layers.12.mlp.gate_proj.weight': 'float32', 'model.model.layers.12.mlp.up_proj.weight': 'float32', 'model.model.layers.12.mlp.down_proj.weight': 'float32', 'model.model.layers.12.post_attention_layernorm.weight': 'float32', 'model.model.layers.12.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.13.self_attn.q_proj.weight': 'float32', 'model.model.layers.13.self_attn.k_proj.weight': 'float32', 'model.model.layers.13.self_attn.v_proj.weight': 'float32', 'model.model.layers.13.self_attn.o_proj.weight': 'float32', 'model.model.layers.13.self_attn.q_norm.weight': 'float32', 'model.model.layers.13.self_attn.k_norm.weight': 'float32', 'model.model.layers.13.mlp.gate_proj.weight': 'float32', 'model.model.layers.13.mlp.up_proj.weight': 'float32', 'model.model.layers.13.mlp.down_proj.weight': 'float32', 'model.model.layers.13.post_attention_layernorm.weight': 'float32', 'model.model.layers.13.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.14.self_attn.q_proj.weight': 'float32', 'model.model.layers.14.self_attn.k_proj.weight': 'float32', 'model.model.layers.14.self_attn.v_proj.weight': 'float32', 'model.model.layers.14.self_attn.o_proj.weight': 'float32', 'model.model.layers.14.self_attn.q_norm.weight': 'float32', 'model.model.layers.14.self_attn.k_norm.weight': 'float32', 'model.model.layers.14.mlp.gate_proj.weight': 'float32', 'model.model.layers.14.mlp.up_proj.weight': 'float32', 'model.model.layers.14.mlp.down_proj.weight': 'float32', 'model.model.layers.14.post_attention_layernorm.weight': 'float32', 'model.model.layers.14.post_feedforward_layernorm.weight': 'float32', 'model.model.layers.15.self_attn.q_proj.weight': 'float32', 'model.model.layers.15.self_attn.k_proj.weight': 'float32', 'model.model.layers.15.self_attn.v_proj.weight': 'float32', 'model.model.layers.15.self_attn.o_proj.weight': 'float32', 'model.model.layers.15.self_attn.q_norm.weight': 'float32', 'model.model.layers.15.self_attn.k_norm.weight': 'float32', 'model.model.layers.15.mlp.gate_proj.weight': 'float32', 'model.model.layers.15.mlp.up_proj.weight': 'float32', 'model.model.layers.15.mlp.down_proj.weight': 'float32', 'model.model.layers.15.post_attention_layernorm.weight': 'float32', 'model.model.layers.15.post_feedforward_layernorm.weight': 'float32', 'model.model.norm.weight': 'float32', 'model.lm_head.weight': 'float32'}
2025-05-13 18:43:05,256 - IntimeModelSelector - INFO - [identity=simulator_server, run=simulate_job, wf=controller, peer=site-oasst1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=0e77d03f-a3ac-49ef-9254-02d0b6a04ff7] - validation metric -2.813129425048828 from client site-oasst1
